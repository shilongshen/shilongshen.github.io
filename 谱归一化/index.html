<!DOCTYPE html>
<html lang="zh-cn">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>谱归一化 - 我的个人博客</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="谱归一化" />
<meta property="og:description" content="1. Lipschitz定义 2. Lipschitz常数 3.深度学习中的Lipschitz约束：泛化与生成模型 L约束与泛化 扰动敏感
记输入为 x，输出为 y，模型为 f，模型参数为 w，记为：
很多时候，我们希望得到一个“稳健”的模型。何为稳健？一般来说有两种含义，一是对于参数扰动的稳定性，比如模型变成了 fw&#43;Δw(x) 后是否还能达到相近的效果？如果在动力学系统中，还要考虑模型最终是否能恢复到 fw(x)；二是对于输入扰动的稳定性，比如输入从 x 变成了 x&#43;Δx 后，fw(x&#43;Δx) 是否能给出相近的预测结果。
读者或许已经听说过深度学习模型存在“对抗攻击样本”，比如图片只改变一个像素就给出完全不一样的分类结果，这就是模型对输入过于敏感的案例。
L约束
所以，大多数时候我们都希望模型对输入扰动是不敏感的，这通常能提高模型的泛化性能。也就是说，我们希望 ||x1−x2|| 很小时：
也尽可能地小。当然，“尽可能”究竟是怎样，谁也说不准。于是 Lipschitz 提出了一个更具体的约束，那就是存在某个常数 C（它只与参数有关，与输入无关），使得下式恒成立
也就是说，希望整个模型被一个线性函数“控制”住。这便是 L 约束了。
**换言之，在这里我们认为满足 L 约束的模型才是一个好模型。**并且对于具体的模型，我们希望估算出 C(w) 的表达式，并且希望 C(w) 越小越好，越小意味着它对输入扰动越不敏感，泛化性越好。
神经网络 在这里我们对具体的神经网络进行分析，以观察神经网络在什么时候会满足 L 约束。
简单而言，我们考虑单层的全连接 f(Wx&#43;b)，这里的 f 是激活函数，而 W,b 则是参数矩阵/向量，这时候 (3) 变为：
让 x1,x2 充分接近，那么就可以将左边用一阶项近似，得到：
显然，要希望左边不超过右边，**∂f/∂x 这一项（每个元素）的绝对值必须不超过某个常数。这就要求我们要使用“导数有上下界”的激活函数，不过我们目前常用的激活函数，比如sigmoid、tanh、relu等，都满足这个条件。**假定激活函数的梯度已经有界，尤其是我们常用的 relu 激活函数来说这个界还是 1，因此 ∂f/∂x 这一项只带来一个常数，我们暂时忽略它，剩下来我们只需要考虑 ||W(x1−x2)||。
多层的神经网络可以逐步递归分析，从而最终还是单层的神经网络问题，而 CNN、RNN 等结构本质上还是特殊的全连接，所以照样可以用全连接的结果。因此，对于神经网络来说，问题变成了：如果下式恒成立，那么 C 的值可以是多少？
找出 C 的表达式后，我们就可以希望 C 尽可能小，从而给参数带来一个正则化项。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" />
<meta property="og:image" content="https://shilongshen.github.io/logo.png"/>
<meta property="article:published_time" content="2020-11-15T13:26:17+08:00" />
<meta property="article:modified_time" content="2020-11-15T13:26:17+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://shilongshen.github.io/logo.png"/>

<meta name="twitter:title" content="谱归一化"/>
<meta name="twitter:description" content="1. Lipschitz定义 2. Lipschitz常数 3.深度学习中的Lipschitz约束：泛化与生成模型 L约束与泛化 扰动敏感
记输入为 x，输出为 y，模型为 f，模型参数为 w，记为：
很多时候，我们希望得到一个“稳健”的模型。何为稳健？一般来说有两种含义，一是对于参数扰动的稳定性，比如模型变成了 fw&#43;Δw(x) 后是否还能达到相近的效果？如果在动力学系统中，还要考虑模型最终是否能恢复到 fw(x)；二是对于输入扰动的稳定性，比如输入从 x 变成了 x&#43;Δx 后，fw(x&#43;Δx) 是否能给出相近的预测结果。
读者或许已经听说过深度学习模型存在“对抗攻击样本”，比如图片只改变一个像素就给出完全不一样的分类结果，这就是模型对输入过于敏感的案例。
L约束
所以，大多数时候我们都希望模型对输入扰动是不敏感的，这通常能提高模型的泛化性能。也就是说，我们希望 ||x1−x2|| 很小时：
也尽可能地小。当然，“尽可能”究竟是怎样，谁也说不准。于是 Lipschitz 提出了一个更具体的约束，那就是存在某个常数 C（它只与参数有关，与输入无关），使得下式恒成立
也就是说，希望整个模型被一个线性函数“控制”住。这便是 L 约束了。
**换言之，在这里我们认为满足 L 约束的模型才是一个好模型。**并且对于具体的模型，我们希望估算出 C(w) 的表达式，并且希望 C(w) 越小越好，越小意味着它对输入扰动越不敏感，泛化性越好。
神经网络 在这里我们对具体的神经网络进行分析，以观察神经网络在什么时候会满足 L 约束。
简单而言，我们考虑单层的全连接 f(Wx&#43;b)，这里的 f 是激活函数，而 W,b 则是参数矩阵/向量，这时候 (3) 变为：
让 x1,x2 充分接近，那么就可以将左边用一阶项近似，得到：
显然，要希望左边不超过右边，**∂f/∂x 这一项（每个元素）的绝对值必须不超过某个常数。这就要求我们要使用“导数有上下界”的激活函数，不过我们目前常用的激活函数，比如sigmoid、tanh、relu等，都满足这个条件。**假定激活函数的梯度已经有界，尤其是我们常用的 relu 激活函数来说这个界还是 1，因此 ∂f/∂x 这一项只带来一个常数，我们暂时忽略它，剩下来我们只需要考虑 ||W(x1−x2)||。
多层的神经网络可以逐步递归分析，从而最终还是单层的神经网络问题，而 CNN、RNN 等结构本质上还是特殊的全连接，所以照样可以用全连接的结果。因此，对于神经网络来说，问题变成了：如果下式恒成立，那么 C 的值可以是多少？
找出 C 的表达式后，我们就可以希望 C 尽可能小，从而给参数带来一个正则化项。"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" /><link rel="prev" href="https://shilongshen.github.io/convolutional-neural-network-architecture-for-geometric-matching/" /><link rel="next" href="https://shilongshen.github.io/___________________________________________________/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "谱归一化",
        "inLanguage": "zh-cn",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/shilongshen.github.io\/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96\/"
        },"genre": "posts","wordcount":  349 ,
        "url": "https:\/\/shilongshen.github.io\/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96\/","datePublished": "2020-11-15T13:26:17+08:00","dateModified": "2020-11-15T13:26:17+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "shilongshen"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="我的个人博客">首页</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="我的个人博客">首页</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">谱归一化</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-11-15">2020-11-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;349 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-lipschitz定义">1. Lipschitz定义</a></li>
    <li><a href="#2-lipschitz常数">2. Lipschitz常数</a></li>
    <li><a href="#3深度学习中的lipschitz约束泛化与生成模型">3.深度学习中的Lipschitz约束：泛化与生成模型</a>
      <ul>
        <li><a href="#l约束与泛化">L约束与泛化</a></li>
        <li><a href="#神经网络">神经网络</a></li>
        <li><a href="#矩阵范数">矩阵范数</a>
          <ul>
            <li><a href="#谱范数">谱范数</a></li>
            <li><a href="#生成模型">生成模型</a>
              <ul>
                <li><a href="#wgan">WGAN</a></li>
                <li><a href="#梯度惩罚">梯度惩罚</a></li>
                <li><a href="#谱归一化">谱归一化</a></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="1-lipschitz定义">1. Lipschitz定义</h1>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://img-blog.csdnimg.cn/20190923152718681.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3,size_16,color_FFFFFF,t_70"
        data-srcset="https://img-blog.csdnimg.cn/20190923152718681.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70, https://img-blog.csdnimg.cn/20190923152718681.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70 1.5x, https://img-blog.csdnimg.cn/20190923152718681.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70 2x"
        data-sizes="auto"
        alt="https://img-blog.csdnimg.cn/20190923152718681.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3,size_16,color_FFFFFF,t_70"
        title="img" /></p>
<h1 id="2-lipschitz常数">2. Lipschitz常数</h1>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://img-blog.csdnimg.cn/20190923152819226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3,size_16,color_FFFFFF,t_70"
        data-srcset="https://img-blog.csdnimg.cn/20190923152819226.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70, https://img-blog.csdnimg.cn/20190923152819226.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70 1.5x, https://img-blog.csdnimg.cn/20190923152819226.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3%2csize_16%2ccolor_FFFFFF%2ct_70 2x"
        data-sizes="auto"
        alt="https://img-blog.csdnimg.cn/20190923152819226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2MDA0Mzg3,size_16,color_FFFFFF,t_70"
        title="img" /></p>
<h1 id="3深度学习中的lipschitz约束泛化与生成模型">3.深度学习中的Lipschitz约束：泛化与生成模型</h1>
<h2 id="l约束与泛化">L约束与泛化</h2>
<p><strong>扰动敏感</strong></p>
<p>记输入为 x，输出为 y，模型为 f，模型参数为 w，记为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYTlhMTlhOS05YWJhLTRjYTAtYjMzYi1hODg2MjExY2QzYzMvMTUzOTY4NjM3MTM1NS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYTlhMTlhOS05YWJhLTRjYTAtYjMzYi1hODg2MjExY2QzYzMvMTUzOTY4NjM3MTM1NS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYTlhMTlhOS05YWJhLTRjYTAtYjMzYi1hODg2MjExY2QzYzMvMTUzOTY4NjM3MTM1NS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYTlhMTlhOS05YWJhLTRjYTAtYjMzYi1hODg2MjExY2QzYzMvMTUzOTY4NjM3MTM1NS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYTlhMTlhOS05YWJhLTRjYTAtYjMzYi1hODg2MjExY2QzYzMvMTUzOTY4NjM3MTM1NS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>很多时候，我们希望得到一个“稳健”的模型。何为稳健？一般来说有两种含义，<strong>一是对于参数扰动的稳定性</strong>，比如模型变成了 fw+Δw(x) 后是否还能达到相近的效果？如果在动力学系统中，还要考虑模型最终是否能恢复到 fw(x)；<strong>二是对于输入扰动的稳定性</strong>，比如输入从 x 变成了 x+Δx 后，fw(x+Δx) 是否能给出相近的预测结果。</p>
<p>读者或许已经听说过深度学习模型存在“对抗攻击样本”，比如图片只改变一个像素就给出完全不一样的分类结果，这就是模型对输入过于敏感的案例。</p>
<p><strong>L约束</strong></p>
<p>所以，大多数时候我们都希望模型对输入扰动是不敏感的，这通常能提高模型的泛化性能。也就是说，我们希望 ||x1−x2|| 很小时：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNGNhNTQ2Yy1hNGExLTQwM2UtODY0YS02NTFmZjQ0ZDEzNTIvMTUzOTY4NjM3MTc4MS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNGNhNTQ2Yy1hNGExLTQwM2UtODY0YS02NTFmZjQ0ZDEzNTIvMTUzOTY4NjM3MTc4MS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNGNhNTQ2Yy1hNGExLTQwM2UtODY0YS02NTFmZjQ0ZDEzNTIvMTUzOTY4NjM3MTc4MS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNGNhNTQ2Yy1hNGExLTQwM2UtODY0YS02NTFmZjQ0ZDEzNTIvMTUzOTY4NjM3MTc4MS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNGNhNTQ2Yy1hNGExLTQwM2UtODY0YS02NTFmZjQ0ZDEzNTIvMTUzOTY4NjM3MTc4MS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>也尽可能地小。当然，“尽可能”究竟是怎样，谁也说不准。<strong>于是 Lipschitz 提出了一个更具体的约束，那就是存在某个常数 C（它只与参数有关，与输入无关），使得下式恒成立</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YzEzMTcxNy1jYTc3LTRjMmUtOGNjNi1jZDE0YmU1OTZiODkvMTUzOTY4NjM3MTU1OS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YzEzMTcxNy1jYTc3LTRjMmUtOGNjNi1jZDE0YmU1OTZiODkvMTUzOTY4NjM3MTU1OS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YzEzMTcxNy1jYTc3LTRjMmUtOGNjNi1jZDE0YmU1OTZiODkvMTUzOTY4NjM3MTU1OS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YzEzMTcxNy1jYTc3LTRjMmUtOGNjNi1jZDE0YmU1OTZiODkvMTUzOTY4NjM3MTU1OS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YzEzMTcxNy1jYTc3LTRjMmUtOGNjNi1jZDE0YmU1OTZiODkvMTUzOTY4NjM3MTU1OS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>也就是说，希望整个模型被一个线性函数“控制”住。<strong>这便是 L 约束了。</strong></p>
<p>**换言之，在这里我们认为满足 L 约束的模型才是一个好模型。**并且对于具体的模型，我们希望估算出 C(w) 的表达式，并且希望 C(w) 越小越好，越小意味着它对输入扰动越不敏感，泛化性越好。</p>
<h2 id="神经网络">神经网络</h2>
<p>在这里我们对具体的神经网络进行分析，以观察神经网络在什么时候会满足 L 约束。</p>
<p>简单而言，我们考虑单层的全连接 f(Wx+b)，这里的 f 是激活函数，而 W,b 则是参数矩阵/向量，这时候 (3) 变为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hYTliODI0NC1hY2U5LTRhZjAtYjdkYS0yNDkyOTc4M2EzM2IvMTUzOTY4NjM3MTUwNy5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hYTliODI0NC1hY2U5LTRhZjAtYjdkYS0yNDkyOTc4M2EzM2IvMTUzOTY4NjM3MTUwNy5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hYTliODI0NC1hY2U5LTRhZjAtYjdkYS0yNDkyOTc4M2EzM2IvMTUzOTY4NjM3MTUwNy5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hYTliODI0NC1hY2U5LTRhZjAtYjdkYS0yNDkyOTc4M2EzM2IvMTUzOTY4NjM3MTUwNy5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hYTliODI0NC1hY2U5LTRhZjAtYjdkYS0yNDkyOTc4M2EzM2IvMTUzOTY4NjM3MTUwNy5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>让 x1,x2 充分接近，那么就可以将左边用一阶项近似，得到：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82OTZkNzFhNy1mODViLTRkYTQtODE1Yy1jOTM1MWRmYjNlZDQvMTUzOTY4NjM3MTY1My5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82OTZkNzFhNy1mODViLTRkYTQtODE1Yy1jOTM1MWRmYjNlZDQvMTUzOTY4NjM3MTY1My5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82OTZkNzFhNy1mODViLTRkYTQtODE1Yy1jOTM1MWRmYjNlZDQvMTUzOTY4NjM3MTY1My5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82OTZkNzFhNy1mODViLTRkYTQtODE1Yy1jOTM1MWRmYjNlZDQvMTUzOTY4NjM3MTY1My5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82OTZkNzFhNy1mODViLTRkYTQtODE1Yy1jOTM1MWRmYjNlZDQvMTUzOTY4NjM3MTY1My5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>显然，要希望左边不超过右边，**∂f/∂x 这一项（每个元素）的绝对值必须不超过某个常数。这就要求我们要使用“导数有上下界”的激活函数，不过我们目前常用的激活函数，比如sigmoid、tanh、relu等，都满足这个条件。**假定激活函数的梯度已经有界，尤其是我们常用的 relu 激活函数来说这个界还是 1，因此 ∂f/∂x 这一项只带来一个常数，我们暂时忽略它，剩下来我们只需要考虑 ||W(x1−x2)||。</p>
<p>多层的神经网络可以逐步递归分析，从而最终还是单层的神经网络问题，而 CNN、RNN 等结构本质上还是特殊的全连接，所以照样可以用全连接的结果。因此，对于神经网络来说，问题变成了：<strong>如果下式恒成立，那么 C 的值可以是多少？</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YjVkZGQ3My03ZDA5LTRhY2EtYmNiOS01MzBiMDRlOGY2ZDgvMTUzOTY4NjM3MTQ0NS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YjVkZGQ3My03ZDA5LTRhY2EtYmNiOS01MzBiMDRlOGY2ZDgvMTUzOTY4NjM3MTQ0NS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YjVkZGQ3My03ZDA5LTRhY2EtYmNiOS01MzBiMDRlOGY2ZDgvMTUzOTY4NjM3MTQ0NS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YjVkZGQ3My03ZDA5LTRhY2EtYmNiOS01MzBiMDRlOGY2ZDgvMTUzOTY4NjM3MTQ0NS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82YjVkZGQ3My03ZDA5LTRhY2EtYmNiOS01MzBiMDRlOGY2ZDgvMTUzOTY4NjM3MTQ0NS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p><strong>找出 C 的表达式后，我们就可以希望 C 尽可能小</strong>，从而给参数带来一个正则化项。</p>
<h2 id="矩阵范数">矩阵范数</h2>
<p><strong>定义</strong></p>
<p>其实到这里，我们已经将问题转化为了一个矩阵范数问题（矩阵范数的作用相当于向量的模长），它定义为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYjc0N2Q2My0zMzgxLTQ2NmUtYjEyOS03Y2UyN2ZmNmQ5YzMvMTUzOTY4NjM3MjM1MC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYjc0N2Q2My0zMzgxLTQ2NmUtYjEyOS03Y2UyN2ZmNmQ5YzMvMTUzOTY4NjM3MjM1MC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYjc0N2Q2My0zMzgxLTQ2NmUtYjEyOS03Y2UyN2ZmNmQ5YzMvMTUzOTY4NjM3MjM1MC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYjc0N2Q2My0zMzgxLTQ2NmUtYjEyOS03Y2UyN2ZmNmQ5YzMvMTUzOTY4NjM3MjM1MC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYjc0N2Q2My0zMzgxLTQ2NmUtYjEyOS03Y2UyN2ZmNmQ5YzMvMTUzOTY4NjM3MjM1MC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>如果 W 是一个方阵，那么该范数又称为“谱范数”、“谱半径”等，在本文中就算它不是方阵我们也叫它“谱范数（Spectral  Norm）”好了。注意 ||Wx|| 和 ||x||  都是指向量的范数，就是普通的向量模长。而左边的矩阵的范数我们本来没有明确定义的，但通过右边的向量模型的极限定义出来的，所以这类矩阵范数称为“由向量范数诱导出来的矩阵范数”。</p>
<p>好了，文绉绉的概念就不多说了，有了向量范数的概念之后，我们就有：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xNjJiOWUzZi03NzRiLTQxNGYtYjY1Zi1hYTMwOTM5ODgxNjYvMTUzOTY4NjM3MTgzMi5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xNjJiOWUzZi03NzRiLTQxNGYtYjY1Zi1hYTMwOTM5ODgxNjYvMTUzOTY4NjM3MTgzMi5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xNjJiOWUzZi03NzRiLTQxNGYtYjY1Zi1hYTMwOTM5ODgxNjYvMTUzOTY4NjM3MTgzMi5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xNjJiOWUzZi03NzRiLTQxNGYtYjY1Zi1hYTMwOTM5ODgxNjYvMTUzOTY4NjM3MTgzMi5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xNjJiOWUzZi03NzRiLTQxNGYtYjY1Zi1hYTMwOTM5ODgxNjYvMTUzOTY4NjM3MTgzMi5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>呃，其实也没做啥，就换了个记号而已，||W||2 等于多少我们还是没有搞出来。</p>
<p><strong>Frobenius范数</strong></p>
<p>其实谱范数 ||W||2 的准确概念和计算方法还是要用到比较多的线性代数的概念，我们暂时不研究它，而是先研究一个更加简单的范数：<strong>Frobenius 范数，简称 F 范数。</strong></p>
<p>这名字让人看着慌，其实定义特别简单，它就是：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zYWM5ZWZkNy1jZGE1LTRhYmYtOTc1NS1jYzc3MTQ4YmM4M2UvMTUzOTY4NjM3MTg4Ni5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zYWM5ZWZkNy1jZGE1LTRhYmYtOTc1NS1jYzc3MTQ4YmM4M2UvMTUzOTY4NjM3MTg4Ni5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zYWM5ZWZkNy1jZGE1LTRhYmYtOTc1NS1jYzc3MTQ4YmM4M2UvMTUzOTY4NjM3MTg4Ni5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zYWM5ZWZkNy1jZGE1LTRhYmYtOTc1NS1jYzc3MTQ4YmM4M2UvMTUzOTY4NjM3MTg4Ni5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zYWM5ZWZkNy1jZGE1LTRhYmYtOTc1NS1jYzc3MTQ4YmM4M2UvMTUzOTY4NjM3MTg4Ni5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>说白了，它就是直接把矩阵当成一个向量，然后求向量的欧氏模长。</p>
<p>简单通过柯西不等式，我们就能证明：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iODA0ZDYyZS1jM2RjLTQ0YjAtYWY4NS00NTRiNWZiMDlhZjgvMTUzOTY4NjM3NjcxNi5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iODA0ZDYyZS1jM2RjLTQ0YjAtYWY4NS00NTRiNWZiMDlhZjgvMTUzOTY4NjM3NjcxNi5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iODA0ZDYyZS1jM2RjLTQ0YjAtYWY4NS00NTRiNWZiMDlhZjgvMTUzOTY4NjM3NjcxNi5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iODA0ZDYyZS1jM2RjLTQ0YjAtYWY4NS00NTRiNWZiMDlhZjgvMTUzOTY4NjM3NjcxNi5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iODA0ZDYyZS1jM2RjLTQ0YjAtYWY4NS00NTRiNWZiMDlhZjgvMTUzOTY4NjM3NjcxNi5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>很明显 ||W||F 提供了 ||W||2 的一个上界，也就是说，你可以理解为 ||W||2 是式 (6) 中最准确的 C（所有满足式  (6) 的 C 中最小的那个），但如果你不大关心精准度，你直接可以取 C=||W||F，也能使得 (6) 成立，毕竟 ||W||F 容易计算。</p>
<p><strong>l2正则项</strong></p>
<p>前面已经说过，为了使神经网络尽可能好地满足L约束，我们应当希望 C=||W||2 尽可能小，我们可以把 C2  作为一个正则项加入到损失函数中。当然，我们还没有算出谱范数 ||W||2，但我们算出了一个更大的上界 ||W||F，那就先用着它吧，即 loss 为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYTJjZmNjOS1kYzJiLTQwNjAtYWQ3Mi0zNmI2OWE0MTllN2EvMTUzOTY4NjM3MjU0NS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYTJjZmNjOS1kYzJiLTQwNjAtYWQ3Mi0zNmI2OWE0MTllN2EvMTUzOTY4NjM3MjU0NS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYTJjZmNjOS1kYzJiLTQwNjAtYWQ3Mi0zNmI2OWE0MTllN2EvMTUzOTY4NjM3MjU0NS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYTJjZmNjOS1kYzJiLTQwNjAtYWQ3Mi0zNmI2OWE0MTllN2EvMTUzOTY4NjM3MjU0NS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYTJjZmNjOS1kYzJiLTQwNjAtYWQ3Mi0zNmI2OWE0MTllN2EvMTUzOTY4NjM3MjU0NS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>其中第一部分是指模型原来的 loss。我们再来回顾一下 ||W||F 的表达式，我们发现加入的正则项是：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZjQ3ZTM4Ny1lY2NiLTQ0ZGItOTM0OS0xNWNlOTExYzAyNWEvMTUzOTY4NjM3MjQ5My5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZjQ3ZTM4Ny1lY2NiLTQ0ZGItOTM0OS0xNWNlOTExYzAyNWEvMTUzOTY4NjM3MjQ5My5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZjQ3ZTM4Ny1lY2NiLTQ0ZGItOTM0OS0xNWNlOTExYzAyNWEvMTUzOTY4NjM3MjQ5My5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZjQ3ZTM4Ny1lY2NiLTQ0ZGItOTM0OS0xNWNlOTExYzAyNWEvMTUzOTY4NjM3MjQ5My5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZjQ3ZTM4Ny1lY2NiLTQ0ZGItOTM0OS0xNWNlOTExYzAyNWEvMTUzOTY4NjM3MjQ5My5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>这不就是 l2 正则化吗？</p>
<p>终于，捣鼓了一番，我们得到了一点回报：<strong>我们揭示了 l2 正则化（也称为 weight decay）与 L 约束的联系，表明 l2 正则化能使得模型更好地满足 L 约束，从而降低模型对输入扰动的敏感性，增强模型的泛化性能。</strong></p>
<h3 id="谱范数">谱范数</h3>
<p><strong>主特征根</strong></p>
<p>这部分我们来正式面对谱范数 ||W||2，这是线性代数的内容，比较理论化。</p>
<p>事实上，<strong>谱范数 ||W||2 等于<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zNzYzODIzMS05NDExLTQ3NDYtYjI1ZC1iODY2NGI5ZTdiMDcvMTUzOTY4NjM3Mjc2OS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zNzYzODIzMS05NDExLTQ3NDYtYjI1ZC1iODY2NGI5ZTdiMDcvMTUzOTY4NjM3Mjc2OS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zNzYzODIzMS05NDExLTQ3NDYtYjI1ZC1iODY2NGI5ZTdiMDcvMTUzOTY4NjM3Mjc2OS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zNzYzODIzMS05NDExLTQ3NDYtYjI1ZC1iODY2NGI5ZTdiMDcvMTUzOTY4NjM3Mjc2OS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8zNzYzODIzMS05NDExLTQ3NDYtYjI1ZC1iODY2NGI5ZTdiMDcvMTUzOTY4NjM3Mjc2OS5wbmc?x-oss-process=image/format,png"
        title="img" />的最大特征根（主特征根）的平方根</strong>，如果 W是方阵，那么||W||2 等于 W 的最大的特征根绝对值。</p>
<p>对于感兴趣理论证明的读者，这里提供一下证明的大概思路。根据定义 (7) 我们有：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci81MTJiMDRlMi0wNzgyLTQ2YTYtYjc1ZC1hMmMyNzg3ODViN2QvMTUzOTY4NjM3MzM2Mi5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci81MTJiMDRlMi0wNzgyLTQ2YTYtYjc1ZC1hMmMyNzg3ODViN2QvMTUzOTY4NjM3MzM2Mi5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci81MTJiMDRlMi0wNzgyLTQ2YTYtYjc1ZC1hMmMyNzg3ODViN2QvMTUzOTY4NjM3MzM2Mi5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci81MTJiMDRlMi0wNzgyLTQ2YTYtYjc1ZC1hMmMyNzg3ODViN2QvMTUzOTY4NjM3MzM2Mi5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci81MTJiMDRlMi0wNzgyLTQ2YTYtYjc1ZC1hMmMyNzg3ODViN2QvMTUzOTY4NjM3MzM2Mi5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>假设**<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ZTkxYTNjMC1mZmIyLTRmNzQtOTZmOC02NDg3NDJmODlmY2EvMTUzOTY4NjM3MjcyOC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ZTkxYTNjMC1mZmIyLTRmNzQtOTZmOC02NDg3NDJmODlmY2EvMTUzOTY4NjM3MjcyOC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ZTkxYTNjMC1mZmIyLTRmNzQtOTZmOC02NDg3NDJmODlmY2EvMTUzOTY4NjM3MjcyOC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ZTkxYTNjMC1mZmIyLTRmNzQtOTZmOC02NDg3NDJmODlmY2EvMTUzOTY4NjM3MjcyOC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ZTkxYTNjMC1mZmIyLTRmNzQtOTZmOC02NDg3NDJmODlmY2EvMTUzOTY4NjM3MjcyOC5wbmc?x-oss-process=image/format,png"
        title="img" />**对角化为diag(λ1,…,λn)，即<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mMTYyODEwYS03MTUzLTRlZWQtOTA4Yy02ZTY4NzhmNjNlNjMvMTUzOTY4NjM3MjY3OS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mMTYyODEwYS03MTUzLTRlZWQtOTA4Yy02ZTY4NzhmNjNlNjMvMTUzOTY4NjM3MjY3OS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mMTYyODEwYS03MTUzLTRlZWQtOTA4Yy02ZTY4NzhmNjNlNjMvMTUzOTY4NjM3MjY3OS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mMTYyODEwYS03MTUzLTRlZWQtOTA4Yy02ZTY4NzhmNjNlNjMvMTUzOTY4NjM3MjY3OS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mMTYyODEwYS03MTUzLTRlZWQtOTA4Yy02ZTY4NzhmNjNlNjMvMTUzOTY4NjM3MjY3OS5wbmc?x-oss-process=image/format,png"
        title="img" />，其中 λi 都是它的特征根，而且非负，而 U 是正交矩阵，由于正交矩阵与单位向量的积还是单位向量，那么：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82ZjZmYzNkMi04MGU2LTRjNzUtYjZiMC01NjUzZTJkNDYzZWYvMTUzOTY4NjM3MzYzNC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82ZjZmYzNkMi04MGU2LTRjNzUtYjZiMC01NjUzZTJkNDYzZWYvMTUzOTY4NjM3MzYzNC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82ZjZmYzNkMi04MGU2LTRjNzUtYjZiMC01NjUzZTJkNDYzZWYvMTUzOTY4NjM3MzYzNC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82ZjZmYzNkMi04MGU2LTRjNzUtYjZiMC01NjUzZTJkNDYzZWYvMTUzOTY4NjM3MzYzNC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82ZjZmYzNkMi04MGU2LTRjNzUtYjZiMC01NjUzZTJkNDYzZWYvMTUzOTY4NjM3MzYzNC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>所以<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NDFjYzdlNy1iNWUxLTRhOTAtODAxMS02OTdhZjVkYWEyZjAvMTUzOTY4NjM3MzY3OC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NDFjYzdlNy1iNWUxLTRhOTAtODAxMS02OTdhZjVkYWEyZjAvMTUzOTY4NjM3MzY3OC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NDFjYzdlNy1iNWUxLTRhOTAtODAxMS02OTdhZjVkYWEyZjAvMTUzOTY4NjM3MzY3OC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NDFjYzdlNy1iNWUxLTRhOTAtODAxMS02OTdhZjVkYWEyZjAvMTUzOTY4NjM3MzY3OC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NDFjYzdlNy1iNWUxLTRhOTAtODAxMS02OTdhZjVkYWEyZjAvMTUzOTY4NjM3MzY3OC5wbmc?x-oss-process=image/format,png"
        title="img" />等于**<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84NjZhODYwOC05OTcyLTQwYTMtODdlZC1hYjQwZWJkZDRjNDAvMTUzOTY4NjM3MjgwOS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84NjZhODYwOC05OTcyLTQwYTMtODdlZC1hYjQwZWJkZDRjNDAvMTUzOTY4NjM3MjgwOS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84NjZhODYwOC05OTcyLTQwYTMtODdlZC1hYjQwZWJkZDRjNDAvMTUzOTY4NjM3MjgwOS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84NjZhODYwOC05OTcyLTQwYTMtODdlZC1hYjQwZWJkZDRjNDAvMTUzOTY4NjM3MjgwOS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84NjZhODYwOC05OTcyLTQwYTMtODdlZC1hYjQwZWJkZDRjNDAvMTUzOTY4NjM3MjgwOS5wbmc?x-oss-process=image/format,png"
        title="img" />**的最大特征根。</p>
<p><strong>幂迭代</strong></p>
<p>也许有读者开始不耐烦了：鬼愿意知道你是不是等于特征根呀，我关心的是怎么算这个鬼范数！</p>
<p>事实上，前面的内容虽然看起来茫然，但却是求 ‖W‖2 的基础。前一节告诉我们<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYwOTEyMC1hNGZiLTQzZGMtYWRhNC00ZDA5ZDVhZjI5M2YvMTUzOTY4NjM3MzczOC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYwOTEyMC1hNGZiLTQzZGMtYWRhNC00ZDA5ZDVhZjI5M2YvMTUzOTY4NjM3MzczOC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYwOTEyMC1hNGZiLTQzZGMtYWRhNC00ZDA5ZDVhZjI5M2YvMTUzOTY4NjM3MzczOC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYwOTEyMC1hNGZiLTQzZGMtYWRhNC00ZDA5ZDVhZjI5M2YvMTUzOTY4NjM3MzczOC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYwOTEyMC1hNGZiLTQzZGMtYWRhNC00ZDA5ZDVhZjI5M2YvMTUzOTY4NjM3MzczOC5wbmc?x-oss-process=image/format,png"
        title="img" />就是**<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xMWM2ZGFkMS0xNWI4LTQ0MDgtOTc2Ni1iY2QzNDc4M2EwNTkvMTUzOTY4NjM3Mjg0OC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xMWM2ZGFkMS0xNWI4LTQ0MDgtOTc2Ni1iY2QzNDc4M2EwNTkvMTUzOTY4NjM3Mjg0OC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xMWM2ZGFkMS0xNWI4LTQ0MDgtOTc2Ni1iY2QzNDc4M2EwNTkvMTUzOTY4NjM3Mjg0OC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xMWM2ZGFkMS0xNWI4LTQ0MDgtOTc2Ni1iY2QzNDc4M2EwNTkvMTUzOTY4NjM3Mjg0OC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8xMWM2ZGFkMS0xNWI4LTQ0MDgtOTc2Ni1iY2QzNDc4M2EwNTkvMTUzOTY4NjM3Mjg0OC5wbmc?x-oss-process=image/format,png"
        title="img" />**的最大特征根，所以问题变成了求**<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hNTU0YTI5ZS1kY2ZjLTRmZGQtOWVlNS0zMzM0Y2ViMTFlMmIvMTUzOTY4NjM3Mjg5My5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hNTU0YTI5ZS1kY2ZjLTRmZGQtOWVlNS0zMzM0Y2ViMTFlMmIvMTUzOTY4NjM3Mjg5My5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hNTU0YTI5ZS1kY2ZjLTRmZGQtOWVlNS0zMzM0Y2ViMTFlMmIvMTUzOTY4NjM3Mjg5My5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hNTU0YTI5ZS1kY2ZjLTRmZGQtOWVlNS0zMzM0Y2ViMTFlMmIvMTUzOTY4NjM3Mjg5My5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hNTU0YTI5ZS1kY2ZjLTRmZGQtOWVlNS0zMzM0Y2ViMTFlMmIvMTUzOTY4NjM3Mjg5My5wbmc?x-oss-process=image/format,png"
        title="img" />**的最大特征根，这可以通过**“幂迭代”法** [3] 来解决。</p>
<p>所谓“幂迭代”，就是通过下面的迭代格式：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OGRlMDYxOC0wMzMxLTQyOTYtOWU0Mi1lNzUyNDYzODc1OTcvMTUzOTY4NjM3MzU0MC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OGRlMDYxOC0wMzMxLTQyOTYtOWU0Mi1lNzUyNDYzODc1OTcvMTUzOTY4NjM3MzU0MC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OGRlMDYxOC0wMzMxLTQyOTYtOWU0Mi1lNzUyNDYzODc1OTcvMTUzOTY4NjM3MzU0MC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OGRlMDYxOC0wMzMxLTQyOTYtOWU0Mi1lNzUyNDYzODc1OTcvMTUzOTY4NjM3MzU0MC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OGRlMDYxOC0wMzMxLTQyOTYtOWU0Mi1lNzUyNDYzODc1OTcvMTUzOTY4NjM3MzU0MC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>迭代若干次后，最后通过：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NWQ1ZGI0Zi04ZGM4LTQ3ZDEtYTE0Ny03ZmE1YTgyNDAzODYvMTUzOTY4NjM3NDQyNC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NWQ1ZGI0Zi04ZGM4LTQ3ZDEtYTE0Ny03ZmE1YTgyNDAzODYvMTUzOTY4NjM3NDQyNC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NWQ1ZGI0Zi04ZGM4LTQ3ZDEtYTE0Ny03ZmE1YTgyNDAzODYvMTUzOTY4NjM3NDQyNC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NWQ1ZGI0Zi04ZGM4LTQ3ZDEtYTE0Ny03ZmE1YTgyNDAzODYvMTUzOTY4NjM3NDQyNC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci82NWQ1ZGI0Zi04ZGM4LTQ3ZDEtYTE0Ny03ZmE1YTgyNDAzODYvMTUzOTY4NjM3NDQyNC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>得到范数（也就是得到最大的特征根的近似值）。也可以等价改写为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jNjY2YzgzMi1lYzIxLTRhYjctYjBlMy1kM2E5ZGQyOTkxMzIvMTUzOTY4NjM3NDIzMS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jNjY2YzgzMi1lYzIxLTRhYjctYjBlMy1kM2E5ZGQyOTkxMzIvMTUzOTY4NjM3NDIzMS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jNjY2YzgzMi1lYzIxLTRhYjctYjBlMy1kM2E5ZGQyOTkxMzIvMTUzOTY4NjM3NDIzMS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jNjY2YzgzMi1lYzIxLTRhYjctYjBlMy1kM2E5ZGQyOTkxMzIvMTUzOTY4NjM3NDIzMS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jNjY2YzgzMi1lYzIxLTRhYjctYjBlMy1kM2E5ZGQyOTkxMzIvMTUzOTY4NjM3NDIzMS5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>这样，初始化 u,v 后（可以用全 1 向量初始化），就可以迭代若干次得到 u,v，然后代入<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mYjkyZDY4ZS1lYzRkLTQ0YTItYjYyNC1iYzBkZjM3NzE4MmMvMTUzOTY4NjM3NDQ3MC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mYjkyZDY4ZS1lYzRkLTQ0YTItYjYyNC1iYzBkZjM3NzE4MmMvMTUzOTY4NjM3NDQ3MC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mYjkyZDY4ZS1lYzRkLTQ0YTItYjYyNC1iYzBkZjM3NzE4MmMvMTUzOTY4NjM3NDQ3MC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mYjkyZDY4ZS1lYzRkLTQ0YTItYjYyNC1iYzBkZjM3NzE4MmMvMTUzOTY4NjM3NDQ3MC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9mYjkyZDY4ZS1lYzRkLTQ0YTItYjYyNC1iYzBkZjM3NzE4MmMvMTUzOTY4NjM3NDQ3MC5wbmc?x-oss-process=image/format,png"
        title="img" />算得 ‖W‖2 的近似值。</p>
<p>对证明感兴趣的读者，这里照样提供一个简单的证明表明为什么这样的迭代会有效。</p>
<p>记，初始化为<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYzZTYyMi00YzM0LTQxNzgtYmMwNC1lZDE3ZjBiMDUyNjgvMTUzOTY4NjM3NTAwNC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYzZTYyMi00YzM0LTQxNzgtYmMwNC1lZDE3ZjBiMDUyNjgvMTUzOTY4NjM3NTAwNC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYzZTYyMi00YzM0LTQxNzgtYmMwNC1lZDE3ZjBiMDUyNjgvMTUzOTY4NjM3NTAwNC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYzZTYyMi00YzM0LTQxNzgtYmMwNC1lZDE3ZjBiMDUyNjgvMTUzOTY4NjM3NTAwNC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84ZjYzZTYyMi00YzM0LTQxNzgtYmMwNC1lZDE3ZjBiMDUyNjgvMTUzOTY4NjM3NTAwNC5wbmc?x-oss-process=image/format,png"
        title="img" />，同样假设 A 可对角化，并且假设 A 的各个特征根 λ1,…,λn 中，最大的特征根严格大于其余的特征根（不满足这个条件意味着最大的特征根是重根，讨论起来有点复杂，需要请读者查找专业证明，这里仅仅抛砖引玉。</p>
<p>当然，从数值计算的角度，几乎没有两个人是完全相等的，因此可以认为重根的情况在实验中不会出现。），那么 A 的各个特征向量 η1,…,ηn 构成完备的基底，所以我们可以设：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hZWE5ZjdkOC05Y2ZjLTQ5YjMtYmViZS0xYThmMWFkMjVlMjYvMTUzOTY4NjM3NTE4Mi5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hZWE5ZjdkOC05Y2ZjLTQ5YjMtYmViZS0xYThmMWFkMjVlMjYvMTUzOTY4NjM3NTE4Mi5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hZWE5ZjdkOC05Y2ZjLTQ5YjMtYmViZS0xYThmMWFkMjVlMjYvMTUzOTY4NjM3NTE4Mi5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hZWE5ZjdkOC05Y2ZjLTQ5YjMtYmViZS0xYThmMWFkMjVlMjYvMTUzOTY4NjM3NTE4Mi5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9hZWE5ZjdkOC05Y2ZjLTQ5YjMtYmViZS0xYThmMWFkMjVlMjYvMTUzOTY4NjM3NTE4Mi5wbmc?x-oss-process=image/format,png"
        title="img" />每次的迭代是 Au/‖Au‖，其中分母只改变模长，我们留到最后再执行，只看 A 的重复作用：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYmQ4OTc3NS1iOTM1LTQ0ZWItOThlZC00ZTQyNDBjZjAzNWIvMTUzOTY4NjM3NTMyNC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYmQ4OTc3NS1iOTM1LTQ0ZWItOThlZC00ZTQyNDBjZjAzNWIvMTUzOTY4NjM3NTMyNC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYmQ4OTc3NS1iOTM1LTQ0ZWItOThlZC00ZTQyNDBjZjAzNWIvMTUzOTY4NjM3NTMyNC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYmQ4OTc3NS1iOTM1LTQ0ZWItOThlZC00ZTQyNDBjZjAzNWIvMTUzOTY4NjM3NTMyNC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kYmQ4OTc3NS1iOTM1LTQ0ZWItOThlZC00ZTQyNDBjZjAzNWIvMTUzOTY4NjM3NTMyNC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>注意对于特征向量有 Aη=λη，从而：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYThjNGMxZS0zODk5LTQ1Y2EtOTRjNC0yNjg4YWI5YzExNDgvMTUzOTY4NjM3NDgxMC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYThjNGMxZS0zODk5LTQ1Y2EtOTRjNC0yNjg4YWI5YzExNDgvMTUzOTY4NjM3NDgxMC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYThjNGMxZS0zODk5LTQ1Y2EtOTRjNC0yNjg4YWI5YzExNDgvMTUzOTY4NjM3NDgxMC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYThjNGMxZS0zODk5LTQ1Y2EtOTRjNC0yNjg4YWI5YzExNDgvMTUzOTY4NjM3NDgxMC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9iYThjNGMxZS0zODk5LTQ1Y2EtOTRjNC0yNjg4YWI5YzExNDgvMTUzOTY4NjM3NDgxMC5wbmc?x-oss-process=image/format,png"
        title="img" />不失一般性设 λ1 为最大的特征值，那么：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80OGE3OWZjNS0wN2ZhLTRiZTUtOWM5Yy0zYjEyMjA0YjNhYTMvMTUzOTY4NjM3NTQxNy5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80OGE3OWZjNS0wN2ZhLTRiZTUtOWM5Yy0zYjEyMjA0YjNhYTMvMTUzOTY4NjM3NTQxNy5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80OGE3OWZjNS0wN2ZhLTRiZTUtOWM5Yy0zYjEyMjA0YjNhYTMvMTUzOTY4NjM3NTQxNy5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80OGE3OWZjNS0wN2ZhLTRiZTUtOWM5Yy0zYjEyMjA0YjNhYTMvMTUzOTY4NjM3NTQxNy5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80OGE3OWZjNS0wN2ZhLTRiZTUtOWM5Yy0zYjEyMjA0YjNhYTMvMTUzOTY4NjM3NTQxNy5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>根据假设 λ2/λ1,…,λn/λ1 都小于 1，所以 r→∞ 时它们都趋于零，或者说当 r 足够大时它们可以忽略，那么就有：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNzMwMmRhNy05ZGY3LTQ4NmYtODM2Ny1mYTcwMzAxOTY1M2QvMTUzOTY4NjM3NTY4OS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNzMwMmRhNy05ZGY3LTQ4NmYtODM2Ny1mYTcwMzAxOTY1M2QvMTUzOTY4NjM3NTY4OS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNzMwMmRhNy05ZGY3LTQ4NmYtODM2Ny1mYTcwMzAxOTY1M2QvMTUzOTY4NjM3NTY4OS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNzMwMmRhNy05ZGY3LTQ4NmYtODM2Ny1mYTcwMzAxOTY1M2QvMTUzOTY4NjM3NTY4OS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9kNzMwMmRhNy05ZGY3LTQ4NmYtODM2Ny1mYTcwMzAxOTY1M2QvMTUzOTY4NjM3NTY4OS5wbmc?x-oss-process=image/format,png"
        title="img" />先不管模长，这个结果表明当 r 足够大时，<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85NzQ2OTk0OC05YzMwLTQ2NTQtODY1Mi1iNWY4YWEyODE2MjQvMTUzOTY4NjM3NTQ2NS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85NzQ2OTk0OC05YzMwLTQ2NTQtODY1Mi1iNWY4YWEyODE2MjQvMTUzOTY4NjM3NTQ2NS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85NzQ2OTk0OC05YzMwLTQ2NTQtODY1Mi1iNWY4YWEyODE2MjQvMTUzOTY4NjM3NTQ2NS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85NzQ2OTk0OC05YzMwLTQ2NTQtODY1Mi1iNWY4YWEyODE2MjQvMTUzOTY4NjM3NTQ2NS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85NzQ2OTk0OC05YzMwLTQ2NTQtODY1Mi1iNWY4YWEyODE2MjQvMTUzOTY4NjM3NTQ2NS5wbmc?x-oss-process=image/format,png"
        title="img" />提供了最大的特征根对应的特征向量的近似方向，其实每一步的归一化只是为了防止溢出而已。这样一来<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8yZGNjMzkwYy1kODg2LTQxZTQtOGQ0Yi0zNmEzNWIyNDc3OGMvMTUzOTY4NjM3NTY0NC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8yZGNjMzkwYy1kODg2LTQxZTQtOGQ0Yi0zNmEzNWIyNDc3OGMvMTUzOTY4NjM3NTY0NC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8yZGNjMzkwYy1kODg2LTQxZTQtOGQ0Yi0zNmEzNWIyNDc3OGMvMTUzOTY4NjM3NTY0NC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8yZGNjMzkwYy1kODg2LTQxZTQtOGQ0Yi0zNmEzNWIyNDc3OGMvMTUzOTY4NjM3NTY0NC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8yZGNjMzkwYy1kODg2LTQxZTQtOGQ0Yi0zNmEzNWIyNDc3OGMvMTUzOTY4NjM3NTY0NC5wbmc?x-oss-process=image/format,png"
        title="img" />就是对应的单位特征向量，即：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZWE3NGIxMy0zODNmLTQ0NmUtYjk5Ni1mZWM4ZTBmMjk2ZjgvMTUzOTY4NjM3NTg4Ny5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZWE3NGIxMy0zODNmLTQ0NmUtYjk5Ni1mZWM4ZTBmMjk2ZjgvMTUzOTY4NjM3NTg4Ny5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZWE3NGIxMy0zODNmLTQ0NmUtYjk5Ni1mZWM4ZTBmMjk2ZjgvMTUzOTY4NjM3NTg4Ny5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZWE3NGIxMy0zODNmLTQ0NmUtYjk5Ni1mZWM4ZTBmMjk2ZjgvMTUzOTY4NjM3NTg4Ny5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci85ZWE3NGIxMy0zODNmLTQ0NmUtYjk5Ni1mZWM4ZTBmMjk2ZjgvMTUzOTY4NjM3NTg4Ny5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p>因此：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84MWQ4NjRiOC00NjRlLTRkYzgtYjAxMy1mYTViZjVjYWNmZTIvMTUzOTY4NjM3NTkzMS5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84MWQ4NjRiOC00NjRlLTRkYzgtYjAxMy1mYTViZjVjYWNmZTIvMTUzOTY4NjM3NTkzMS5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84MWQ4NjRiOC00NjRlLTRkYzgtYjAxMy1mYTViZjVjYWNmZTIvMTUzOTY4NjM3NTkzMS5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84MWQ4NjRiOC00NjRlLTRkYzgtYjAxMy1mYTViZjVjYWNmZTIvMTUzOTY4NjM3NTkzMS5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84MWQ4NjRiOC00NjRlLTRkYzgtYjAxMy1mYTViZjVjYWNmZTIvMTUzOTY4NjM3NTkzMS5wbmc?x-oss-process=image/format,png"
        title="img" />，这就求出了谱范数的平方。</p>
<p><strong>谱正则化</strong></p>
<p>前面我们已经表明了 Frobenius 范数与 l2 正则化的关系，而我们已经说明了  Frobenius 范数是一个更强（更粗糙）的条件，更准确的范数应该是谱范数。虽然谱范数没有  Frobenius 范数那么容易计算，但依然可以通过式 (15) 迭代几步来做近似。</p>
<p>所以，我们可以提出**“谱正则化（Spectral Norm Regularization）”**的概念，即把谱范数的平方作为额外的正则项，取代简单的 l2 正则项。即式 (11) 变为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wZjYwMTc2NC0wZTgzLTQ1OWYtYTg3Yy1jNTk5ODc0OTk2M2EvMTUzOTY4NjM3NjAwNC5wbmc?x-oss-process=image/format,png"
        data-srcset="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wZjYwMTc2NC0wZTgzLTQ1OWYtYTg3Yy1jNTk5ODc0OTk2M2EvMTUzOTY4NjM3NjAwNC5wbmc?x-oss-process=image/format%2cpng, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wZjYwMTc2NC0wZTgzLTQ1OWYtYTg3Yy1jNTk5ODc0OTk2M2EvMTUzOTY4NjM3NjAwNC5wbmc?x-oss-process=image/format%2cpng 1.5x, https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wZjYwMTc2NC0wZTgzLTQ1OWYtYTg3Yy1jNTk5ODc0OTk2M2EvMTUzOTY4NjM3NjAwNC5wbmc?x-oss-process=image/format%2cpng 2x"
        data-sizes="auto"
        alt="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wZjYwMTc2NC0wZTgzLTQ1OWYtYTg3Yy1jNTk5ODc0OTk2M2EvMTUzOTY4NjM3NjAwNC5wbmc?x-oss-process=image/format,png"
        title="img" /></p>
<p><em><strong>Spectral Norm Regularization for Improving the Generalizability of Deep Learning</strong></em> [1]一文已经做了多个实验，表明“谱正则化”在多个任务上都能提升模型性能</p>
<h3 id="生成模型">生成模型</h3>
<h4 id="wgan"><strong>WGAN</strong></h4>
<p>如果说在普通的监督训练模型中，L 约束只是起到了“锦上添花”的作用，那么在 WGAN 的判别器中，L 约束就是必不可少的关键一步了。因为 WGAN 的判别器的优化目标是：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/55d86993-84ed-4085-bc00-07f4d621149a/1539686376251.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/55d86993-84ed-4085-bc00-07f4d621149a/1539686376251.png, https://image.jiqizhixin.com/uploads/editor/55d86993-84ed-4085-bc00-07f4d621149a/1539686376251.png 1.5x, https://image.jiqizhixin.com/uploads/editor/55d86993-84ed-4085-bc00-07f4d621149a/1539686376251.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/55d86993-84ed-4085-bc00-07f4d621149a/1539686376251.png"
        title="img" /></p>
<p>这里的 Pr,Pg 分别是真实分布和生成分布，|f|L=1 指的就是要满足特定的 L 约束 |f(x1)−f(x2)|≤‖x1−x2‖（那个 C=1）。所以上述目标的意思是，在所有满足这个L约束的函数中，挑出使得<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/1aa2020c-a8ac-4c20-92f3-5537ffba4dff/1539686376341.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/1aa2020c-a8ac-4c20-92f3-5537ffba4dff/1539686376341.png, https://image.jiqizhixin.com/uploads/editor/1aa2020c-a8ac-4c20-92f3-5537ffba4dff/1539686376341.png 1.5x, https://image.jiqizhixin.com/uploads/editor/1aa2020c-a8ac-4c20-92f3-5537ffba4dff/1539686376341.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/1aa2020c-a8ac-4c20-92f3-5537ffba4dff/1539686376341.png"
        title="img" />最大的那个 f，就是最理想的判别器。写成 loss 的形式就是：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/b391a4e5-3c5d-46c3-8771-298624b8dda7/1539686376837.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/b391a4e5-3c5d-46c3-8771-298624b8dda7/1539686376837.png, https://image.jiqizhixin.com/uploads/editor/b391a4e5-3c5d-46c3-8771-298624b8dda7/1539686376837.png 1.5x, https://image.jiqizhixin.com/uploads/editor/b391a4e5-3c5d-46c3-8771-298624b8dda7/1539686376837.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/b391a4e5-3c5d-46c3-8771-298624b8dda7/1539686376837.png"
        title="img" /></p>
<h4 id="梯度惩罚"><strong>梯度惩罚</strong></h4>
<p>目前比较有效的一种方案就是梯度惩罚，<strong>即 ‖f′(x)‖=1 是 |f|L=1 的一个充分条件</strong>，那么我把这一项加入到判别器的 loss 中作为惩罚项，即：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/ac119fe8-57f1-4044-b256-be0c7dbe8137/1539686376434.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/ac119fe8-57f1-4044-b256-be0c7dbe8137/1539686376434.png, https://image.jiqizhixin.com/uploads/editor/ac119fe8-57f1-4044-b256-be0c7dbe8137/1539686376434.png 1.5x, https://image.jiqizhixin.com/uploads/editor/ac119fe8-57f1-4044-b256-be0c7dbe8137/1539686376434.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/ac119fe8-57f1-4044-b256-be0c7dbe8137/1539686376434.png"
        title="img" /></p>
<p>事实上我觉得加个 relu(x)=max(x,0) 会更好：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/fc29cccc-9bb0-4d86-82f0-218fb2c7d7db/1539686376895.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/fc29cccc-9bb0-4d86-82f0-218fb2c7d7db/1539686376895.png, https://image.jiqizhixin.com/uploads/editor/fc29cccc-9bb0-4d86-82f0-218fb2c7d7db/1539686376895.png 1.5x, https://image.jiqizhixin.com/uploads/editor/fc29cccc-9bb0-4d86-82f0-218fb2c7d7db/1539686376895.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/fc29cccc-9bb0-4d86-82f0-218fb2c7d7db/1539686376895.png"
        title="img" /></p>
<p>其中<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/0dac7e7f-db0e-4e5a-a233-97d72b9bc4e0/1539686376777.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/0dac7e7f-db0e-4e5a-a233-97d72b9bc4e0/1539686376777.png, https://image.jiqizhixin.com/uploads/editor/0dac7e7f-db0e-4e5a-a233-97d72b9bc4e0/1539686376777.png 1.5x, https://image.jiqizhixin.com/uploads/editor/0dac7e7f-db0e-4e5a-a233-97d72b9bc4e0/1539686376777.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/0dac7e7f-db0e-4e5a-a233-97d72b9bc4e0/1539686376777.png"
        title="img" />采用随机插值的方式：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://image.jiqizhixin.com/uploads/editor/88b4a286-b6cb-4097-8431-4abe7cc13e76/1539686377231.png"
        data-srcset="https://image.jiqizhixin.com/uploads/editor/88b4a286-b6cb-4097-8431-4abe7cc13e76/1539686377231.png, https://image.jiqizhixin.com/uploads/editor/88b4a286-b6cb-4097-8431-4abe7cc13e76/1539686377231.png 1.5x, https://image.jiqizhixin.com/uploads/editor/88b4a286-b6cb-4097-8431-4abe7cc13e76/1539686377231.png 2x"
        data-sizes="auto"
        alt="https://image.jiqizhixin.com/uploads/editor/88b4a286-b6cb-4097-8431-4abe7cc13e76/1539686377231.png"
        title="img" /></p>
<p>梯度惩罚不能保证 ‖f′(x)‖=1，但是直觉上它会在 1 附近浮动，所以 |f|L 理论上也在 1 附近浮动，从而近似达到 L 约束。</p>
<p>这种方案在很多情况下都已经 work 得比较好了，但是在真实样本的类别数比较多的时候却比较差（尤其是条件生成）。</p>
<p>**问题就出在随机插值上：**原则上来说，L  约束要在整个空间满足才行，但是通过线性插值的梯度惩罚只能保证在一小块空间满足。如果这一小块空间刚好差不多就是真实样本和生成样本之间的空间，那勉勉强强也就够用了，但是如果类别数比较多，不同的类别进行插值，往往不知道插到哪里去了，导致该满足 L 条件的地方不满足，因此判别器就失灵了。</p>
<p>思考：梯度惩罚能不能直接用作有监督的模型的正则项呢？有兴趣的读者可以试验一下。</p>
<h4 id="谱归一化"><strong>谱归一化</strong></h4>
<p>梯度惩罚的问题在于它只是一个惩罚，只能在局部生效。真正妙的方案是构造法：构建特殊的 f，使得不管 f 里边的参数是什么，f 都满足 L 约束。</p>
<p>事实上，WGAN 首次提出时用的是参数裁剪——将所有参数的绝对值裁剪到不超过某个常数，这样一来参数的 Frobenius 范数不会超过某个常数，从而 |f|L 不会超过某个常数，虽然没有准确地实现 |f|L=1，但这只会让 loss 放大常数倍，因此不影响优化结果。参数裁剪就是一种构造法，这不过这种构造法对优化并不友好。</p>
<p>简单来看，这种裁剪的方案优化空间有很大，比如改为将所有参数的 Frobenius 范数裁剪到不超过某个常数，这样模型的灵活性比直接参数裁剪要好。如果觉得裁剪太粗暴，换成参数惩罚也是可以的，即对所有范数超过 Frobenius 范数的参数施加一个大惩罚，我也试验过，基本有效，但是收敛速度比较慢。</p>
<p>然而，上面这些方案都只是某种近似，现在我们已经有了谱范数，那么可以用最精准的方案了：<strong>将 f 中所有的参数都替换为 w/‖w‖2</strong>。这就是谱归一化（Spectral Normalization），在<em><strong>Spectral Normalization for Generative Adversarial Networks</strong></em> [2] 一文中被提出并实验。</p>
<p>这样一来，如果 f 所用的激活函数的导数绝对值都不超过 1，那么我们就有 |f|L≤1，从而用最精准的方案实现了所需要的 L 约束。</p>
<p>注：“激活函数的导数绝对值都不超过 1”，这个通常都能满足，但是如果判别模型使用了残差结构，则激活函数相当于是 x+relu(Wx+b)，这时候它的导数就不一定不超过 1 了。但不管怎样，它会不超过一个常数，因此不影响优化结果。</p>
<p>我自己尝试过在 WGAN 中使用谱归一化（不加梯度惩罚，参考代码见后面），**发现最终的收敛速度（达到同样效果所需要的 epoch）比 WGAN-GP 还要快，效果还要更好一些。**而且，还有一个影响速度的原因：就是每个 epoch 的运行时间，梯度惩罚会比用谱归一化要长，因为用了梯度惩罚后，在梯度下降的时候相当于要算二次梯度了，要执行整个前向过程两次，所以速度比较慢。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2020-11-15</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="Share on Myspace" data-sharer="myspace" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://shilongshen.github.io/%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96/" data-title="谱归一化"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/convolutional-neural-network-architecture-for-geometric-matching/" class="prev" rel="prev" title="Convolutional neural network architecture for geometric matching"><i class="fas fa-angle-left fa-fw"></i>Convolutional neural network architecture for geometric matching</a>
            <a href="/___________________________________________________/" class="next" rel="next" title="训练集、验证集和测试集的区别和联系">训练集、验证集和测试集的区别和联系<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.63.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">shilongshen</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
