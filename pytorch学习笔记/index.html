<!DOCTYPE html>
<html lang="zh-cn">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>pytorch学习笔记 - 我的个人博客</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="pytorch学习笔记" />
<meta property="og:description" content="tensor的一些性质
1 2 3 4 5  import torch x=torch.tensor(2.5) print(x.dtype) &gt;&gt;torch.float32#torch默认的数据类型为float32   数据类型包括：
1 2 3 4  #如何将tensor的数据类型转换？ #在Tensor后加 .long(), .int(), .float(), .double()等即可 x.half() &gt;&gt;tensor(2.5000, dtype=torch.float16)   1 2 3 4 5 6 7  #将numpy.ndarray转换为tensor: a=numpy.array([1,2,3]) a.dtype &gt;&gt;dtype(&#39;int32&#39;) t=torch.from_numpy(a) t &gt;&gt;tensor([1, 2, 3], dtype=torch.int32)   1 2 3 4 5  #torch.zeros_like(input)-&gt;生成一个与input相同size的零矩阵,同理还有torch.ones_like(input);torch.randn_like(input) input=torch.ones(2,3) torch.zeros_like(input) &gt;&gt;tensor([[0., 0., 0.], [0., 0., 0.]])   1 2 3 4 5 6  #torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" />
<meta property="og:image" content="https://shilongshen.github.io/logo.png"/>
<meta property="article:published_time" content="2020-11-15T13:26:17+08:00" />
<meta property="article:modified_time" content="2020-11-15T13:26:17+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://shilongshen.github.io/logo.png"/>

<meta name="twitter:title" content="pytorch学习笔记"/>
<meta name="twitter:description" content="tensor的一些性质
1 2 3 4 5  import torch x=torch.tensor(2.5) print(x.dtype) &gt;&gt;torch.float32#torch默认的数据类型为float32   数据类型包括：
1 2 3 4  #如何将tensor的数据类型转换？ #在Tensor后加 .long(), .int(), .float(), .double()等即可 x.half() &gt;&gt;tensor(2.5000, dtype=torch.float16)   1 2 3 4 5 6 7  #将numpy.ndarray转换为tensor: a=numpy.array([1,2,3]) a.dtype &gt;&gt;dtype(&#39;int32&#39;) t=torch.from_numpy(a) t &gt;&gt;tensor([1, 2, 3], dtype=torch.int32)   1 2 3 4 5  #torch.zeros_like(input)-&gt;生成一个与input相同size的零矩阵,同理还有torch.ones_like(input);torch.randn_like(input) input=torch.ones(2,3) torch.zeros_like(input) &gt;&gt;tensor([[0., 0., 0.], [0., 0., 0.]])   1 2 3 4 5 6  #torch."/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" /><link rel="prev" href="https://shilongshen.github.io/semantic-hierarchy-emerges-in-deep-generative-representations-for-scene-synthesis/" /><link rel="next" href="https://shilongshen.github.io/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "pytorch学习笔记",
        "inLanguage": "zh-cn",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/shilongshen.github.io\/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0\/"
        },"genre": "posts","wordcount":  6186 ,
        "url": "https:\/\/shilongshen.github.io\/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0\/","datePublished": "2020-11-15T13:26:17+08:00","dateModified": "2020-11-15T13:26:17+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "shilongshen"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="我的个人博客">首页</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="我的个人博客">首页</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">pytorch学习笔记</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-11-15">2020-11-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;6186 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;30 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li>
              <ul>
                <li><a href="#pytorch-自动微分">pytorch 自动微分</a></li>
                <li><a href="#神经网络的构建1">神经网络的构建（1）</a></li>
                <li><a href="#神经网络的构建2">神经网络的构建（2）</a></li>
                <li><a href="#数据加载和处理1">数据加载和处理1</a></li>
                <li><a href="#数据的加载和处理2----自定义数据集类">数据的加载和处理2 &ndash; 自定义数据集类</a></li>
                <li><a href="#python中parseradd_argument的用法">Python中parser.add_argument()的用法</a></li>
                <li><a href="#pandas中loc和iloc函数用法">Pandas中loc和iloc函数用法</a></li>
                <li><a href="#__init__与__getitem__及__len__的使用">init()与__getitem__()及__len__()的使用</a></li>
                <li><a href="#pytorch-module里的children与modules的区别">pytorch Module里的children()与modules()的区别</a></li>
                <li><a href="#numpy中matrix和ndarray的区别">Numpy中matrix和ndarray的区别</a></li>
                <li><a href="#pytorch并行处理方法">pytorch并行处理方法</a></li>
                <li><a href="#matplotlib-画动态图以及pltion和pltioff的使用">matplotlib 画动态图以及plt.ion()和plt.ioff()的使用</a></li>
                <li><a href="#链表推导式">链表推导式</a></li>
                <li><a href="#iter函数和next函数">iter()函数和next()函数</a></li>
                <li><a href="#深拷贝和浅拷贝的区别">深拷贝和浅拷贝的区别</a></li>
                <li><a href="#学习率调整">学习率调整</a></li>
                <li><a href="#pytorch的初始化-torchnninit">pytorch的初始化 (torch.nn.init)</a></li>
                <li><a href="#pytorch使用tensorboard进行可视化">Pytorch使用tensorboard进行可视化</a></li>
                <li><a href="#pytorch使用visdom进行可视化">Pytorch使用visdom进行可视化</a></li>
                <li><a href="#gan中detach的作用">GAN中detach()的作用</a></li>
                <li><a href="#pytorch-中data和detach的区别和联系">pytorch 中.data和.detach的区别和联系</a></li>
                <li><a href="#pytorch保存模型等相关参数利用torchsave以及读取保存之后的文件">pytorch保存模型等相关参数，利用torch.save()，以及读取保存之后的文件</a></li>
                <li><a href="#pytorch中contiguous">pytorch中contiguous()</a></li>
                <li><a href="#pytorch中的t">pytorch中的.t()</a></li>
                <li><a href="#卷积中的填充方法">卷积中的填充方法</a></li>
                <li><a href="#棋盘效应">棋盘效应</a></li>
                <li><a href="#torchmean和torchmeandim0meandim1">torch.mean()和torch.mean(dim=0).mean(dim=1)</a></li>
                <li><a href="#矩阵乘法">矩阵乘法</a></li>
                <li><a href="#pytorch-模型训练时多卡负载不均衡gpu的0卡显存过高解决办法">pytorch 模型训练时多卡负载不均衡（GPU的0卡显存过高）解决办法</a></li>
                <li><a href="#pytorch张量数据索引切片与维度变换操作">Pytorch张量数据索引切片与维度变换操作</a></li>
                <li><a href="#python-hasattr-函数">Python hasattr() 函数</a></li>
                <li><a href="#python-getattr-函数">Python getattr() 函数</a></li>
                <li><a href="#python-setattr-函数">Python setattr() 函数</a></li>
                <li><a href="#python-lambda函数">python lambda函数</a></li>
                <li><a href="#python--filter-函数">Python  filter() 函数</a></li>
                <li><a href="#pytorch-typeerror-cannot-assign-torchfloattensor-as-parameter-weight">[pytorch] TypeError cannot assign torch.FloatTensor as parameter weight</a></li>
                <li><a href="#pytorch-中的-tensor--variable--parameter">Pytorch 中的 Tensor , Variable & Parameter</a></li>
                <li><a href="#pytorch自定义网络结构不进行参数初始化会怎样">pytorch自定义网络结构不进行参数初始化会怎样？</a></li>
                <li><a href="#pytorch中的item用法">pytorch中的item()用法</a></li>
                <li><a href="#pytorch中tensor与各种图像格式的相互转化">Pytorch中Tensor与各种图像格式的相互转化</a></li>
                <li><a href="#instancenorm2d-load-error-unexpected-running-stats-buffers-modelmodel1model2running_mean">InstanceNorm2d Load Error: Unexpected running stats buffer(s) “model.model.1.model.2.running_mean“</a></li>
                <li><a href="#pytorch中modelmodules和modelchildren的区别">pytorch中model.modules()和model.children()的区别</a></li>
                <li><a href="#如何使用预训练的vgg模型">如何使用预训练的VGG模型</a></li>
                <li><a href="#mask的使用">mask的使用</a></li>
                <li><a href="#importlibimport_module的使用">importlib.import_module()的使用</a></li>
                <li><a href="#__dict__的使用">__dict__的使用</a></li>
                <li><a href="#python中items的用法">Python中items()的用法</a></li>
                <li><a href="#staticmethod和classmethod的用法">@staticmethod和@classmethod的用法</a></li>
                <li><a href="#torchnnmodule和torchautogradfunction">torch.nn.Module和torch.autograd.Function</a></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>tensor的一些性质</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="c1">#torch默认的数据类型为float32</span>

</code></pre></td></tr></table>
</div>
</div><p>数据类型包括：</p>
<p><img src="https://gitee.com/shilongshen/image-bad/raw/master/20200702083022.png"  /></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#如何将tensor的数据类型转换？</span>
<span class="c1">#在Tensor后加 .long(), .int(), .float(), .double()等即可</span>
<span class="n">x</span><span class="o">.</span><span class="n">half</span><span class="p">(</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#将numpy.ndarray转换为tensor:</span>
    <span class="n">a</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="p">)</span>
    <span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
    <span class="o">&gt;&gt;</span><span class="n">dtype</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">int32</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">t</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">t</span>
    <span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.zeros_like(input)-&gt;生成一个与input相同size的零矩阵,同理还有torch.ones_like(input);torch.randn_like(input)</span>
<span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.arange() 和torch.range()的区别</span>
<span class="c1">#Note that arange generates values in [start; end), not [start; end].</span>
<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.full(size,fill_value)</span>
<span class="c1">#Returns a tensor of size size filled with fill_value.常用于GAN训练中的真假标签赋值</span>
<span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="p">,</span><span class="mf">3.1415</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">3.1415</span><span class="p">,</span> <span class="mf">3.1415</span><span class="p">,</span> <span class="mf">3.1415</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">3.1415</span><span class="p">,</span> <span class="mf">3.1415</span><span class="p">,</span> <span class="mf">3.1415</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="p">)</span><span class="p">,</span><span class="mf">3.14</span><span class="p">)</span><span class="c1">#注意size的表达形式（*，*）或者是（*，*，*）；不能使用（*）</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">3.1400</span><span class="p">,</span> <span class="mf">3.1400</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.cat(tensor,dim)</span>
<span class="c1">#将输入的张量在指定的维度上进行叠加</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>

<span class="c1">#torch.split(tensor,split_size,dim)</span>
<span class="c1">#将输入张量在指定维度上分为特定的份</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="o">-</span><span class="mf">1.0159</span><span class="p">,</span>  <span class="mf">0.2249</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0083</span><span class="p">,</span>  <span class="mf">0.4495</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5427</span><span class="p">,</span>  <span class="mf">0.0253</span><span class="p">,</span>  <span class="mf">0.3100</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8563</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span> <span class="mf">0.6721</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5261</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1104</span><span class="p">,</span>  <span class="mf">1.5312</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span> <span class="mf">0.4096</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4238</span><span class="p">,</span>  <span class="mf">0.5939</span><span class="p">,</span>  <span class="mf">0.2700</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">b</span>
<span class="o">&gt;&gt;</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="o">-</span><span class="mf">1.0159</span><span class="p">,</span>  <span class="mf">0.2249</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0083</span><span class="p">,</span>  <span class="mf">0.4495</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5427</span><span class="p">,</span>  <span class="mf">0.0253</span><span class="p">,</span>  <span class="mf">0.3100</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8563</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span> <span class="mf">0.6721</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5261</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1104</span><span class="p">,</span>  <span class="mf">1.5312</span><span class="p">]</span><span class="p">,</span>
        <span class="p">[</span> <span class="mf">0.4096</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4238</span><span class="p">,</span>  <span class="mf">0.5939</span><span class="p">,</span>  <span class="mf">0.2700</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>


</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.reshape(input,shape)</span>
<span class="c1">#将input重构为shape的形状</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span><span class="p">)</span>

<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="p">)</span><span class="p">)</span>
<span class="c1">#当某一维度为-1时，这一维度的形状不用管，根据input和另一维度计算形状。当另一维度的表达式没有时，就讲input按照“某一维度为-1”的维度排列input</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">16</span><span class="p">]</span><span class="p">)</span>

<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#torch.squeeze(input,dim)</span>
<span class="c1">#将输入张量维度为1的size去掉</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="c1">#如果不指定维度就会将所有维度为1的size去掉，如果加了维度，只会在指定维度上</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">)</span>
<span class="c1">#torch.unsqueeze(input,dim)</span>
<span class="c1">#在指定维度插入size=1</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch-自动微分">pytorch 自动微分</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#autograd 包是 PyTorch 中所有神经网络的核心.该 autograd 软件包为 Tensors 上的所有操作提供自动微分</span>
<span class="c1">#####动态计算图（DCG）-&gt;由autograd动态生成。这个图的叶节点是输入张量，根节点是输出张量。梯度是通过跟踪从根到叶的图形，并使用链式法则将每个梯度相乘来计算的。</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-</span><span class="o">&gt;</span> <span class="n">Tensor</span>
 
<span class="c1">#参数:</span>
<span class="c1">#    data： (array_like): tensor的初始值. 可以是列表，元组，numpy数组，标量等;</span>
<span class="c1">#    dtype： tensor元素的数据类型</span>
<span class="c1">#    device： 指定CPU或者是GPU设备，默认是None</span>
<span class="c1">#    requires_grad：是否可以求导，即求梯度，默认是False，即不可导的</span>
<span class="c1">#	注意data必须为浮点数才可导</span>
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png, https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/20200702113424.png" /></p>
<blockquote>
<p>data:输入数据</p>
<p>requires_grad：这个成员(如果为true)开始跟踪所有的操作历史，并形成一个用于梯度计算的后向图。</p>
<p>grad: <strong>grad保存梯度值</strong>。如果requires_grad 为False，它将持有一个None值。即使requires_grad 为真，它也将持有一个None值，除非从其他节点调用.backward()函数。例如，如果你对out关于x计算梯度，调用out.backward()，则x.grad的值为∂out/∂x。</p>
<p>grad_fn：这是用来计算梯度的向后函数。</p>
<p>在调用<code>backward()</code>时，只计算<code>requires_grad</code>和<code>is_leaf</code>同时为真的节点的梯度。</p>
</blockquote>
<p>当打开 <code>requires_grad = True</code>时，PyTorch将开始跟踪操作，并在每个步骤中存储梯度函数，如下所示：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png, https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/20200702161327.png" /></p>
<blockquote>
<p>Backward函数实际上是通过传递参数(默认情况下是1x1单位张量)来计算梯度的，它通过Backward图一直到每个叶节点，每个叶节点都可以从调用的根张量追溯到叶节点。然后将计算出的梯度存储在每个叶节点的.grad中。请记住，在正向传递过程中已经动态生成了后向图。backward函数仅使用已生成的图形计算梯度，并将其存储在叶节点中。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#参考：https://blog.csdn.net/qq_27825451/article/details/89393332</span>
<span class="c1">###求导的核心函数-torch.autograd.backward</span>
<span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1">#在pytorch中，默认是标量对标量求导或者是标量对向量求导</span>
<span class="c1">#向量对向量求导-&gt;当使用向量对向量求导时需要设置gradient的参数</span>
<span class="c1">#一个计算图在进行反向求导之后，为了节省内存，这个计算图就销毁了。 如果你想再次求导，就会报错。那怎么办呢？遇到#这种问题，我们可以通过设置 retain_graph=True 来保留计算图</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#标量对标量求导</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
 
<span class="c1">#判断x,y是否是可以求导的</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
 
<span class="c1">#求导，通过backward函数来实现</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>  
 
<span class="c1">#查看导数，也即所谓的梯度</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>


</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#标量对向量求导</span>
<span class="c1">#在深度学习中，损失函数一般为标量，即将所有项的损失相加。而输入一般为多维张量</span>
<span class="c1">#例:1：</span>
<span class="c1">#x-&gt;(1,3),w-&gt;(3,1)</span>
<span class="c1">#输出y是一个标量</span>
<span class="c1">#创建一个多元函数，即Y=XW+b=Y=x1*w1+x2*w2*x3*w3+b，x不可求导，W,b设置可求导</span>
<span class="n">X</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">]</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">W</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">]</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">Y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
 
 
<span class="c1">#判断每个tensor是否是可以求导的</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
 
 
<span class="c1">#求导，通过backward函数来实现</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>  
 
<span class="c1">#查看导数，也即所谓的梯度</span>
<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
 
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">运行结果为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">False</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([1.5000, 2.5000, 3.5000])</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor(1.)</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
<span class="c1">#例子2：</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">
</span><span class="s1"></span><span class="s1">x 是一个（2,3）的矩阵，设置为可导，是叶节点，即leaf variable</span><span class="s1">
</span><span class="s1"></span><span class="s1">y 是中间变量,由于x可导，所以y可导</span><span class="s1">
</span><span class="s1"></span><span class="s1">z 是中间变量,由于x，y可导，所以z可导</span><span class="s1">
</span><span class="s1"></span><span class="s1">f 是一个求和函数，最终得到的是一个标量scaler</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
 
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]</span><span class="p">,</span><span class="p">[</span><span class="mf">4.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">6.</span><span class="p">]</span><span class="p">]</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>  
</code></pre></td></tr></table>
</div>
</div><p>x,y,z的关系如下：</p>
<p><img src="https://gitee.com/shilongshen/image-bad/raw/master/20200702171745.png" style="zoom: 67%;" /></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#这样就可以手动求出f对于x11,x12,x13,x21,x22,x23的导数了</span>
<span class="c1">#验证：</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">===================================</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
 
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">运行结果为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">===================================</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([[1.3333, 2.0000, 2.6667],</span><span class="s1">
</span><span class="s1"></span><span class="s1">        [3.3333, 4.0000, 4.6667]])</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#向量对向量求导</span>
<span class="c1">#例子1：</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">
</span><span class="s1"></span><span class="s1">x 是一个（2,3）的矩阵，设置为可导，是叶节点，即leaf variable</span><span class="s1">
</span><span class="s1"></span><span class="s1">y 也是一个（2,3）的矩阵，即</span><span class="s1">
</span><span class="s1"></span><span class="s1">y=x^2+x (x的平方加x)</span><span class="s1">
</span><span class="s1"></span><span class="s1">实际上，就是要y的各个元素对相对应的x求导</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
 
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]</span><span class="p">,</span><span class="p">[</span><span class="mf">4.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">6.</span><span class="p">]</span><span class="p">]</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
 
<span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]</span><span class="p">,</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]</span><span class="p">]</span><span class="p">)</span><span class="c1">#参数都为1</span>
 
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
 
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
 
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">运行结果为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([[ 3.,  5.,  7.],</span><span class="s1">
</span><span class="s1"></span><span class="s1">        [ 9., 11., 13.]])</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>

<span class="c1">#例子2：</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
 
<span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">得到的结果：</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([2., 4., 6.])   这和我们期望的是一样的</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>  
<span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.01</span><span class="p">]</span><span class="p">)</span>
 
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">输出为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([2.0000, 0.4000, 0.0600])</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
<span class="c1">#从结果上来看，就是第二个导数缩小了十倍，第三个导数缩小了100倍，这个倍数和gradient里面的数字是息息相关的。</span>
<span class="c1">#dy/dx=0.1*dy1/dx+1.0*dy2/dx+0.0001*dy3/dx</span>
<span class="c1">#总结：gradient参数的维度与最终的函数y保持一样的形状，每一个元素表示当前这个元素所对应的权</span>

</code></pre></td></tr></table>
</div>
</div><hr>
<h4 id="神经网络的构建1">神经网络的构建（1）</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span><span class="c1">#用于定义神经网络模型</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">f</span><span class="c1">#用于激活函数的调用</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span><span class="c1">#用于更新梯度</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><i class="far fa-square fa-fw"></i>定义神经网络的模型</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span><span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span><span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="p">:</span>
        <span class="n">x</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="p">)</span><span class="p">,</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="p">)</span>
        <span class="n">x</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="p">)</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="c1">#将特征重构</span>
        <span class="n">x</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="p">)</span>
        <span class="n">x</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="p">)</span>
        <span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><i class="far fa-square fa-fw"></i>前向传播</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">net</span><span class="o">=</span><span class="n">Net</span><span class="p">(</span><span class="p">)</span>
<span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">output</span><span class="o">=</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><i class="far fa-square fa-fw"></i>定义损失函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="c1">#label，根据实际定义</span>
<span class="n">criterion</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="p">)</span>	<span class="c1">#定义损失函数</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><i class="far fa-square fa-fw"></i>定义更新方式，梯度清零，计算损失损失函数，计算梯度,梯度更新（反向传播）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">opt</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="c1">#定义更新方式</span>
<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度清零，清空现存梯度，否则梯度将会叠加</span>
<span class="n">loss</span><span class="o">=</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>	<span class="c1">#计算损失函数</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span><span class="c1">#计算梯度</span>
<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度更新</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h4 id="神经网络的构建2">神经网络的构建（2）</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">)</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="p">:</span>
        <span class="n">h_relu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>

<span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span>

<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">)</span><span class="c1">#定义输入数据</span>
<span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_out</span><span class="p">)</span><span class="c1">#定义labels</span>

<span class="c1">#实例化Net</span>
<span class="n">net</span><span class="o">=</span><span class="n">Net</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">)</span>
<span class="c1">#定义损失函数</span>
<span class="n">criterion</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="p">)</span>
<span class="c1">#定义更新方式</span>
<span class="n">opt</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>


<span class="c1">#进行前向传播、损失函数计算、梯度清零、梯度计算、梯度更新</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span><span class="p">:</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="c1">#前向传播</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="c1">#损失函数计算、</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度清零</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度计算</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度更新</span>





</code></pre></td></tr></table>
</div>
</div><h4 id="数据加载和处理1">数据加载和处理1</h4>
<p>使用torchvision包括了常用的数据集类（Dataset）和转换（transforms），还有一个常用的数据集类ImageFolder;数据加载器DataLoader</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>
<span class="c1">#from torch.utils.data.dataloader import DataLoader</span>

<span class="c1">#transforms的功能为对输入数据进行预处理，包括了Rescale：缩放图片；RandomCrop:对图片进行随机剪裁；ToTensor:把numpy格式的图片转换为tensor格式的图片</span>
<span class="c1">#transforms.Compose表示将这几类转换结合，进行组合转换</span>

<span class="c1">#另外transforms.Normalize的工作原理如下例所示：</span>
<span class="c1">#    transform.ToTensor(),</span>
<span class="c1">#    transform.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))</span>
<span class="c1">#那transform.Normalize()是怎么工作的呢？以上面代码为例，ToTensor()能够把灰度范围从0-255变换到0-1之间，而后面的transform.Normalize()则把0-1变换到#(-1,1).具体地说，对每个通道而言，Normalize执行以下操作：</span>
<span class="c1">#image=(image-mean)/std</span>
<span class="c1">#其中mean和std分别通过(0.5,0.5,0.5)和(0.5,0.5,0.5)进行指定。原来的0-1最小值0则变成(0-0.5)/0.5=-1，而最大值1则变成(1-0.5)/0.5=1.</span>
<span class="c1">#</span>
<span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="p">[</span>
<span class="n">transforms</span><span class="o">.</span><span class="n">RandomSizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span><span class="p">,</span>
<span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
<span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
<span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span><span class="p">,</span>
<span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span><span class="p">)</span>
<span class="p">]</span><span class="p">)</span>

<span class="c1">#ImageFolder是一个数据集类，它假定数据集是以如下方式构造的：</span>
<span class="c1">#root/ants/xxx.png</span>
<span class="c1">#root/ants/xxy.jpeg</span>
<span class="c1">#root/ants/xxz.png</span>
<span class="c1">#.</span>
<span class="c1">#.</span>
<span class="c1">#.</span>
<span class="c1">#root/bees/123.jpg</span>
<span class="c1">#root/bees/nsdf3.png</span>
<span class="c1">#root/bees/asd932_.png</span>
<span class="c1">#</span>

<span class="n">hymenoptera_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">hymenoptera_data/train</span><span class="s1">&#39;</span><span class="p">,</span>
<span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">)</span>

<span class="c1">#Dataloader 作为迭代器，每次产生一个 batch 大小的数据</span>
<span class="c1">#</span>
<span class="n">dataset_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">hymenoptera_dataset</span><span class="p">,</span>
<span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#Dataset和DataLoader的区别</span>
<span class="c1">#</span>
<span class="c1">#</span>
<span class="c1">#在pytorch中用一个类来抽象的表示数据集，即数据集类，常用的数据集类为Dataset和ImageFloder</span>
<span class="c1">#DataLoader做为一个迭代器，每次产生一个batch大小的数据。</span>

<span class="c1">#</span>
<span class="c1">#</span>
<span class="c1">#举例：</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.dataloader</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">../data</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                   <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="p">[</span>
                       <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
                       <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="p">(</span><span class="mf">0.1307</span><span class="p">,</span><span class="p">)</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,</span><span class="p">)</span><span class="p">)</span>
                   <span class="p">]</span><span class="p">)</span><span class="p">)</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1">#datasets.MNIST()是一个torch.utils.data.Datasets对象，batch_size表示我们定义的batch大小（即每轮训练使用的批大小），shuffle表示是否打乱数据顺序（对于整个datasets里包含的所有数据）</span>
<span class="c1">#</span>
<span class="c1">#一般如何加载DataLoader中的数据进行训练以及测试？</span>
<span class="c1">#</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="c1">#定义epochs次数</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span><span class="err">：</span>
	<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="p">:</span>
    <span class="c1">#batch_idx表示加载的第几批数据（数据以batch的形式进行加载，每一批中有每次产生一个batch大小的数据）、</span>
    <span class="c1">#data表示这一批中的训练数据，label表示相应data的标签</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span>
        <span class="o">.</span>
        <span class="o">.</span>
        <span class="o">.</span>
     
        

</code></pre></td></tr></table>
</div>
</div><h4 id="数据的加载和处理2----自定义数据集类">数据的加载和处理2 &ndash; 自定义数据集类</h4>
<p>假设我们网络有两个输入，这两个输入分别存放在不同的文件夹，如何同时读取两个文件加中的文件进行训练呢？</p>
<ul>
<li>自定义数据集类必须继承torch.utils.data.Dataset，必须覆写__getitem__       __len__</li>
<li>并覆写__len__ 实现len(dataset) 返还数据集尺寸</li>
<li>__getitem__用来获取索引数据</li>
<li>在__init__ 中读取文件内容</li>
<li>在__getitem__中按照索引读取图片</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#自定义数据集类</span>
<span class="c1">#torch.utils.data.Dataset 是表示数据集的抽象类，因此自定义数据集应该继承Dataset并覆盖以下方法__len__ 实现len(dataset) 返还数据集尺寸。__getitem__用来获取索引数据</span>

<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">os.path</span>

<span class="k">class</span> <span class="nc">FlatFolderDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root</span><span class="p">,</span> <span class="n">transform</span><span class="p">)</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlatFolderDataset</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="n">root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">*</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span><span class="c1">#将path路径下的文件以列表的形式存放，在__init__ 中读取文件内容,list存放的是文件的路径</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="p">:</span><span class="c1">#构造加载一个patch的数据集</span>
        <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="c1">#在__getitem__中按照索引读取图片</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">RGB</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="c1">#并进行必要的转换</span>
        <span class="k">return</span> <span class="n">img</span><span class="c1">#返回图像，即训练数据</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">)</span><span class="c1">#可以设置返回的训练集大小</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">FlatFolderDataset</span><span class="s1">&#39;</span>

    
    
</code></pre></td></tr></table>
</div>
</div><p>假设我们有来两个文件加分别存放两个训练集，路径分别为content_dir、style_dir</p>
<p>总的代码</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">os.path</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>


<span class="k">class</span> <span class="nc">FlatFolderDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style_root</span><span class="p">,</span> <span class="n">content_root</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlatFolderDataset</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style_root</span> <span class="o">=</span> <span class="n">style_root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">style_root</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">*</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>  <span class="c1"># 将path路径下的文件路径以列表的形式存放,list存放的是文件的路径</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">content_root</span> <span class="o">=</span> <span class="n">content_root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">content_root</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">*</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>  <span class="c1"># 将path路径下的文件路径以列表的形式存放,list存放的是文件的路径</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="p">:</span>
        <span class="n">style_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">style_paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>  <span class="c1"># 通过索引的方式获取图片路径</span>
        <span class="n">style_img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">style_path</span><span class="p">)</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">RGB</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># 获取图片</span>
        <span class="n">style_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">style_img</span><span class="p">)</span>  <span class="c1"># 对图像进行必要的处理</span>

        <span class="n">content_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">content_paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>  <span class="c1"># 通过索引的方式获取图片路径</span>
        <span class="n">content_img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">content_path</span><span class="p">)</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">RGB</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># 获取图片</span>
        <span class="n">content_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">content_img</span><span class="p">)</span>  <span class="c1"># 对图像进行必要的处理</span>

        <span class="k">return</span> <span class="p">{</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">style_img</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">style_img</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">content_img</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">content_img</span><span class="p">}</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">style_paths</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">FlatFolderDataset</span><span class="s1">&#39;</span>


<span class="k">def</span> <span class="nf">train_transform</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
    <span class="n">transform_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="c1"># transforms.Resize(size=(512, 512)),</span>
        <span class="c1"># transforms.RandomCrop(256),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span><span class="p">,</span>
                             <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span><span class="p">)</span>

    <span class="p">]</span>
    <span class="k">return</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="n">transform_list</span><span class="p">)</span>


<span class="n">train_transform</span> <span class="o">=</span> <span class="n">train_transform</span><span class="p">(</span><span class="p">)</span>

<span class="n">style_dir</span> <span class="o">=</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">/home/wag/ssl/kongfupainter/gan-getting-started</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">/monet_jpg</span><span class="s2">&#34;</span>
<span class="c1"># style_dataset = FlatFolderDataset(style_dir)  # 加载训练数据集</span>
<span class="c1"># print(len(style_dataset))</span>
<span class="n">content_dir</span> <span class="o">=</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">/home/wag/ssl/kongfupainter/gan-getting-started</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">/photo_jpg</span><span class="s2">&#34;</span>
<span class="c1"># content_dataset = FlatFolderDataset(content_dir)  # 加载训练数据集</span>
<span class="c1"># print(len(content_dataset))</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">FlatFolderDataset</span><span class="p">(</span><span class="n">style_dir</span><span class="p">,</span> <span class="n">content_dir</span><span class="p">,</span> <span class="n">train_transform</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_set</span><span class="p">)</span><span class="p">)</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="p">:</span>
    <span class="n">style</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">style_img</span><span class="s1">&#39;</span><span class="p">]</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">content_img</span><span class="s1">&#39;</span><span class="p">]</span>
    <span class="c1"># print(style.shape)</span>
    <span class="c1"># print(content.shape)</span>
    <span class="c1"># print(i)</span>

<span class="c1"># print(len(content))</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="python中parseradd_argument的用法">Python中parser.add_argument()的用法</h4>
<p><strong>Python解析命令行读取参数 – argparse模块</strong>
在多个文件或者不同语言协同的项目中，python脚本经常需要<strong>从命令行直接读取参数</strong>。万能的python就自带了argprase包使得这一工作变得简单而规范。</p>
<p><a href="https://www.cnblogs.com/arkenstone/p/6250782.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://www.jb51.net/article/179189.htm" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">your script description</span><span class="s2">&#34;</span><span class="p">)</span>  <span class="c1"># description参数可以用于插入描述脚本用途的信息，可以为空</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">--verbose</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>  <span class="c1">#这种模式用于确保某些必需的参数有输入。 required标签就是说--verbose参数是必需的，并且类型为int，输入别的类型会报错。</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">filename</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">text.txt</span><span class="s1">&#39;</span><span class="p">)</span> <span class="c1">#一般情况下会设置一些默认参数从而不需要每次输入某些不需要变动的参数，利用default参数即可实现。</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#argparse 是 Python 内置的一个用于命令项选项与参数解析的模块，通过在程序中定义好我们需要的参数，argparse 将会从 sys.argv 中解析出这些参数，并自动生成帮助和使用信息。</span>

<span class="c1">#简单示例</span>

<span class="c1">#我们先来看一个简单示例。主要有三个步骤：</span>

 <span class="c1">#   创建 ArgumentParser() 对象</span>
  <span class="c1">#  调用 add_argument() 方法添加参数</span>
   <span class="c1"># 使用 parse_args() 解析添加的参数</span>



<span class="kn">import</span> <span class="nn">argparse</span>
 
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">--sparse</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">store_true</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">GAT with sparse version or not.</span><span class="s1">&#39;</span><span class="p">)</span><span class="c1">#action - 命令行遇到参数时的动作，默认值是 False。</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">--seed</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Random seed.</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">--epochs</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Number of epochs to train.</span><span class="s1">&#39;</span><span class="p">)</span>
 
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="p">)</span>
 
<span class="k">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sparse</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
<span class="bp">False</span>
<span class="mi">72</span>
<span class="mi">10000</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">--sparse</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">action</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">store_true</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">help</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">GAT</span><span class="s1">&#39;</span><span class="p">)</span><span class="c1">#默认为False</span>
<span class="n">args</span><span class="o">=</span><span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sparse</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
<span class="bp">False</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pandas中loc和iloc函数用法">Pandas中loc和iloc函数用法</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">abcd</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">,</span>
                    <span class="n">columns</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">ABCD</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
 <span class="n">A</span>   <span class="n">B</span>   <span class="n">C</span>   <span class="n">D</span>
<span class="n">a</span>   <span class="mi">0</span>   <span class="mi">1</span>   <span class="mi">2</span>   <span class="mi">3</span>
<span class="n">b</span>   <span class="mi">4</span>   <span class="mi">5</span>   <span class="mi">6</span>   <span class="mi">7</span>
<span class="n">c</span>   <span class="mi">8</span>   <span class="mi">9</span>  <span class="mi">10</span>  <span class="mi">11</span>
<span class="n">d</span>  <span class="mi">12</span>  <span class="mi">13</span>  <span class="mi">14</span>  <span class="mi">15</span>

<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">a</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span>
<span class="n">A</span>    <span class="mi">0</span>
<span class="n">B</span>    <span class="mi">1</span>
<span class="n">C</span>    <span class="mi">2</span>
<span class="n">D</span>    <span class="mi">3</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
        
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">)</span>
<span class="n">A</span>    <span class="mi">4</span>
<span class="n">B</span>    <span class="mi">5</span>
<span class="n">C</span>    <span class="mi">6</span>
<span class="n">D</span>    <span class="mi">7</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">b</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
        
<span class="c1">#loc函数：通过行索引 “Index” 中的具体值来取行数据（如取&#34;Index&#34;为&#34;A&#34;的行）</span>
<span class="c1">#iloc函数：通过行号来取行数据（如取第二行的数据）</span>

<span class="c1">#利用loc、iloc提取列数据</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">A</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">]</span><span class="p">)</span> <span class="c1"># #取&#39;A&#39;列所有行，多取几列格式为 data.loc[:,[&#39;A&#39;,&#39;B&#39;]]</span>
<span class="o">&gt;&gt;</span>
    <span class="n">A</span>
<span class="n">a</span>   <span class="mi">0</span>
<span class="n">b</span>   <span class="mi">4</span>
<span class="n">c</span>   <span class="mi">8</span>
<span class="n">d</span>  <span class="mi">12</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
  	<span class="n">A</span>
<span class="n">a</span>   <span class="mi">0</span>
<span class="n">b</span>   <span class="mi">4</span>
<span class="n">c</span>   <span class="mi">8</span>
<span class="n">d</span>  <span class="mi">12</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">A</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">B</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
<span class="mi">0</span> <span class="mi">1</span>
<span class="mi">4</span> <span class="mi">5</span>
<span class="mi">8</span> <span class="mi">9</span>
<span class="mi">12</span> <span class="mi">13</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="__init__与__getitem__及__len__的使用"><strong>init</strong>()与__getitem__()及__len__()的使用</h4>
<p><a href="https://zhuanlan.zhihu.com/p/87786297" target="_blank" rel="noopener noreffer">参考</a></p>
<p><strong>自定义类，实现三个魔法方法并分析其作用</strong></p>
<p>首先给出类的结构，定义一个Fun类，并实现上述的三个魔法方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Fun</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_list</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>init()函数实现</strong></p>
<p><strong>init</strong>（）方法负责初始化，根据自己的经验，必须要对初始化函数的参数进行类型检查，避免不必要的麻烦。self表示这个方法是属于类实例的，而不是类的。（个人认为，python不像是java或者C++那样，它崇尚“万物皆引用”，如果不小心就会搞错对象类型），初始化函数如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_list</span><span class="p">)</span><span class="p">:</span>
        <span class="sa"></span><span class="s2">&#34;&#34;&#34;</span><span class="s2"> initialize the class instance</span><span class="s2">
</span><span class="s2"></span><span class="s2">        Args:</span><span class="s2">
</span><span class="s2"></span><span class="s2">            x_list: data with list type</span><span class="s2">
</span><span class="s2"></span><span class="s2">        Returns:</span><span class="s2">
</span><span class="s2"></span><span class="s2">            None</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">&#34;&#34;&#34;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">input x_list is not a list type</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">x_list</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">intialize success</span><span class="s2">&#34;</span><span class="p">)</span>

<span class="c1">#函数中的self.data是非常重要的实例属性，后面的两个方法的意义实际上就是来获取这个实例属性的信息。</span>

<span class="c1">#那么__init__函数是什么时候被调用的呢？其实它是在对象创建时被解释器自动调用的，现在创建一个实例试一试：</span>

<span class="n">fun</span> <span class="o">=</span> <span class="n">Fun</span><span class="p">(</span><span class="n">x_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span><span class="p">)</span>

<span class="c1">#终端会输出下面的字符串，说明__init__()函数被调用了：</span>

<span class="o">&gt;&gt;</span><span class="n">intialize</span> <span class="n">success</span>




</code></pre></td></tr></table>
</div>
</div><p>__init__相当于java中的构造方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">Student</span><span class="o">{</span>
    <span class="kd">private</span> <span class="n">String</span> <span class="n">name</span><span class="o">;</span><span class="c1">//field
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kt">int</span> <span class="n">age</span><span class="o">;</span><span class="c1">//field,相当于python中的self.data
</span><span class="c1"></span>    
    <span class="kd">public</span> <span class="nf">Student</span><span class="o">(</span><span class="n">String</span> <span class="n">name</span><span class="o">,</span><span class="kt">int</span> <span class="n">age</span><span class="o">)</span><span class="o">{</span><span class="c1">//java中的构造方法用于实例初始化
</span><span class="c1"></span>        <span class="k">this</span><span class="o">.</span><span class="na">name</span><span class="o">=</span><span class="n">name</span><span class="o">;</span>
        <span class="k">this</span><span class="o">.</span><span class="na">age</span><span class="o">=</span><span class="n">age</span><span class="o">;</span>
    <span class="o">}</span>
       
<span class="o">}</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Main</span><span class="o">{</span>
	<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[</span><span class="o">]</span> <span class="n">args</span><span class="o">)</span><span class="o">{</span>
        <span class="n">Student</span> <span class="n">s1</span><span class="o">=</span><span class="k">new</span> <span class="n">Student</span><span class="o">(</span><span class="s">&#34;xiaohong&#34;</span><span class="o">,</span><span class="n">18</span><span class="o">)</span><span class="o">;</span><span class="c1">//构建实例时用于初始化
</span><span class="c1"></span>    <span class="o">}</span>
<span class="o">}</span>

</code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>getitem</strong>()函数的实现</p>
<p>对于value[0]这个使用方框索引，来获取序列中的指定位置的值的方法，相信大家一定很熟悉。但是如果自己定义一个类，能否使用索引的方式获取这个类实例的属性值？答案就是使用__getitem__()方法（这个方法的作用不止于此，以后笔记中关于迭代器和for语法中还要用到它）。</p>
<ul>
<li><strong>使用索引的方式来获取类实例的属性值</strong></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">__getitem__ is called</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>getitem</strong>()方法接收一个idx参数，这个参数就是fun[2]中的2，也就是自己给的索引值。当**fun[2]**这个语句出现时，就会触发__getitem__(self,idx)，这个方法就会返回self.data[2]。一个很自然的问题就是，self.data[2]这里为什么使用了[2]这种索引方式？这里应该由于self.data是list类型，[2]触发的是list的__getitem__方法。测试一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">fun</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>终端会输出下面两条，说明__getitem__被调用了:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="fm">__getitem__</span> <span class="ow">is</span> <span class="n">called</span>
<span class="mi">3</span>
</code></pre></td></tr></table>
</div>
</div><p>__getitem__相当于java中的javabean的读方法（getter）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">Student</span><span class="o">{</span>
    <span class="kd">private</span> <span class="n">String</span> <span class="n">name</span><span class="o">;</span><span class="c1">//field
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kt">int</span> <span class="n">age</span><span class="o">;</span><span class="c1">//field,相当于python中的self.data
</span><span class="c1"></span>    
    <span class="kd">public</span> <span class="nf">Student</span><span class="o">(</span><span class="n">String</span> <span class="n">name</span><span class="o">,</span><span class="kt">int</span> <span class="n">age</span><span class="o">)</span><span class="o">{</span><span class="c1">//java中的构造方法用于实例初始化
</span><span class="c1"></span>        <span class="k">this</span><span class="o">.</span><span class="na">name</span><span class="o">=</span><span class="n">name</span><span class="o">;</span>
        <span class="k">this</span><span class="o">.</span><span class="na">age</span><span class="o">=</span><span class="n">age</span><span class="o">;</span>
    <span class="o">}</span>
    
    <span class="kd">public</span> <span class="n">String</span> <span class="nf">getName</span><span class="o">(</span><span class="o">)</span><span class="o">{</span><span class="c1">//读取实例field
</span><span class="c1"></span>        <span class="k">return</span> <span class="k">this</span><span class="o">.</span><span class="na">name</span><span class="o">;</span>
    <span class="o">}</span>
       
<span class="o">}</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Main</span><span class="o">{</span>
	<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[</span><span class="o">]</span> <span class="n">args</span><span class="o">)</span><span class="o">{</span>
        <span class="n">Student</span> <span class="n">s1</span><span class="o">=</span><span class="k">new</span> <span class="n">Student</span><span class="o">(</span><span class="s">&#34;xiaohong&#34;</span><span class="o">,</span><span class="n">18</span><span class="o">)</span><span class="o">;</span><span class="c1">//构建实例时用于初始化
</span><span class="c1"></span>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">s1</span><span class="o">.</span><span class="na">getName</span><span class="o">)</span><span class="o">;</span>
    <span class="o">}</span> 
<span class="o">}</span>

</code></pre></td></tr></table>
</div>
</div><hr>
<p>__len__函数实现</p>
<p>python包含一个内置方法len（），使用它可以测量list、tuple等序列对象的长度，如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">Li Hua</span><span class="s2">&#34;</span><span class="p">,</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">Daming</span><span class="s2">&#34;</span><span class="p">,</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">Honghong</span><span class="s2">&#34;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>终端将会输出：3</p>
<ul>
<li><strong>测量实例属性的长度</strong></li>
</ul>
<p>那么如果自定义一个类，是不是也能使用len（）函数测量某个实例属性的长度呢？只要实现__len__（）就能实现这个功能，如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">__len__ is called</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>这时就能使用len()函数测量self.data的长度了，试一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>终端输出下面两条，说明__len__()被自动调用了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="fm">__len__</span> <span class="ow">is</span> <span class="n">called</span>
<span class="mi">5</span>
</code></pre></td></tr></table>
</div>
</div><p>完整代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Fun</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_list</span><span class="p">)</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">input error</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">x_list</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">init success</span><span class="s2">&#34;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">__getitem__ is called</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">__len__ is called</span><span class="s2">&#34;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


<span class="n">fun</span> <span class="o">=</span> <span class="n">Fun</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">fun</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span><span class="p">)</span>

<span class="c1">#输出为：</span>
<span class="c1">#init success</span>
<span class="c1">#__getitem__ is called</span>
<span class="c1">#3</span>
<span class="c1">#__len__ is called</span>
<span class="c1">#3</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch-module里的children与modules的区别">pytorch Module里的children()与modules()的区别</h4>
<p><a href="https://blog.csdn.net/LXX516/article/details/79016980?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="p">,</span> 
                  <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
                 <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>

<span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">(</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
<span class="p">[</span><span class="n">Sequential</span><span class="p">(</span>
   <span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
   <span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(</span><span class="p">)</span>
   <span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="p">:</span> <span class="n">Sequential</span><span class="p">(</span>
     <span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="p">)</span>
     <span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(</span><span class="p">)</span>
   <span class="p">)</span>
 <span class="p">)</span><span class="p">,</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">Sequential</span><span class="p">(</span>
   <span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="p">)</span>
   <span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(</span><span class="p">)</span>
 <span class="p">)</span><span class="p">,</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">(</span><span class="p">)</span><span class="p">]</span>


<span class="c1">#用list举例就是：</span>
<span class="n">a</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="c1">#modules返回</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="p">]</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="numpy中matrix和ndarray的区别">Numpy中matrix和ndarray的区别</h4>
<p><a href="https://blog.csdn.net/wzyaiwl/article/details/90552935" target="_blank" rel="noopener noreffer">参考</a></p>
<h4 id="pytorch并行处理方法">pytorch并行处理方法</h4>
<p><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://oldpan.me/archives/pytorch-to-use-multiple-gpus" target="_blank" rel="noopener noreffer">参考</a></p>
<p>pytorch默认只使用一个GPU进行训练，如果想要使用多个GPU运行，需要使用<code>nn.DataParallel</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">cuda:0</span><span class="s2">&#34;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1">#默认只使用一个GPU进行训练</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># CUDA_VISIBLE_DEVICES设置说明，设置device对程序可见</span>

<span class="c1">#CUDA_VISIBLE_DEVICES=1 // 仅使用device1 (即卡一)</span>

<span class="c1">#CUDA_VISIBLE_DEVICES=0,1 // 仅使用device 0和 device1</span>

<span class="c1">#CUDA_VISIBLE_DEVICES=&#34;0,1&#34; // 同上, 仅使用device 0和 device1</span>

<span class="c1">#CUDA_VISIBLE_DEVICES=0,2,3 // 仅使用device 0, device2和device3</span>

<span class="c1">#CUDA_VISIBLE_DEVICES=2,0,3 // 仅使用device0, device2和device3</span>


<span class="c1">#device为用户	主机上从0号开始可用的GPU编号</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">cuda:0</span><span class="s2">&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">(</span><span class="p">)</span> <span class="k">else</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">cpu</span><span class="s2">&#34;</span><span class="p">)</span>

<span class="c1">#Model为用户的网络模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="c1">#使用多个GPU并行处理       在全部GPU上训练</span>
  <span class="c1">#model = nn.DataParallel(model,device_ids=[0,1,2,3])  可以指定在哪个GPU上训练，这里指定在0,1,2,3号GPU上训练（注意是以list的方式传入的）,这个的作用与设置CUDA_VISIBLE_DEVICES=0,1,2,3是一样的</span>

<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#model.cuda()与model.to(device)的效果是一样的</span>
<span class="c1">#</span>


<span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1">#输入数据也需要放入device中进行处理</span>


<span class="err">官</span><span class="err">方</span><span class="err">解</span><span class="err">释</span><span class="err">：</span>
<span class="c1"># DataParallel splits your data automatically and sends job orders to multiple</span>
<span class="c1"># models on several GPUs. After each model finishes their job, DataParallel</span>
<span class="c1"># collects and merges the results before returning it to you.</span>




<span class="c1">#在跑程序的时候出现了一个bug提示：AttributeError: &#39;DataParallel&#39; object has no attribute &#39;**&#39;</span>

<span class="c1">#这次参考了：https://github.com/jytime/Mask_RCNN_Pytorch/issues/2</span>

<span class="c1">#其中提到，因为使用torch.nn.DataParallel而造成了AttributeError: &#39;DataParallel&#39; object has no attribute &#39;**&#39;，‘**’为用户自定义的模型中的函数，此时因为使用了DataParallel函数对原本用户定义的model进行了包装，原model变为了DataParallel的一个子模块。</span>

<span class="c1">#因此想要再次调用原model中的函数，需要将原本的model.load()函数变为model.module.load()形式。其中load()为我在model中自定义的函数，综上，在model与函数中间加入.module即可。</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="matplotlib-画动态图以及pltion和pltioff的使用">matplotlib 画动态图以及plt.ion()和plt.ioff()的使用</h4>
<p><a href="https://blog.csdn.net/zbrwhut/article/details/80625702" target="_blank" rel="noopener noreffer">参考</a></p>
<p>在使用matplotlib的过程中，常常会需要画很多图，但是好像并不能同时展示许多图。这是因为python可视化库matplotlib的显示模式默认为阻塞（block）模式。什么是阻塞模式那？我的理解就是在plt.show()之后，程序会暂停到那儿，并不会继续执行下去。如果需要继续执行程序，就要关闭图片。那如何展示动态图或多个窗口呢？这就要使用plt.ion()这个函数，使matplotlib的显示模式转换为交互（interactive）模式。即使在脚本中遇到plt.show()，代码还是会继续执行。下面这段代码是展示两个不同的窗口：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">(</span><span class="p">)</span>    <span class="c1"># 打开交互模式</span>
    <span class="c1"># 同时打开两个窗口显示图片</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="p">)</span>  <span class="c1">#图片一</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">i1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="p">)</span>    <span class="c1">#图片二</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">i2</span><span class="p">)</span>
    <span class="c1"># 显示前关掉交互模式</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">(</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>在交互模式下：</p>
<p>1、plt.plot(x)或plt.imshow(x)是直接出图像，不需要plt.show()</p>
<p>2、如果在脚本中使用ion()命令开启了交互模式，没有使用ioff()关闭的话，则图像会一闪而过，并不会常留。要想防止这种情况，需要在plt.show()之前加上ioff()命令。</p>
<p>在阻塞模式下：</p>
<p>1、打开一个窗口以后必须关掉才能打开下一个新的窗口。这种情况下，默认是不能像Matlab一样同时开很多窗口进行对比的。</p>
<p>2、plt.plot(x)或plt.imshow(x)是直接出图像，需要plt.show()后才能显示图像</p>
<h4 id="链表推导式">链表推导式</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="iter函数和next函数">iter()函数和next()函数</h4>
<p>list、tuple等都是可迭代对象，我们可以通过iter()函数获取这些可迭代对象的迭代器。然后我们可以对获取到的迭代器不断使⽤next()函数来获取下⼀条数据。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span><span class="o">=</span><span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">next</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="o">&gt;&gt;</span>
<span class="mi">1</span>

<span class="c1">#判断一个对象是否为可迭代对象</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Iterable</span>
<span class="nb">isinstance</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">abc</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">Iterable</span><span class="p">)</span><span class="c1">#str是否可迭代</span>
<span class="o">&gt;&gt;</span><span class="bp">True</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="深拷贝和浅拷贝的区别">深拷贝和浅拷贝的区别</h4>
<p><a href="https://blog.csdn.net/u010712012/article/details/79754132" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">copy</span>

<span class="n">a</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="n">copy1</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">copy2</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">copy1</span><span class="o">==</span><span class="n">copy2</span>
<span class="o">&gt;&gt;</span><span class="n">Ture</span>
<span class="n">copy1</span> <span class="ow">is</span> <span class="n">copy2</span>
<span class="o">&gt;&gt;</span><span class="bp">False</span>

<span class="c1">#当改变a的值时</span>
<span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">9</span>
<span class="n">a</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="n">copy1</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="n">copy2</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>

<span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="mi">5</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="n">copy1</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>
<span class="n">copy2</span>
<span class="o">&gt;&gt;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="学习率调整">学习率调整</h4>
<p><a href="https://blog.csdn.net/qyhaill/article/details/103043637" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://blog.csdn.net/shanglianlm/article/details/85143614" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>

<span class="c1">#设置优化器</span>
<span class="n">optimizer_ft</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_ft</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># 学习率衰减</span>
<span class="n">exp_lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer_ft</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">optimizer_ft</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span><span class="c1">#梯度更新</span>
<span class="n">exp_lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>  <span class="c1"># 学习率调整</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>学习率调整应放在梯度更新之后</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>

<span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="k">class</span> <span class="nc">model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="n">net_1</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="p">)</span>

<span class="n">optimizer_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span><span class="p">)</span>
<span class="n">scheduler_1</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer_1</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">初始化的学习率：</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">optimizer_1</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">lr</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span><span class="p">:</span>
    <span class="c1"># train</span>

    <span class="n">optimizer_1</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span>
    <span class="n">optimizer_1</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">第</span><span class="si">%d</span><span class="s2">个epoch的学习率：</span><span class="si">%f</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">optimizer_1</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">lr</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span><span class="p">)</span>
    <span class="n">scheduler_1</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch的初始化-torchnninit">pytorch的初始化 (torch.nn.init)</h4>
<p><a href="https://pytorch.org/docs/1.5.1/nn.init.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://blog.csdn.net/dss_dssssd/article/details/83959474" target="_blank" rel="noopener noreffer">参考</a></p>
<ol>
<li>
<p>Xavier 初始化</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100636.png" /></p>
</li>
</ol>
<p>为什么需要Xavier 初始化？</p>
<ol>
<li>简答的说就是：</li>
</ol>
<ul>
<li>如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，<strong>失去了非线性</strong></li>
<li>如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到<strong>梯度消失</strong>的问题</li>
</ul>
<p>其他的激活函数同样存在相同的问题。</p>
<p>所以论文提出，<strong>在每一层网络保证输入和输出的方差相同</strong>。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730100816.png" /></p>
<ol start="2">
<li>He initialization</li>
</ol>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101102.png" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730101219.png" /></p>
<h4 id="pytorch使用tensorboard进行可视化">Pytorch使用tensorboard进行可视化</h4>
<p><a href="https://blog.csdn.net/MiaoB226/article/details/104213709?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.channel_param" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://github.com/Sun-DongYang/Pytorch" target="_blank" rel="noopener noreffer">参考</a></p>
<h4 id="pytorch使用visdom进行可视化">Pytorch使用visdom进行可视化</h4>
<p><a href="https://blog.csdn.net/wen_fei/article/details/82979497" target="_blank" rel="noopener noreffer">参考</a></p>
<p>visdom支持多种数据格式的可视化，包括数值、图像、文本以及视频等，支持Pytorch、Torch和Numpy。用户可以通过编程的方式组织可视化空间或者通过用户接口为数据打造仪表板，检查实验结果和调试代码。</p>
<p><img src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150438.png"  /></p>
<p>激活visdom服务：</p>
<p>​								或后台运行 nohup python -m visdom.server &amp;</p>
<p>本地打开浏览器，输入http://localhost:8097，Environment选择test1,即可查看图像</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png"
        data-srcset="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png 1.5x, https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png 2x"
        data-sizes="auto"
        alt="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png"
        title="https://gitee.com/shilongshen/image-bad/raw/master/img/20200730150726.png" /></p>
<p><code>在本地浏览器打开服务器端口：</code>服务器IP+端口号</p>
<p>① visdom.image()显示的图像数组的格式是 通道数 x 高 x 宽，而像PIL.Image读取的图像是高 x 宽 x 通道数，因此需要对其numpy数组进行转置一下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">visdom</span>

<span class="n">vis</span> <span class="o">=</span> <span class="n">visdom</span><span class="o">.</span><span class="n">Visdom</span><span class="p">(</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">xxx.jpg</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="p">)</span>

<span class="n">vis</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>利用visdom画损失函数</strong></p>
<p><strong>visdom.line的使用方法：</strong></p>
<p>画一条曲线时(Y只有一个方括号)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">viz</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="p">[</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span><span class="p">]</span><span class="p">,</span> <span class="p">[</span><span class="n">global_step</span><span class="p">]</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">train_loss</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">append</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span><span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">train loss</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span><span class="c1">#第一个参数表示Y，第二个参数表示X</span>
</code></pre></td></tr></table>
</div>
</div><p>画多条曲线时（Y有两个方括号）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">viz</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="p">]</span><span class="p">]</span><span class="p">,</span>
			 <span class="p">[</span><span class="n">global_step</span><span class="p">]</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">test</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">append</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span><span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">test loss&amp;acc.</span><span class="s1">&#39;</span><span class="p">,</span>
												   <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">loss</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">acc.</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>注意此处的更新方式（很重要），<code>update='append' if global_step &gt; 0 else None</code> 当<code>global_step=0</code> 不更新，此时传入的初值为第一个损失函数值，<strong>这一步相当于创建了一个新的新的曲线</strong>，当<code>global_step&gt;0</code> 时曲线才以<code>append </code> 的方式进行更新。</p>
<p><strong>visdom.images的使用方法：</strong></p>
<p>注意visdom.image：显示单张图像;visdom.images：可以显示多张图像。visdom.images显示的图像格式为C<em>H</em>W，输入可以为tensor或numpy(注意numpy图像的格式为H<em>W</em>C，使用时需要进行转换，例如：image_numpy.transpose([2, 0, 1]))</p>
<p>\1.  vis.image：显示一张图片</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">viz.image(
    np.random.rand(3, 512, 256),
    opts=dict(title=&#39;Random!&#39;, caption=&#39;How random.&#39;),
)
</code></pre></td></tr></table>
</div>
</div><ul>
<li><code>opts.jpgquality</code>：<code>JPG</code>质量（<code>number0-100</code>;默认= <code>100</code>）</li>
<li><code>opts.caption</code>：图像的标题</li>
</ul>
<p>\2.  vis.images：显示一组图片</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">viz.images(
    np.random.randn(20, 3, 64, 64),
    opts=dict(title=&#39;Random images&#39;, caption=&#39;How random.&#39;)
)
</code></pre></td></tr></table>
</div>
</div><p><code>输入类型：B x C x H x W</code>张量或<code>list of images</code>全部相同的大小。它使大小的图像（<code>B / Nrow，Nrow</code>）的网格。 可选参数opts：</p>
<ul>
<li><code>nrow</code>：连续的图像数量</li>
<li><code>padding</code>：在图像周围填充，四边均匀填充</li>
<li><code>opts.jpgquality</code>：<code>JPG</code>质量（<code>number0-100;</code>默认= <code>100</code>）</li>
<li><code>opts.caption</code>：图像的标题</li>
</ul>
<p><strong>注意</strong>：进行操作：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">transforms.Normalize(mean=[0.5, 0.5, 0.5],
                     std=[0.5, 0.5, 0.5])
</code></pre></td></tr></table>
</div>
</div><p>得到的张量数值为-1～1,此时直接进行显示可能会出现图像失真的情况，要进行处理为0～1</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Photo_show</span><span class="o">=</span><span class="p">(</span><span class="n">Photo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1，Photo为输入的张量</span>
<span class="n">Monet_show</span><span class="o">=</span><span class="p">(</span><span class="n">Monet</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1</span>
<span class="n">fake_photo_show</span><span class="o">=</span><span class="p">(</span><span class="n">fake_photo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1</span>
<span class="n">fake_monet_show</span><span class="o">=</span><span class="p">(</span><span class="n">fake_monet</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1</span>
<span class="n">cycle_photo_show</span><span class="o">=</span><span class="p">(</span><span class="n">cycle_photo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1</span>
<span class="n">cycle_monet_show</span><span class="o">=</span><span class="p">(</span><span class="n">cycle_monet</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="c1">#0~1</span>
<span class="c1"># if i==0:</span>
<span class="c1">#     print(&#39;after&#39;,photo_show)</span>
<span class="c1"># photo_show=self.tensor2im(Photo)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">Photo_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Photo</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Photo</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">Monet_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Monet</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Monet</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">fake_photo_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">fake_photo</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">fake_photo</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">fake_monet_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">fake_monet</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">fake_monet(target_image)</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">cycle_photo_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cycle_photo</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cycle_photo</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">cycle_monet_show</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cycle_monet</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cycle_monet</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><a href="https://blog.csdn.net/SHU15121856/article/details/88818539?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-1-88818539.nonecase&amp;utm_term=visdom%E6%80%8E%E4%B9%88%E6%B7%BB%E5%8A%A0%E5%88%B0%E5%AE%9E%E6%97%B6%E8%AE%AD%E7%BB%83" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="kn">from</span> <span class="nn">visdom</span> <span class="kn">import</span> <span class="n">Visdom</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
	<span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">../data</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
				   <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="p">[</span>
					   <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
					   <span class="c1"># transforms.Normalize((0.1307,), (0.3081,))</span>
				   <span class="p">]</span><span class="p">)</span><span class="p">)</span><span class="p">,</span>
	<span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
	<span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">../data</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="p">[</span>
		<span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">,</span>
		<span class="c1"># transforms.Normalize((0.1307,), (0.3081,))</span>
	<span class="p">]</span><span class="p">)</span><span class="p">)</span><span class="p">,</span>
	<span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span><span class="p">:</span>

	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span><span class="p">,</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="p">,</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span><span class="p">,</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="p">,</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="p">,</span>
			<span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="p">,</span>
		<span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="p">:</span>
		<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">x</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cuda:0</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criteon</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">viz</span> <span class="o">=</span> <span class="n">Visdom</span><span class="p">(</span><span class="p">)</span>
<span class="c1">#viz.line([0.], [0.], win=&#39;train_loss&#39;, opts=dict(title=&#39;train loss&#39;))</span>
<span class="c1">#viz.line([[0.0, 0.0]], [0.], win=&#39;test&#39;, opts=dict(title=&#39;test loss&amp;acc.&#39;,</span>
<span class="c1">#												   legend=[&#39;loss&#39;, &#39;acc.&#39;]))</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span><span class="p">:</span>

	<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="p">:</span>
		<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
		<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="p">)</span>

		<span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">criteon</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

		<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>
		<span class="c1"># print(w1.grad.norm(), w2.grad.norm())</span>
		<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>

		<span class="n">global_step</span> <span class="o">+</span><span class="o">=</span> <span class="mi">1</span>
		<span class="n">viz</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="p">[</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span><span class="p">]</span><span class="p">,</span> <span class="p">[</span><span class="n">global_step</span><span class="p">]</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">train_loss</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">append</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span><span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">train loss</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
			<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">Train Epoch: {} [{}/{} ({:.0f}</span><span class="s1">%</span><span class="s1">)]</span><span class="se">\t</span><span class="s1">Loss: {:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
				<span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="p">,</span>
					   <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>

	<span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
		<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
		<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="p">)</span>
		<span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
		<span class="n">test_loss</span> <span class="o">+</span><span class="o">=</span> <span class="n">criteon</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span>

		<span class="n">pred</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="n">correct</span> <span class="o">+</span><span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span>

	<span class="n">viz</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="p">]</span><span class="p">]</span><span class="p">,</span>
			 <span class="p">[</span><span class="n">global_step</span><span class="p">]</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">test</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">append</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span><span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">test loss&amp;acc.</span><span class="s1">&#39;</span><span class="p">,</span>
												   <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">loss</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">acc.</span><span class="s1">&#39;</span><span class="p">]</span><span class="p">)</span><span class="p">)</span>
	<span class="n">viz</span><span class="o">.</span><span class="n">images</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">x</span><span class="s1">&#39;</span><span class="p">)</span>
	<span class="n">viz</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">detach</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">pred</span><span class="s1">&#39;</span><span class="p">,</span>
			 <span class="n">opts</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">pred</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>

	<span class="n">test_loss</span> <span class="o">/</span><span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}</span><span class="s1">%</span><span class="s1">)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
		<span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="p">,</span>
		<span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="gan中detach的作用">GAN中detach()的作用</h4>
<p><a href="https://blog.csdn.net/hungryof/article/details/78035332" target="_blank" rel="noopener noreffer">参考</a></p>
<p>简单来说detach就是<strong>截断反向传播的梯度流</strong>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#GAN的pytorch实现</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_A</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_A</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fake_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">netG</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">real_A</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_B</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_B</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">backward_D</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="c1"># Fake</span>
        <span class="c1"># stop backprop to the generator by detaching fake_B</span>
        <span class="n">fake_AB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_B</span>
        <span class="c1"># fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">netD</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">fake_AB</span><span class="o">.</span><span class="n">detach</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_D_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterionGAN</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_fake</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

        <span class="c1"># Real</span>
        <span class="n">real_AB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_B</span> <span class="c1"># GroundTruth</span>
        <span class="c1"># real_AB = torch.cat((self.real_A, self.real_B), 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_real</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">netD</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">real_AB</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_D_real</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterionGAN</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_real</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Combined loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_D</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_D_fake</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_D_real</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_D</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backward_G</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="c1"># First, G(A) should fake the discriminator</span>
        <span class="n">fake_AB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_B</span>
        <span class="n">pred_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">netD</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">fake_AB</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_G_GAN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterionGAN</span><span class="p">(</span><span class="n">pred_fake</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Second, G(A) = B</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_G_L1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterionL1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fake_B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_B</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">lambda_A</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_G</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_G_GAN</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_G_L1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_G</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>

    <span class="c1"># 先调用 forward, 再 D backward， 更新D之后； 再G backward， 再更新G</span>
    <span class="k">def</span> <span class="nf">optimize_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_D</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_D</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_D</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_G</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_G</span><span class="p">(</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_G</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>解释<code>backward_D</code>:</strong></p>
<p>对于D，我们值需要，如果输入是真实图，那么产生loss，输入生成图，也产生loss。
这两个梯度进行更新D。如果是真实图(real_B)，由于real_B是初始结点，所以没什么可担心的。<strong>但是对于生成图fake_B</strong>，<strong>由于  fake_B是由 netG.forward(real_A)产生的。我们<code>只希望该loss更新D不要影响到 G</code>.  因此这里需要“截断反传的梯度流”，用 fake_AB = fake_B,</strong> <strong>fake_AB.detach()从而让梯度不要通过 fake_AB反传到netG中！</strong></p>
<p><strong>解释<code>backward_G</code>:</strong></p>
<p>由于在调用 backward_G已经调用了zero_grad，所以没什么好担心的。
更新G时，来自D的GAN损失是， netD.forward(fake_AB)，得到 pred_fake，然后得到损失，反传播即可。
注意，这里反向传播时，会先将梯度传到 fake_AB结点，然而我们知道 fake_AB即 fake_B结点，而fake_B正是由netG(real_A)产生的，所以还会顺着继续往前传播，从而得到G的对应的梯度。</p>
<h4 id="pytorch-中data和detach的区别和联系">pytorch 中.data和.detach的区别和联系</h4>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/96837905?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param" target="_blank" rel="noopener noreffer">参考</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#.data的使用</span>
<span class="kn">import</span> <span class="nn">torch</span>
 
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">]</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># 需要注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化</span>
<span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">(</span><span class="p">)</span>     <span class="c1"># 改变c的值，原来的out也会改变</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">----------------------------------------------</span><span class="s2">&#34;</span><span class="p">)</span>
 
<span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span> <span class="c1"># 对原来的out求导，</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 不会报错，但是结果却并不正确</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">运行结果为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">False</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0., 0., 0.])</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</span><span class="s1">
</span><span class="s1"></span><span class="s1">----------------------------------------------</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0., 0., 0.])</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>tensor.data的两点总结：</strong></p>
<p><strong>（1）tensor .data</strong> 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实detach是一样的）</p>
<p>**（2）使用tensor.data的局限性。**文档中说使用tensor.data是不安全的, 因为 x.data 不能被 autograd 追踪求微分  。什么意思呢？从上面的例子可以看出，由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，但是这种改变对于autograd是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#detach()的使用</span>
<span class="kn">import</span> <span class="nn">torch</span>
 
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">]</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">(</span><span class="p">)</span>  <span class="c1"># 需要走注意的是，通过.detach() “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化</span>
<span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">(</span><span class="p">)</span>     <span class="c1"># 改变c的值，原来的out也会改变</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">----------------------------------------------</span><span class="s2">&#34;</span><span class="p">)</span>
 
<span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span> <span class="c1"># 对原来的out求导，</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 此时会报错，错误结果参考下面,显示梯度计算所需要的张量已经被“原位操作inplace”所更改了。</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">
</span><span class="s1"></span><span class="s1">False</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0., 0., 0.])</span><span class="s1">
</span><span class="s1"></span><span class="s1">True</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</span><span class="s1">
</span><span class="s1"></span><span class="s1">----------------------------------------------</span><span class="s1">
</span><span class="s1"></span><span class="s1">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>tensor.detach()的两点总结：</strong></p>
<p><strong>（1）tensor .detach()</strong> 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实 .data是一样的）</p>
<p>**（2）使用tensor.detach()的优点。**从上面的例子可以看出，由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于out已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。</p>
<p><code>总结</code>：</p>
<p>**相同点：**tensor.data和tensor.detach() 都是变量从图中分离，而且这都是“原位操作 inplace operation”。</p>
<p><strong>不同点：</strong></p>
<p>（1）.data 是一个属性，二.detach()是一个方法；</p>
<p>（2）.data 是不安全的，.detach()是安全的。</p>
<h4 id="pytorch保存模型等相关参数利用torchsave以及读取保存之后的文件">pytorch保存模型等相关参数，利用torch.save()，以及读取保存之后的文件</h4>
<p><a href="https://www.cnblogs.com/qinduanyinghua/p/9311410.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://github.com/soravux/pytorch_tutorial/blob/master/example3.py" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p>第一部分讲如何保存模型参数，优化器参数等等，第二部分则讲如何读取。</p>
<p>假设网络为model = Net(), optimizer = optim.Adam(model.parameters(), lr=args.lr), 假设在某个epoch，我们要保存模型参数，优化器参数以及epoch</p>
<p>一、</p>
<p>\1. 先建立一个字典，保存三个参数：</p>
<p>state = {‘net&rsquo;:model.state_dict(), &lsquo;optimizer&rsquo;:optimizer.state_dict(), &lsquo;epoch&rsquo;:epoch}</p>
<p>2.调用torch.save():</p>
<p>torch.save(state, dir)</p>
<p>其中dir表示保存文件的绝对路径+保存文件名，如&rsquo;/home/qinying/Desktop/modelpara.pth&rsquo;</p>
<p>二、</p>
<p>当你想恢复某一阶段的训练（或者进行测试）时，那么就可以读取之前保存的网络模型参数等。</p>
<p>checkpoint = torch.load(dir)</p>
<p>model.load_state_dict(checkpoint[&lsquo;net&rsquo;])</p>
<p>optimizer.load_state_dict(checkpoint[&lsquo;optimizer&rsquo;])</p>
<p>start_epoch = checkpoint[&lsquo;epoch&rsquo;] + 1</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="p">)</span><span class="c1">#读取模型的参数</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="p">)</span><span class="p">,</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">dir</span><span class="s1">&#39;</span><span class="p">)</span><span class="c1">#将模型的参数保存，路径加文件名（以.pth结尾）</span>

<span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">dir</span><span class="s1">&#39;</span><span class="p">)</span><span class="c1">#将模型的参数进行加载</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">dir</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span><span class="c1">#将模型的参数赋予当前的网络</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch中contiguous">pytorch中contiguous()</h4>
<p><a href="https://blog.csdn.net/appleml/article/details/80143212" target="_blank" rel="noopener noreffer">参考</a></p>
<p>contiguous：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。
一种可能的解释是：
有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的<code>view()</code>操作依赖于内存是整块的，这时只需要执行<code>contiguous()</code>这个函数，把tensor变成在内存中连续分布的形式。
判断是否contiguous用<code>torch.Tensor.is_contiguous()</code>函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="p">)</span>  <span class="c1"># True</span>
<span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="p">)</span>  <span class="c1"># False</span>
<span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="p">)</span>  <span class="c1"># True</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch中的t">pytorch中的.t()</h4>
<p><strong>意义就是将Tensor进行转置</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="k">def</span> <span class="nf">t</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span><span class="p">:</span> <span class="c1"># real signature unknown; restored from __doc__</span>
        <span class="sa"></span><span class="s2">&#34;&#34;&#34;</span><span class="s2">
</span><span class="s2"></span><span class="s2">        t(input) -&gt; Tensor</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">
</span><span class="s2"></span><span class="s2">        Expects :attr:`input` to be a matrix (2-D tensor) and transposes dimensions 0</span><span class="s2">
</span><span class="s2"></span><span class="s2">        and 1.</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">
</span><span class="s2"></span><span class="s2">        Can be seen as a short-hand function for :meth:`transpose(input, 0, 1)`</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">
</span><span class="s2"></span><span class="s2">        Args:</span><span class="s2">
</span><span class="s2"></span><span class="s2">            input (Tensor): the input tensor</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">
</span><span class="s2"></span><span class="s2">        Example::</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">
</span><span class="s2"></span><span class="s2">            &gt;&gt;&gt; x = torch.randn(2, 3)</span><span class="s2">
</span><span class="s2"></span><span class="s2">            &gt;&gt;&gt; x</span><span class="s2">
</span><span class="s2"></span><span class="s2">            tensor([[ 0.4875,  0.9158, -0.5872],</span><span class="s2">
</span><span class="s2"></span><span class="s2">                    [ 0.3938, -0.6929,  0.6932]])</span><span class="s2">
</span><span class="s2"></span><span class="s2">            &gt;&gt;&gt; torch.t(x)</span><span class="s2">
</span><span class="s2"></span><span class="s2">            tensor([[ 0.4875,  0.3938],</span><span class="s2">
</span><span class="s2"></span><span class="s2">                    [ 0.9158, -0.6929],</span><span class="s2">
</span><span class="s2"></span><span class="s2">                    [-0.5872,  0.6932]])</span><span class="s2">
</span><span class="s2"></span><span class="s2">        </span><span class="s2">&#34;&#34;&#34;</span>
        <span class="k">pass</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="卷积中的填充方法">卷积中的填充方法</h4>
<p><a href="https://zhuanlan.zhihu.com/p/95368411" target="_blank" rel="noopener noreffer">参考</a></p>
<p>*<strong>1、零填充ZeroPad2d*</strong></p>
<p>*<strong>2、常数填充ConstantPad2d*</strong></p>
<p>*<strong>3、镜像填充ReflectionPad2d*</strong></p>
<p>使用输入边界的反射来填充输入tensor</p>
<p>*<strong>4、重复填充ReplicationPad2d*</strong></p>
<h4 id="棋盘效应">棋盘效应</h4>
<p><a href="https://www.cnblogs.com/hellcat/p/9707204.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p>通过神经网络生成的图片，放大了看会有棋盘格的现象</p>
<h4 id="torchmean和torchmeandim0meandim1">torch.mean()和torch.mean(dim=0).mean(dim=1)</h4>
<p>以二维为例：torch.mean(）返回的是一个标量，而torch.mean(dim=0).mean(dim=1)返回的是一个1行1列的张量，虽然数值相同</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="p">)</span>
<span class="n">x_mean</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_mean</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">tensor</span><span class="p">(</span><span class="mf">5.5000</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mf">5.5000</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="n">Args</span><span class="p">:</span>
        <span class="nb">input</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">)</span><span class="p">:</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">tensor</span><span class="o">.</span>
        <span class="n">dim</span> <span class="p">(</span><span class="nb">int</span> <span class="ow">or</span> <span class="nb">tuple</span> <span class="n">of</span> <span class="n">ints</span><span class="p">)</span><span class="p">:</span> <span class="n">the</span> <span class="n">dimension</span> <span class="ow">or</span> <span class="n">dimensions</span> <span class="n">to</span> <span class="nb">reduce</span><span class="o">.</span>
        <span class="n">keepdim</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span><span class="p">:</span> <span class="n">whether</span> <span class="n">the</span> <span class="n">output</span> <span class="n">tensor</span> <span class="n">has</span> <span class="p">:</span><span class="n">attr</span><span class="p">:</span><span class="sb">`dim`</span> <span class="n">retained</span> <span class="ow">or</span> <span class="ow">not</span><span class="o">.</span>
        <span class="n">out</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">optional</span><span class="p">)</span><span class="p">:</span> <span class="n">the</span> <span class="n">output</span> <span class="n">tensor</span><span class="o">.</span>
    
    <span class="n">Example</span><span class="p">:</span><span class="p">:</span>
    
        <span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span>
        <span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="o">-</span><span class="mf">0.3841</span><span class="p">,</span>  <span class="mf">0.6320</span><span class="p">,</span>  <span class="mf">0.4254</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7384</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span><span class="o">-</span><span class="mf">0.9644</span><span class="p">,</span>  <span class="mf">1.0131</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6549</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4279</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span><span class="o">-</span><span class="mf">0.2951</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3350</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7694</span><span class="p">,</span>  <span class="mf">0.5600</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span> <span class="mf">1.0842</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9580</span><span class="p">,</span>  <span class="mf">0.3623</span><span class="p">,</span>  <span class="mf">0.2343</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
        <span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="c1">#按行求均值</span>
 <span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="o">-</span><span class="mf">0.0163</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5085</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4599</span><span class="p">,</span>  <span class="mf">0.1807</span><span class="p">]</span><span class="p">)</span><span class="c1">#此时的shape为torch.Size([4])</span>
        <span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span><span class="c1">#此时的shape为torch.Size([4, 1])</span>
        <span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="o">-</span><span class="mf">0.0163</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span><span class="o">-</span><span class="mf">0.5085</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span><span class="o">-</span><span class="mf">0.4599</span><span class="p">]</span><span class="p">,</span>
                <span class="p">[</span> <span class="mf">0.1807</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="矩阵乘法">矩阵乘法</h4>
<p><a href="https://zhuanlan.zhihu.com/p/100069938" target="_blank" rel="noopener noreffer">参考</a></p>
<h4 id="pytorch-模型训练时多卡负载不均衡gpu的0卡显存过高解决办法">pytorch 模型训练时多卡负载不均衡（GPU的0卡显存过高）解决办法</h4>
<blockquote>
<p>出现0卡显存更高的原因：网络在反向传播的时候，计算loss的梯度默认都在0卡上计算。因此会比其他显卡多用一些显存，具体多用多少，主要还要看网络的结构。</p>
</blockquote>
<p><a href="https://blog.csdn.net/weixin_43922901/article/details/106117774" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/86441879" target="_blank" rel="noopener noreffer">参考</a></p>
<p>重写DataParallel</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DataParallel</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel._functions</span> <span class="kn">import</span> <span class="n">Scatter</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel.parallel_apply</span> <span class="kn">import</span> <span class="n">parallel_apply</span>

<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span>
    <span class="sa">r</span><span class="s2">&#34;&#34;&#34;</span><span class="s2">
</span><span class="s2"></span><span class="s2">    Slices tensors into approximately equal chunks and</span><span class="s2">
</span><span class="s2"></span><span class="s2">    distributes them across given GPUs. Duplicates</span><span class="s2">
</span><span class="s2"></span><span class="s2">    references to objects that are not tensors.</span><span class="s2">
</span><span class="s2"></span><span class="s2">    </span><span class="s2">&#34;&#34;&#34;</span>
    <span class="k">def</span> <span class="nf">scatter_map</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Scatter</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">obj</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">dim</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">chunk_sizes</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">)</span>
                <span class="n">quit</span><span class="p">(</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">scatter_map</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">scatter_map</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">scatter_map</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">obj</span> <span class="k">for</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">target_gpus</span><span class="p">]</span>

    <span class="c1"># After scatter_map is called, a scatter_map cell will exist. This cell</span>
    <span class="c1"># has a reference to the actual function scatter_map, which has references</span>
    <span class="c1"># to a closure that has a reference to the scatter_map cell (because the</span>
    <span class="c1"># fn is recursive). To avoid this reference cycle, we set the function to</span>
    <span class="c1"># None, clearing the cell</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">scatter_map</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">scatter_map</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span>
    <span class="sa">r</span><span class="s2">&#34;&#34;&#34;</span><span class="s2">Scatter with support for kwargs dictionary</span><span class="s2">&#34;&#34;&#34;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">inputs</span> <span class="k">else</span> <span class="p">[</span><span class="p">]</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="p">[</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span><span class="p">:</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="p">[</span><span class="p">(</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="p">)</span><span class="p">]</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="p">[</span><span class="p">{</span><span class="p">}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span><span class="p">)</span><span class="p">]</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span>

<span class="k">class</span> <span class="nc">BalancedDataParallel</span><span class="p">(</span><span class="n">DataParallel</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">)</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu0_bsz</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="nb">super</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">)</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu0_bsz</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">device_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">)</span>
        <span class="n">replicas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu0_bsz</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">replicas</span> <span class="o">=</span> <span class="n">replicas</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parallel_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replicas</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span><span class="p">:</span>
        <span class="n">bsz</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">num_dev</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="n">gpu0_bsz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu0_bsz</span>
        <span class="n">bsz_unit</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">-</span> <span class="n">gpu0_bsz</span><span class="p">)</span> <span class="o">/</span><span class="o">/</span> <span class="p">(</span><span class="n">num_dev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gpu0_bsz</span> <span class="o">&lt;</span> <span class="n">bsz_unit</span><span class="p">:</span>
            <span class="n">chunk_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpu0_bsz</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">bsz_unit</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_dev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">bsz</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">chunk_sizes</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="p">:</span>
                <span class="n">chunk_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span><span class="o">=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">gpu0_bsz</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">chunk_sizes</span> <span class="o">=</span> <span class="n">chunk_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>然后</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># netG = torch.nn.DataParallel(netG, device_ids=gpu_ids)</span>
        <span class="n">netG</span> <span class="o">=</span> <span class="n">BalancedDataParallel</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">cuda</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="c1"># netG.cuda()</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch张量数据索引切片与维度变换操作">Pytorch张量数据索引切片与维度变换操作</h4>
<p><a href="http://www.mamicode.com/info-detail-2783943.html" target="_blank" rel="noopener noreffer">参考</a></p>
<p>(1-1)pytorch张量数据的索引与切片操作
1、对于张量数据的<strong>索引操作</strong>主要有以下几种方式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span><span class="p">:</span><span class="n">DIM</span><span class="o">=</span><span class="mi">4</span><span class="err">的</span><span class="err">张</span><span class="err">量</span><span class="err">数</span><span class="err">据</span><span class="n">a</span>

<span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="p">:</span><span class="o">*</span><span class="o">*</span><span class="err">取</span><span class="err">第</span><span class="err">一</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">的</span><span class="err">前</span><span class="mi">2</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">数</span><span class="err">据</span><span class="p">(</span><span class="err">不</span><span class="err">包</span><span class="err">括</span><span class="mi">2</span><span class="p">)</span><span class="err">；</span>      
<span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">]</span><span class="err">：</span><span class="o">*</span><span class="o">*</span><span class="err">取</span><span class="err">第</span><span class="err">一</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">的</span><span class="err">前</span><span class="err">两</span><span class="err">个</span><span class="err">数</span><span class="err">据</span><span class="err">，</span><span class="err">取</span><span class="err">第</span><span class="mi">2</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">的</span><span class="err">前</span><span class="mi">1</span><span class="err">个</span><span class="err">数</span><span class="err">据</span><span class="err">，</span><span class="err">后</span><span class="err">两</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">全</span><span class="err">都</span><span class="err">取</span><span class="err">到</span><span class="err">；</span>
<span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">]</span><span class="err">：</span><span class="o">*</span><span class="o">*</span><span class="err">取</span><span class="err">第</span><span class="err">一</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">的</span><span class="err">前</span><span class="err">两</span><span class="err">个</span><span class="err">数</span><span class="err">据</span><span class="err">，</span><span class="err">取</span><span class="err">第</span><span class="mi">2</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">的</span><span class="err">第</span><span class="mi">1</span><span class="err">个</span><span class="err">索</span><span class="err">引</span><span class="err">到</span><span class="err">最</span><span class="err">后</span><span class="err">索</span><span class="err">引</span><span class="err">的</span><span class="err">数</span><span class="err">据</span><span class="p">(</span><span class="err">包</span><span class="err">含</span><span class="mi">1</span><span class="p">)</span><span class="err">，</span><span class="err">后</span><span class="err">两</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">全</span><span class="err">都</span><span class="err">取</span><span class="err">到</span><span class="err">；</span>
<span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="p">]</span><span class="err">：</span><span class="o">*</span><span class="o">*</span><span class="err">负</span><span class="err">号</span><span class="err">表</span><span class="err">示</span><span class="err">第</span><span class="mi">2</span><span class="err">个</span><span class="err">维</span><span class="err">度</span><span class="err">上</span><span class="err">从</span><span class="err">倒</span><span class="err">数</span><span class="err">第</span><span class="mi">3</span><span class="err">个</span><span class="err">数</span><span class="err">据</span><span class="err">取</span><span class="err">到</span><span class="err">最</span><span class="err">后</span><span class="err">倒</span><span class="err">数</span><span class="err">第</span><span class="err">一</span><span class="err">个</span><span class="err">数</span><span class="err">据</span><span class="o">-</span><span class="mi">1</span><span class="p">(</span><span class="err">包</span><span class="err">含</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="err">；</span>
<span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="err">：</span><span class="o">*</span><span class="o">*</span><span class="err">两</span><span class="err">个</span><span class="err">冒</span><span class="err">号</span><span class="err">表</span><span class="err">示</span><span class="err">隔</span><span class="err">行</span><span class="err">取</span><span class="err">数</span><span class="err">据</span><span class="err">，</span><span class="err">一</span><span class="err">定</span><span class="err">的</span><span class="err">间</span><span class="err">隔</span><span class="err">；</span>
<span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="p">:</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="err">：</span><span class="o">*</span><span class="o">*</span><span class="err">两</span><span class="err">个</span><span class="err">冒</span><span class="err">号</span><span class="err">直</span><span class="err">接</span><span class="err">写</span><span class="err">表</span><span class="err">示</span><span class="err">从</span><span class="err">所</span><span class="err">有</span><span class="err">的</span><span class="err">数</span><span class="err">据</span><span class="err">中</span><span class="err">隔</span><span class="err">行</span><span class="err">取</span><span class="err">数</span><span class="err">据</span><span class="err">。</span>
</code></pre></td></tr></table>
</div>
</div><p>2、对于tensor数据的<strong>切片与其中某些维度数据的提取方法</strong>:
**a.index_select(x,torch.tensor([m,n])):**表示提取tensor数据a的第x个维度上的索引为m和n的数据</p>
<p>3、**torch.masked_select(x,mask):**该函数主要用来选取x数据中的mask性质的数据，比如mask=x.ge(0.5)表示选出大于0.5的所有数据，并且输出时将其转换为了dim=1的打平tensor数据。</p>
<p>4、<strong>#take函数的应用</strong>:先将张量数据打平为一个dim=1的张量数据（依次排序下来成为一个数据列），然后按照索引进行取数据
a=torch.tensor([[1,2,3],[4,5,6]])
<strong>torch.take(a,torch.tensor([1,2,5]))：<strong>表示提取a这个tensor数据打平以后的索引为1/2/5的数据元素
(1-2)tensor数据的维度变换
1、对于tensor数据的</strong>维度变换主要有四大API函数：</strong>
<strong>(1)view/reshape</strong>：主要是在保证tensor数据大小不变的情况下对tensor数据进行形状的重新定义与转换
**(2)Squeeze/unsqueeze:**删减维度或者增加维度操作
<strong>(3)transpose/t/permute</strong>：类似矩阵的转置操作，对于多维的数据具有多次或者单次的转换操作
(4)Expand/repeat：维度的扩展，将低维数据转换为高维的数据
2、**view(reshape)**维度转换操作时需要保证数据的大小numl保持不变，即数据变换前后的prod是相同的：
prod(a.size)=prod(b.size)
另外，对于view操作有一个致命的缺陷就是在数据进行维度转换之后数据之前的存储与维度顺序信息会丢失掉，不能够复原，而这对于训练的数据来说非常重要。
3、<strong>squeeze/unsqueeze挤压和增加维度操作的函数</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">:</span><span class="err">在</span><span class="n">a原来维度索引1之间增加一个维度</span>
<span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="p">:</span><span class="err">在</span><span class="n">a原来维度索引</span><span class="o">-</span><span class="mi">1</span><span class="err">之</span><span class="err">后</span><span class="err">增</span><span class="err">加</span><span class="err">维</span><span class="err">度</span>
<span class="c1">#例如：</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.2</span><span class="p">,</span><span class="mf">1.3</span><span class="p">]</span><span class="p">)</span>  <span class="c1">#[2]</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="p">)</span>      <span class="c1">#[1,2]</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="p">)</span>     <span class="c1">#[2,1]</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>   <span class="c1">#如果要实现a和数据b的叠加，则需要对于数据b进行维度扩张</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


</code></pre></td></tr></table>
</div>
</div><p>4、维度删减squeeze()
对于维度的挤压squeeze，主要是挤压掉tensor数据中维度特征数为1的维度，如果不是1的话就不可以挤压</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>5、<strong>维度的扩展：expand(绝对扩展)/repeat(相对扩展</strong>)
#维度的扩张expand（绝对值）/repeat,repeat扩展实质是重复拷贝的次数-相对值，并且由于拷贝操作，原来的数据不能再用，已经改变，而expand是绝对扩展，其实现只能从1扩张到n，不能从M扩张到N，另外-1表示对该维度保持不变的操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#-1表示对维度保持不变</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>6、维度交换操作：</strong>
<strong>(1).t()操作</strong>：只可以对DIM=2的矩阵进行转置操作
<strong>(2)transpose</strong>操作：对不同的DIM均可以进行维度交换</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">a1</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a1</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
<span class="c1">#整体的变换顺序为a[b,c,h,w]-&gt;[b,w,h,c]-&gt;[b,w*h*c]-&gt;[b,w,h,c]-&gt;[b,c,h,w]</span>
</code></pre></td></tr></table>
</div>
</div><p>7、permute操作
相比于transpose只可以进行两个维度之间的一次交换操作，permute维度交换操作可以一步实现多个维度之间的交换(相当于transpose操作的多步操作)
#.t()和transpose/permute维度交换操作，需要考虑数据的信息保存，不能出现数据的污染和混乱.contiguous()操作保持存储顺序不变</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">c</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">a1</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a1</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">a1</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">a2</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span><span class="n">a2</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>对于以上的数据维度变换和索引切片训练代码如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dim</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="c1">#4</span>
<span class="c1">#索引与切片操作**</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([3, 28, 28])</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([2, 3, 28, 28]) -&gt; 第一个维度取0～1</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([2, 1, 28, 28]) -&gt; 第一个维度取0～1,第二个维度取0，第三，四个维度全取</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1"># torch.Size([2, 2, 28, 28]) -&gt; 第一个维度取0～1,第二个维度取1到最后一个（包括1），第三，四个维度全取</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([2, 3, 28, 28]) -&gt; 第一个维度取0～1,第二个维度取倒数第三到倒数第一（包括倒数第三），第三，四个维度全取</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([4, 3, 14, 14]) -&gt; 第一个维度全取,第二个维度全取，第三维度取0～27,间隔为2,第四维度取0～27,间隔为2</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">,</span><span class="p">:</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="p">:</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([4, 3, 14, 10]) -&gt; 第一个维度全取,第二个维度全取，第三维度取0～27,间隔为2,第四维度取0～27,间隔为3</span>

<span class="c1">#########################################################################################</span>

<span class="c1">#选择其中某维度的某些索引数据**</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#########################################################################################</span>

<span class="c1">#...操作表示自动判断其中得到维度区间**</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">.</span><span class="o">.</span><span class="o">.</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="o">.</span><span class="o">.</span><span class="p">,</span><span class="p">:</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">.</span><span class="o">.</span><span class="o">.</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#########################################################################################</span>

<span class="c1">#msaked_select**</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">mask</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>          <span class="c1">#选出所有元素中大于0.5的数据</span>
<span class="k">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span><span class="p">)</span>  <span class="c1">#选出所有元素中大于0.5的数据，并且输出时将其转换为了dim=1的打平tensor数据</span>

<span class="c1">#########################################################################################</span>

<span class="o">*</span><span class="o">*</span><span class="c1">#take函数的应用**:先将张量数据打平为一个dim=1的张量数据（依次排序下来成为一个数据列），然后按照索引进行取数据</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="p">,</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span><span class="p">]</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>


<span class="c1">#########################################################################################</span>

<span class="o">*</span><span class="o">*</span><span class="c1">#tensor数据的维度变换**</span>
<span class="o">*</span><span class="o">*</span><span class="c1">#view/reshape操作**：不进行额外的记住和存贮就会丢失掉原来的数据的数据和维度顺序信息，而这是非常重要的</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="c1">#########################################################################################</span>

<span class="o">*</span><span class="o">*</span><span class="c1">#squeeze/unsqueeze挤压和增加维度的操作**</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="p">[</span><span class="mf">1.2</span><span class="p">,</span><span class="mf">1.3</span><span class="p">]</span><span class="p">)</span>      <span class="c1">#[2]</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="p">)</span>          <span class="c1">#[1,2]</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="p">)</span>         <span class="c1">#[2,1]</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>   <span class="c1">#如果要实现a和数据b的叠加，则需要对于数据b进行维度扩张</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="c1">#########################################################################################</span>

<span class="o">*</span><span class="o">*</span><span class="c1">#维度的扩张expand（绝对值）/repeat**（重复拷贝的次数-相对值，并且由于拷贝操作，原来的数据不能再用，已经改变），只能从1扩张到n，不能从M扩张到N，另外***-1表示对该维度保持不变的操作***</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#-1表示对维度保持不变</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#########################################################################################</span>

<span class="o">*</span><span class="o">*</span><span class="c1">#.t()和transpose/permute*****维度交换操作，需要考虑数据的信息保存，不能出现数据的污染和混乱.contiguous()操作保持存储顺序不变***</span>
<span class="n">c</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">a1</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a1</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">a1</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">a2</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span><span class="n">a2</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="python-hasattr-函数">Python hasattr() 函数</h4>
<p>​    <strong>hasattr()</strong> 函数用于判断对象是否包含对应的属性。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">hasattr(object, name)
</code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>object &ndash; 对象。</p>
</li>
<li>
<p>name &ndash; 字符串，属性名</p>
</li>
<li>
<p>如果对象有该属性返回 True，否则返回 False。</p>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="c1">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>
 
<span class="k">class</span> <span class="nc">Coordinate</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>
    <span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
 
<span class="n">point1</span> <span class="o">=</span> <span class="n">Coordinate</span><span class="p">(</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">x</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">y</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">z</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">no</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>  <span class="c1"># 没有该属性</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="bp">True</span>
<span class="bp">True</span>
<span class="bp">True</span>
<span class="bp">False</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="python-getattr-函数">Python getattr() 函数</h4>
<p><strong>getattr()</strong> 函数用于返回一个对象属性值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">getattr(object, name[, default])
</code></pre></td></tr></table>
</div>
</div><ul>
<li>object &ndash; 对象。</li>
<li>name &ndash; 字符串，对象属性。</li>
<li>default &ndash; 默认返回值，如果不提供该参数，在没有对应属性时，将触发 AttributeError。</li>
<li>返回对象属性值。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;</span><span class="o">&gt;</span><span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span><span class="p">:</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span>     <span class="n">bar</span> <span class="o">=</span> <span class="mi">1</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span> 
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar</span><span class="s1">&#39;</span><span class="p">)</span>        <span class="c1"># 获取属性 bar 值</span>
<span class="mi">1</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar2</span><span class="s1">&#39;</span><span class="p">)</span>       <span class="c1"># 属性 bar2 不存在，触发异常</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span><span class="p">:</span>
  <span class="n">File</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">&lt;stdin&gt;</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">AttributeError</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">A</span><span class="s1">&#39;</span> <span class="nb">object</span> <span class="n">has</span> <span class="n">no</span> <span class="n">attribute</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar2</span><span class="s1">&#39;</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar2</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># 属性 bar2 不存在，但设置了默认值</span>
<span class="mi">3</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="python-setattr-函数">Python setattr() 函数</h4>
<p><strong>setattr()</strong> 函数对应函数 <a href="https://www.runoob.com/python/python-func-getattr.html" target="_blank" rel="noopener noreffer">getattr()</a>，用于设置属性值，该属性不一定是存在的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">setattr(object, name, value)
</code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>object &ndash; 对象。</p>
</li>
<li>
<p>name &ndash; 字符串，对象属性。</p>
</li>
<li>
<p>value &ndash; 属性值。</p>
</li>
<li>
<p>无返回值</p>
</li>
</ul>
<p>对已存在的属性进行赋值：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="o">&gt;&gt;</span><span class="o">&gt;</span><span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span><span class="p">:</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span>     <span class="n">bar</span> <span class="o">=</span> <span class="mi">1</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span> 
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar</span><span class="s1">&#39;</span><span class="p">)</span>          <span class="c1"># 获取属性 bar 值</span>
<span class="mi">1</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">setattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">bar</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>       <span class="c1"># 设置属性 bar 值</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">bar</span>
<span class="mi">5</span>

</code></pre></td></tr></table>
</div>
</div><p>如果属性不存在会创建一个新的对象属性，并对属性赋值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;</span><span class="o">&gt;</span><span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span>     <span class="n">name</span> <span class="o">=</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">runoob</span><span class="s2">&#34;</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span> 
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">setattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">age</span><span class="s2">&#34;</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">age</span><span class="p">)</span>
<span class="mi">28</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="python-lambda函数">python lambda函数</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">lambda</span> <span class="n">x</span> <span class="err">，</span> <span class="n">y</span> <span class="err">：</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
</code></pre></td></tr></table>
</div>
</div><p>等价为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="p">:</span>
	<span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="python--filter-函数">Python  filter() 函数</h4>
<p><strong>filter()</strong> 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。</p>
<p>该接收两个参数，<strong>第一个为函数，第二个为序列</strong>，<u>序列的每个元素作为参数传递给函数进行判断，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。</u></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">filter(function, iterable)
</code></pre></td></tr></table>
</div>
</div><ul>
<li>function &ndash;  判断函数。</li>
<li>iterable &ndash;  可迭代对象。</li>
</ul>
<p>过滤出列表中的所有奇数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="c1">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>
 
<span class="k">def</span> <span class="nf">is_odd</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span>
 
<span class="n">newlist</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">is_odd</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">newlist</span><span class="p">)</span>

<span class="c1">#[1, 3, 5, 7, 9]</span>
</code></pre></td></tr></table>
</div>
</div><p>或者</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">newlist</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span> <span class="p">:</span> <span class="n">n</span><span class="o">%</span><span class="mi">2</span><span class="o">==</span><span class="mi">1</span> <span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch-typeerror-cannot-assign-torchfloattensor-as-parameter-weight">[pytorch] TypeError cannot assign torch.FloatTensor as parameter weight</h4>
<p><a href="https://blog.csdn.net/weixin_35499984/article/details/106416197" target="_blank" rel="noopener noreffer">参考</a></p>
<p>TypeError: cannot assign ‘torch.FloatTensor’ as parameter ‘weight’ (torch.nn.Parameter or None expected)</p>
<p>在尝试赋值线性层权重时候出现的错误</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">atten_dim</span><span class="p">,</span> <span class="n">attribute</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span><span class="p">:</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MODEL</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>
    
	<span class="n">nclass</span><span class="p">,</span> <span class="n">smt_dim</span> <span class="o">=</span> <span class="n">attribute</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc_smt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">smt_dim</span><span class="p">,</span> <span class="n">nclass</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc_smt</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">attribute</span><span class="c1">#出错</span>
    <span class="k">for</span> <span class="n">para</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_smt</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
        <span class="n">para</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></td></tr></table>
</div>
</div><p>将tensor赋值给了线性层的权重，应该是parameter才能赋值，或者将tensor赋值给weight.data</p>
<p>解决方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc_smt</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">attribute</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch-中的-tensor--variable--parameter">Pytorch 中的 Tensor , Variable &amp; Parameter</h4>
<p><a href="https://www.jianshu.com/p/cb739922ce88" target="_blank" rel="noopener noreffer">参考</a></p>
<h4 id="pytorch自定义网络结构不进行参数初始化会怎样">pytorch自定义网络结构不进行参数初始化会怎样？</h4>
<p>pytorch自己默认有初始化</p>
<p><a href="https://blog.csdn.net/u011668104/article/details/81670544" target="_blank" rel="noopener noreffer">参考</a></p>
<h4 id="pytorch中的item用法">pytorch中的item()用法</h4>
<p>pytorch中，.item()方法 是得到一个元素张量里面的元素值
具体就是 用于<strong>将一个零维张量转换成浮点数</strong>，比如计算loss，accuracy的值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">G_losses</span> <span class="o">=</span> <span class="p">[</span><span class="p">]</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span>
<span class="n">errG</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="o">.</span><span class="o">.</span><span class="o">.</span>
<span class="n">G_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">errG</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch中tensor与各种图像格式的相互转化">Pytorch中Tensor与各种图像格式的相互转化</h4>
<p><a href="https://blog.csdn.net/qq_36955294/article/details/82888443" target="_blank" rel="noopener noreffer">参考</a></p>
<p><a href="https://stackoverflow.com/questions/54862480/topilimage-object-has-no-attribute-show" target="_blank" rel="noopener noreffer">参考</a></p>
<p>PIL转tensor</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="n">form</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="nn">Image</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="p">[</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(</span><span class="p">)</span><span class="p">]</span><span class="p">)</span>  
<span class="c1"># 输入图片地址</span>
<span class="c1"># 返回tensor变量</span>
<span class="k">def</span> <span class="nf">image_loader</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">RGB</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">loader</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

<span class="c1"># 输入PIL格式图片</span>
<span class="c1"># 返回tensor变量</span>
<span class="k">def</span> <span class="nf">PIL_to_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">loader</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><p>tensor转PIL</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">unloader</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="p">)</span>
<span class="c1"># 输入tensor变量</span>
<span class="c1"># 输出PIL格式图片</span>
<span class="k">def</span> <span class="nf">tensor_to_PIL</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="p">)</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span>
</code></pre></td></tr></table>
</div>
</div><p>实际应用中：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="k">def</span> <span class="nf">Test</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="c1"># TODO :修改代码</span>
        <span class="c1"># pred_monet_list = []</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="p">)</span>
        <span class="c1"># trans = transforms.ToPILImage()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">saving test images...</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_data_loader</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">test_data_loader</span><span class="o">.</span><span class="fm">__len__</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">photo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
                <span class="n">pred_monet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P2M</span><span class="p">(</span><span class="n">photo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(</span><span class="p">)</span>


            <span class="n">pred_monet</span><span class="o">=</span><span class="n">pred_monet</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="p">)</span>
            <span class="n">pred_monet</span><span class="o">=</span><span class="n">pred_monet</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">pred_monet</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="p">)</span><span class="p">(</span><span class="n">pred_monet</span><span class="p">)</span>
            <span class="n">pred_monet</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">/home/wag/ssl/kongfupainter/result/</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">.jpg</span><span class="s2">&#34;</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">saving test images end.</span><span class="s1">&#39;</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><p>numpy转tensor</p>
<p>注意tensor的维度是<code>C*H*W</code> ， numpy的维度的<code>H*W*C</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">toTensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">the img type is {}, but ndarry expected</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 255也可以改为256</span>
</code></pre></td></tr></table>
</div>
</div><p>tensor转numpy</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">tensor_to_np</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">(</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="instancenorm2d-load-error-unexpected-running-stats-buffers-modelmodel1model2running_mean">InstanceNorm2d Load Error: Unexpected running stats buffer(s) “model.model.1.model.2.running_mean“</h4>
<p><a href="https://blog.csdn.net/qazwsxrx/article/details/107094002" target="_blank" rel="noopener noreffer">参考</a></p>
<p>方法2的思路是：</p>
<ol>
<li>遍历保存的state_dict的所有keys；</li>
<li>使用字符串匹配的方法判断每一个key是否包括running_mean或running_var；</li>
<li>如果包括则从state_dict中删除掉该key；否则继续遍历。</li>
</ol>
<p>其中，判断字符串是否含有特定字符使用Python自带的.find()方法；删除key则使用del[].</p>
<p>代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">load_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">network_label</span><span class="p">,</span> <span class="n">epoch_label</span><span class="p">)</span><span class="p">:</span>
    <span class="c1"># pdb.set_trace()</span>
    <span class="n">save_filename</span> <span class="o">=</span> <span class="sa"></span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_net_</span><span class="si">%s</span><span class="s1">.pth</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch_label</span><span class="p">,</span> <span class="n">network_label</span><span class="p">)</span>  <span class="c1"># &#39;latest_net_G.pth&#39;</span>
    <span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">save_filename</span><span class="p">)</span>  <span class="c1"># &#39;./checkpoints/net_G.pth&#39;</span>
    <span class="k">try</span><span class="p">:</span>    <span class="c1"># 先尝试直接load</span>
        <span class="n">network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>     <span class="c1"># 如果报错，则删除running_mean和running var</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">running_mean</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">running_var</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
                <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="nb">sorted</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="p">)</span><span class="p">)</span><span class="p">)</span><span class="p">)</span><span class="p">)</span>
            
         <span class="n">network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="pytorch中modelmodules和modelchildren的区别">pytorch中model.modules()和model.children()的区别</h4>
<p>model.modules()和model.children()均为迭代器，model.modules()会遍历model中所有的子层，而model.children()仅会遍历当前层。</p>
<p>使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">(</span><span class="p">)</span><span class="err">：</span>



    <span class="k">print</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="c1"># model.modules()类似于 [[1, 2], 3],其遍历结果为：</span>



<span class="p">[</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>



 



<span class="c1"># model.children()类似于 [[1, 2], 3],其遍历结果为：</span>



<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="p">,</span> <span class="mi">3</span>
</code></pre></td></tr></table>
</div>
</div><p>也就是说，用model.children()进行初始化参数时，可能会漏掉部分，用model.modules()会遍历所有层</p>
<p>reference: <a href="https://discuss.pytorch.org/t/module-children-vs-module-modules/4551">https://discuss.pytorch.org/t/module-children-vs-module-modules/4551</a></p>
<h4 id="如何使用预训练的vgg模型">如何使用预训练的VGG模型</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.models.vgg</span> <span class="kn">as</span> <span class="nn">models</span>


<span class="k">class</span> <span class="nc">Pre_VGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>  <span class="c1"># perceptual_layers=3</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Pre_VGG</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="p">)</span>


        <span class="c1"># vgg = models.vgg19(pretrained=True).features</span>

        <span class="n">vgg19</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 加载VGG19模型</span>
        <span class="n">vgg19</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">/home/wag/ssl/deepfashion/vgg19-dcbb9e9d.pth</span><span class="s1">&#39;</span><span class="p">)</span><span class="p">)</span>  <span class="c1"># 赋予参数</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vgg</span> <span class="o">=</span> <span class="n">vgg19</span><span class="o">.</span><span class="n">features</span>

        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vgg</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 不需要训练</span>

    <span class="k">def</span> <span class="nf">get_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="p">:</span>  <span class="c1"># 这是提取VGG19对应特征层的特征</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">{</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">4</span><span class="s1">&#39;</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_1</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">9</span><span class="s1">&#39;</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_2</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">18</span><span class="s1">&#39;</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_3</span><span class="s1">&#39;</span><span class="p">}</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">{</span><span class="p">}</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">image</span>
        <span class="c1"># model._modules is a dictionary holding each module in the model</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span><span class="p">:</span><span class="c1">#遍历VGG中的所有层（所有可能组合的层数）</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
                <span class="n">features</span><span class="p">[</span><span class="n">layers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">features</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span><span class="p">:</span>
        <span class="n">sty_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vgg</span><span class="p">)</span>
        <span class="n">target_feature64</span> <span class="o">=</span> <span class="n">sty_fea</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_1</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="n">target_feature128</span> <span class="o">=</span> <span class="n">sty_fea</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_2</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="n">target_feature256</span> <span class="o">=</span> <span class="n">sty_fea</span><span class="p">[</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">pool_3</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">target_feature64</span><span class="p">,</span> <span class="n">target_feature128</span><span class="p">,</span> <span class="n">target_feature256</span>


<span class="n">net</span><span class="o">=</span><span class="n">Pre_VGG</span><span class="p">(</span><span class="p">)</span>
<span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">176</span><span class="p">)</span>
<span class="n">out1</span><span class="p">,</span><span class="n">out2</span><span class="p">,</span><span class="n">out3</span><span class="o">=</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([1, 64, 128, 88])</span>
<span class="k">print</span><span class="p">(</span><span class="n">out2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([1, 128, 64, 44])</span>
<span class="k">print</span><span class="p">(</span><span class="n">out3</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#torch.Size([1, 256, 32, 22])</span>


<span class="c1">#VGG19的层数有0,1,2,3,4,5,6,7,8,9...</span>
<span class="c1">#即name可能等于0,1,2,3,4,5,6,7,8,9...</span>
<span class="c1">#一旦name与layers中的“key”部分相等，则将VGG19该子层的输出赋予对应的“value”</span>
<span class="c1">#&#34;value&#34;用一个列表来存放</span>

</code></pre></td></tr></table>
</div>
</div><h4 id="mask的使用">mask的使用</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">mask</span><span class="p">[</span><span class="n">mask</span> <span class="o">&lt;</span> <span class="mf">0.9999</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span><span class="p">[</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="importlibimport_module的使用">importlib.import_module()的使用</h4>
<p>Python中动态导入对象importlib.import_module()的使用,<a href="https://blog.csdn.net/edward_zcl/article/details/88809212" target="_blank" rel="noopener noreffer">参考</a></p>
<p>背景</p>
<p>一个函数运行需要根据不同项目的配置，动态导入对应的配置文件运行。</p>
<p>解决</p>
<p><strong>文件结构</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">a #文件夹
	│a.py
	│__init__.py
b #文件夹
	│b.py
	│__init__.py
	├─c#文件夹
		│c.py
		│__init__.py

# c.py 中内容
args = {&#39;a&#39;:1}

class C:
    
    def c(self):
        pass
1234567891011121314151617
</code></pre></td></tr></table>
</div>
</div><p><strong>目的</strong>
<a href="http://xn--ac-qy2cu0kw8dyrg3vjup3a.py" target="_blank" rel="noopener noreffer">向a模块中导入c.py</a> 中的对象</p>
<p><strong>解决方案</strong></p>
<p><a href="http://a.py" target="_blank" rel="noopener noreffer">a.py</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">import importlib

params = importlib.import_module(&#39;b.c.c&#39;) #绝对导入
params_ = importlib.import_module(&#39;.c.c&#39;,package=&#39;b&#39;) #相对导入

# 对象中取出需要的对象
params.args #取出变量
params.C  #取出class C
params.C.c  #取出class C 中的c 方法
123456789
</code></pre></td></tr></table>
</div>
</div><p>以上就是动态函数import_module的使用方法</p>
<h4 id="__dict__的使用">__dict__的使用</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span><span class="p">:</span>
    <span class="sa"></span><span class="s2">&#34;&#34;&#34;</span><span class="s2">
</span><span class="s2"></span><span class="s2">    Class A.</span><span class="s2">
</span><span class="s2"></span><span class="s2">    </span><span class="s2">&#34;&#34;&#34;</span>

    <span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">static_test</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">class_test</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>
        <span class="k">pass</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">A</span><span class="o">.</span><span class="vm">__dict__</span>

<span class="n">mappingproxy</span><span class="p">(</span><span class="p">{</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">__module__</span><span class="s1">&#39;</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__main__</span><span class="s1">&#39;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__doc__</span><span class="s1">&#39;</span><span class="p">:</span> <span class="sa"></span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">    Class A.</span><span class="se">\n</span><span class="s1">    </span><span class="s1">&#39;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">a</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">b</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__init__</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">function</span> <span class="n">__main__</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">test</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">function</span> <span class="n">__main__</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">static_test</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="nb">staticmethod</span> <span class="n">at</span> <span class="mh">0x7ff8c81a7bd0</span><span class="o">&gt;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">class_test</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="nb">classmethod</span> <span class="n">at</span> <span class="mh">0x7ff8c81a7c10</span><span class="o">&gt;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__dict__</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">attribute</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__dict__</span><span class="s1">&#39;</span> <span class="n">of</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">A</span><span class="s1">&#39;</span> <span class="n">objects</span><span class="o">&gt;</span><span class="p">,</span>
              <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__weakref__</span><span class="s1">&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">attribute</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">__weakref__</span><span class="s1">&#39;</span> <span class="n">of</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">A</span><span class="s1">&#39;</span> <span class="n">objects</span><span class="o">&gt;</span><span class="p">}</span><span class="p">)</span>

<span class="n">obj</span><span class="o">=</span><span class="n">A</span><span class="p">(</span><span class="p">)</span>
<span class="n">obj</span><span class="o">.</span><span class="vm">__dict__</span>

<span class="p">{</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">a</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">b</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>


</code></pre></td></tr></table>
</div>
</div><p>由此可见， 类的静态函数、类函数、普通函数、全局变量以及一些内置的属性都是放在**类__dict__**里的。**对象的__dict__**中存储了一些self.xxx的一些东西。</p>
<p>以字典的形式返回</p>
<p>在 <a href="http://c.biancheng.net/python/" target="_blank" rel="noopener noreffer">Python</a> 类的内部，无论是类属性还是实例属性，都是以<strong>字典</strong>的形式进行存储的，其中属性名作为键，而值作为该键对应的值。</p>
<p>为了方便用户查看类中包含哪些属性，Python 类提供了 <strong>dict</strong> 属性。需要注意的一点是，该属性可以用类名或者类的实例对象来调用，用类名直接调用 <strong>dict</strong>，会输出该由类中所有类属性组成的字典；而使用类的实例对象调用 <strong>dict</strong>，会输出由类中所有实例属性组成的字典。</p>
<h4 id="python中items的用法">Python中items()的用法</h4>
<p>items() 方法的遍历：items() 方法把字典中每对 key 和 value 组成一个元组，并把这些元组放在列表中返回。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">one</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">two</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">three</span><span class="s1">&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span>
<span class="n">dict_items</span><span class="p">(</span><span class="p">[</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">one</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="p">,</span> <span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">two</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="p">,</span> <span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">three</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="p">]</span><span class="p">)</span>

<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span><span class="p">)</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">dict_items</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">&gt;</span>
 
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span><span class="p">:</span><span class="c1">#当两个参数时</span>
    <span class="k">print</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">:</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="p">)</span>
<span class="n">one</span><span class="p">:</span><span class="mi">1</span>
<span class="n">two</span><span class="p">:</span><span class="mi">2</span>
<span class="n">three</span><span class="p">:</span><span class="mi">3</span>
 
<span class="o">&gt;&gt;</span><span class="o">&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">(</span><span class="p">)</span><span class="p">:</span><span class="c1">#当参数只有一个时</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">one</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">two</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">three</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="staticmethod和classmethod的用法">@staticmethod和@classmethod的用法</h4>
<p><a href="https://blog.csdn.net/polyhedronx/article/details/81911548" target="_blank" rel="noopener noreffer">参考</a></p>
<p><strong>一般来说，要使用某个类的方法，需要先实例化一个对象再调用方法。</strong>  <strong>而使用@staticmethod或@classmethod，就可以不需要实例化，直接类名.方法名()来调用。</strong></p>
<p>既然@staticmethod和@classmethod都可以直接类名.方法名()来调用，那他们有什么区别呢
从它们的使用上来看,</p>
<ul>
<li>
<p>@staticmethod不需要表示自身对象的self和自身类的cls参数，就跟使用函数一样。</p>
</li>
<li>
<p>@classmethod也不需要self参数，但第一个参数需要是表示自身类的cls参数。</p>
<p>如果在@staticmethod中要调用到这个类的一些属性方法，只能直接类名.属性名或类名.方法名。
而@classmethod因为持有cls参数，可以来调用类的属性，类的方法，实例化对象等，避免硬编码。
下面上代码。</p>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span><span class="p">:</span>  
    <span class="n">bar</span> <span class="o">=</span> <span class="mi">1</span>  
    <span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="p">:</span>  
        <span class="k">print</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">foo</span><span class="s1">&#39;</span>  

    <span class="nd">@staticmethod</span>  
    <span class="k">def</span> <span class="nf">static_foo</span><span class="p">(</span><span class="p">)</span><span class="p">:</span>  
        <span class="k">print</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">static_foo</span><span class="s1">&#39;</span>  
        <span class="k">print</span> <span class="n">A</span><span class="o">.</span><span class="n">bar</span>  

    <span class="nd">@classmethod</span>  
    <span class="k">def</span> <span class="nf">class_foo</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span><span class="p">:</span>  
        <span class="k">print</span> <span class="sa"></span><span class="s1">&#39;</span><span class="s1">class_foo</span><span class="s1">&#39;</span>  
        <span class="k">print</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bar</span>  
        <span class="bp">cls</span><span class="p">(</span><span class="p">)</span><span class="o">.</span><span class="n">foo</span><span class="p">(</span><span class="p">)</span>  
<span class="c1">###执行  </span>
<span class="n">A</span><span class="o">.</span><span class="n">static_foo</span><span class="p">(</span><span class="p">)</span>  <span class="c1">#不需要实例化，直接通过类名调用静态方法</span>
<span class="n">A</span><span class="o">.</span><span class="n">class_foo</span><span class="p">(</span><span class="p">)</span>  
</code></pre></td></tr></table>
</div>
</div><h4 id="torchnnmodule和torchautogradfunction">torch.nn.Module和torch.autograd.Function</h4>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/95189376?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161053798116780277045932%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161053798116780277045932&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-2-95189376.pc_v1_rank_blog_v1&amp;utm_term=pytorch%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%93%E5%B1%95%E4%B9%8B%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94torch.nn.Module%E5%92%8Ctorch.autograd.Function" target="_blank" rel="noopener noreffer">参考</a></p>
<p>通过继承torch.nn.Function类来实现拓展</p>
<blockquote>
<ul>
<li>在有些操作通过组合pytorch中已有的层或者是已有的方法实现不了的时候，比如你要实现一个新的方法，这个新的方法需要forward和backward一起写，然后自己写对中间变量的操作。</li>
<li>需要重新实现__init__和forward函数，以及backward函数，需要自己定义求导规则；</li>
<li>不可以保存参数和状态信息</li>
</ul>
</blockquote>
<p><strong>总结：</strong> 当不使用自动求导机制，需要自定义求导规则的时候，就应该拓展torch.autograd.Function类。 否则就是用torch.nn.Module类，后者更简单更常用。</p>
<p><strong>autograd.Function类的定义</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">FunctionMeta</span><span class="p">,</span> <span class="n">_C</span><span class="o">.</span><span class="n">_FunctionBase</span><span class="p">,</span> <span class="n">_ContextMethodMixin</span><span class="p">,</span> <span class="n">_HookMixin</span><span class="p">)</span><span class="p">)</span><span class="p">:</span>
 
    <span class="fm">__call__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_FunctionBase</span><span class="o">.</span><span class="n">_do_forward</span>
    <span class="n">is_traceable</span> <span class="o">=</span> <span class="bp">False</span>
 
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="o">*</span><span class="n">kwargs</span><span class="p">)</span><span class="p">:</span>
 
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">)</span><span class="p">:</span>
</code></pre></td></tr></table>
</div>
</div><p>其实就是实现前向传播和反向传播两个函数。注意这里和Module类最明显的区别是它多了一个backward方法，这也是他俩<strong>最本质的区别：</strong></p>
<p>**（1）**torch.autograd.Function类实际上是某一个操作函数的父类，一个操作函数必须具备两个基本的过程，即<u>前向</u>的运算过程和<u>反向</u>的求导过程，</p>
<p>**（2）**torch.nn.Module类实际上是对torch.xxxx以及torch.nn.functional.xxxx这些函数的包装组合，而torch.xxxx和torch.nn.functional.xxxx都是实现了autograd.Function类的两个基本功能（前向运算和反向传播），如果是我们需要的某一个功能torch.xxxx和torch.nn.functional里面都没有，也不能通过组合得到，这就需要<u>定义新的操作函数，这个函数就需要继承自autograd.Function类，重写前向运算和反向传播</u>。（<strong>注意体会这段话</strong>）</p>
<p>**（3）**很显然，nn.Module更加高层，而autograd.Function更加底层，其实从名字中也能看出二者的区别，Module是针对模块的，即神经网络中的层、激活层、损失函数、网络模型等等，而Function是针对函数的，针对的是一些需要自己定义的函数而言的。如果某一个函数my_function继承自Function类，实现了这个类的forward和backward方法，那么我依然可以用nn.Module对这个自定义的的函数my_function进行包装组合，因为此时my_function跟torch.xxxx和torch.nn.functional.xxxx里面的函数已经具备了等同的地位了。（<strong>注意体会这段话</strong>），可以这么说，Module不仅包括了Function，还包括了对应的参数，以及其他函数与变量，这是Function所不具备的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">My_Function</span><span class="p">(</span><span class="n">Function</span><span class="p">)</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_for_backward</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span><span class="p">]</span>
        <span class="c1"># output = [对输入和参数进行的操作，其实就是前向运算的函数表达式]</span>
        <span class="k">return</span> <span class="n">output</span>
 
 <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span> <span class="c1"># 或者是self.saved_variables</span>
        <span class="c1"># grad_inputs = [求函数forward(input)关于 parameters 的导数，其实就是反向运算的导数表达式] * grad_output</span>
        <span class="k">return</span> <span class="n">grad_input</span>
</code></pre></td></tr></table>
</div>
</div><p>例</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>
 
<span class="c1"># 类需要继承Function类，此处forward和backward都是静态方法</span>
<span class="k">class</span> <span class="nc">MultiplyAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">)</span><span class="p">:</span>  
                                                             
    <span class="nd">@staticmethod</span>                                  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="p">:</span>                 
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>    <span class="c1">#保存参数,这跟前一篇的self.save_for_backward()是一样的</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
        <span class="k">return</span> <span class="n">output</span>                        
         
    <span class="nd">@staticmethod</span>                                 
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span><span class="p">:</span>    <span class="c1">#获取保存的参数,这跟前一篇的self.saved_variables()是一样的</span>
        <span class="n">w</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_variables</span>  
        <span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">=======================================</span><span class="s2">&#34;</span><span class="p">)</span>             
        <span class="n">grad_w</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">grad_x</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">w</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_b</span>  <span class="c1"># backward输入参数和forward输出参数必须一一对应</span>
 
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># x 是1，所以grad_w=1</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># w 是随机的，所以grad_x=随机的一个数</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># grad_b 恒等于1</span>
 
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">开始前向传播</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">z</span><span class="o">=</span><span class="n">MultiplyAdd</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>   <span class="c1"># forward,这里的前向传播是不一样的，这里没有使用函数去包装自定义的类，而是直接使用apply方法</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">开始反向传播</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="p">)</span>                   <span class="c1"># backward</span>
 
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="sa"></span><span class="s1">&#39;&#39;&#39;</span><span class="s1">运行结果为：</span><span class="s1">
</span><span class="s1"></span><span class="s1">开始前向传播</span><span class="s1">
</span><span class="s1"></span><span class="s1">开始反向传播</span><span class="s1">
</span><span class="s1"></span><span class="s1">=======================================</span><span class="s1">
</span><span class="s1"></span><span class="s1">tensor([0.1784]) tensor([1.]) tensor([1.])</span><span class="s1">
</span><span class="s1"></span><span class="s1">&#39;&#39;&#39;</span>
</code></pre></td></tr></table>
</div>
</div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2020-11-15</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="Share on Myspace" data-sharer="myspace" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://shilongshen.github.io/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="pytorch学习笔记"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/semantic-hierarchy-emerges-in-deep-generative-representations-for-scene-synthesis/" class="prev" rel="prev" title="Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis"><i class="fas fa-angle-left fa-fw"></i>Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis</a>
            <a href="/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="next" rel="next" title="python学习笔记">python学习笔记<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.63.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">shilongshen</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
