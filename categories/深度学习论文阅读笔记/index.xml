<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Ê∑±Â∫¶Â≠¶‰π†ËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞ - Category - ÊàëÁöÑ‰∏™‰∫∫ÂçöÂÆ¢</title>
        <link>https://shilongshen.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <description>Ê∑±Â∫¶Â≠¶‰π†ËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞ - Category - ÊàëÁöÑ‰∏™‰∫∫ÂçöÂÆ¢</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 19 Dec 2020 13:26:17 &#43;0800</lastBuildDate><atom:link href="https://shilongshen.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" rel="self" type="application/rss+xml" /><item>
    <title>video-based pose transfer method</title>
    <link>https://shilongshen.github.io/video-based-pose-transfer-method/</link>
    <pubDate>Sat, 19 Dec 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/video-based-pose-transfer-method/</guid>
    <description><![CDATA[Dance Dance Generation: Motion Transfer for Internet Videos ËØ•ÊñáÁ´†ÂèØ‰ª•ÂÆûÁé∞Âú®Â§çÊùÇËÉåÊôØ‰∏ãÁöÑpose transfer„ÄÇ
In summary, our contributions include the following.
  We demonstrate personalized motion transfer on videos from the Internet.
  We propose a novel two-stage frame-work to synthesize people performing new movements and fuse them seamlessly with background scenes. Ôºà‰∏ªË¶ÅË¥°ÁåÆÔºöÂÆûÁé∞Â§çÊùÇËÉåÊôØ‰∏ãÁöÑÂßøÊÄÅËΩ¨Êç¢Ôºâ
  We perform qualitative and quantitative evaluations validating the superiority of our method over existing state-of-the-art.
  method: (‰∏ªË¶ÅÁöÑÊÄùÊÉ≥ÊòØÂÖàÂ∞ÜÂà©Áî®ËØ≠‰πâÂàÜÂâ≤ÂõæÂ∞ÜÂâçÊôØ‰∏≠ÁöÑ‰∫∫Áâ©ËøõË°åÂàÜÂâ≤ÔºåÈááÁî®STN Â∞ÜÂâçÊôØ‰∫∫Áâ©‰∏éÁõÆÊ†á‰∫∫Áâ©ËøõË°åÂØπÈΩê„ÄÇÈöèÂêéÈÄöËøáÈò∂ÊÆµËøõË°å‰øÆÊ≠£Ôºâ]]></description>
</item><item>
    <title>group convolution.</title>
    <link>https://shilongshen.github.io/group-______/</link>
    <pubDate>Sat, 12 Dec 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/group-______/</guid>
    <description><![CDATA[ÂèÇËÄÉ
Á¨¨‰∏ÄÂº†Âõæ‰ª£Ë°®Ê†áÂáÜÂç∑ÁßØÊìç‰Ωú„ÄÇËã•ËæìÂÖ•ÁâπÂæÅÂõæÂ∞∫ÂØ∏‰∏∫ ÔºåÂç∑ÁßØÊ†∏Â∞∫ÂØ∏‰∏∫ ÔºåËæìÂá∫ÁâπÂæÅÂõæÂ∞∫ÂØ∏‰∏∫ ÔºåÊ†áÂáÜÂç∑ÁßØÂ±ÇÁöÑÂèÇÊï∞Èáè‰∏∫Ôºö „ÄÇÔºà‰∏Ä‰∏™Êª§Ê≥¢Âô®Âú®ËæìÂÖ•ÁâπÂæÅÂõæ Â§ßÂ∞èÁöÑÂå∫ÂüüÂÜÖÊìç‰ΩúÔºåËæìÂá∫ÁªìÊûú‰∏∫1‰∏™Êï∞ÂÄºÔºåÊâÄ‰ª•ÈúÄË¶Å ‰∏™Êª§Ê≥¢Âô®„ÄÇÔºâ
Á¨¨‰∫åÂº†Âõæ‰ª£Ë°®ÂàÜÁªÑÂç∑ÁßØÊìç‰Ωú„ÄÇÂ∞ÜËæìÂÖ•ÁâπÂæÅÂõæÊåâÁÖßÈÄöÈÅìÊï∞ÂàÜÊàê ÁªÑÔºåÂàôÊØèÁªÑËæìÂÖ•ÁâπÂæÅÂõæÁöÑÂ∞∫ÂØ∏‰∏∫ ÔºåÂØπÂ∫îÁöÑÂç∑ÁßØÊ†∏Â∞∫ÂØ∏‰∏∫ ÔºåÊØèÁªÑËæìÂá∫ÁâπÂæÅÂõæÂ∞∫ÂØ∏‰∏∫ „ÄÇÂ∞Ü ÁªÑÁªìÊûúÊãºÊé•(concat)ÔºåÂæóÂà∞ÊúÄÁªàÂ∞∫ÂØ∏‰∏∫ ÁöÑËæìÂá∫ÁâπÂæÅÂõæ„ÄÇÂàÜÁªÑÂç∑ÁßØÂ±ÇÁöÑÂèÇÊï∞Èáè‰∏∫ „ÄÇ
Ê∑±ÂÖ•ÊÄùËÄÉ‰∏Ä‰∏ãÔºåÂ∏∏ËßÑÂç∑ÁßØËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰∏äÔºåÊØè‰∏Ä‰∏™ÁÇπÊòØÁî±ËæìÂÖ•ÁâπÂæÅÂõæ ‰∏™ÁÇπËÆ°ÁÆóÂæóÂà∞ÁöÑÔºõËÄåÂàÜÁªÑÂç∑ÁßØËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰∏äÔºåÊØè‰∏Ä‰∏™ÁÇπÊòØÁî±ËæìÂÖ•ÁâπÂæÅÂõæ ‰∏™ÁÇπËÆ°ÁÆóÂæóÂà∞ÁöÑ„ÄÇËá™ÁÑ∂ÔºåÂàÜÁªÑÂç∑ÁßØÁöÑÂèÇÊï∞ÈáèÊòØÊ†áÂáÜÂç∑ÁßØÁöÑ „ÄÇ
 Â∞ÜËæìÂÖ•ÁâπÂæÅÂõæÊ≤øÁùÄÈÄöÈÅìÊñπÂêëËøõË°åÂàíÂàÜÔºåÂàÜÂâ≤Êàê‰∏çÂêåÁªÑÔºåÊØè‰∏ÄÁªÑÂàÜÂà´ÈááÁî®‰∏Ä‰∏™Âç∑ÁßØÂíåËøõË°åÂç∑ÂèäÊìç‰Ωú„ÄÇ
ËæìÂÖ•ÁâπÂæÅÂõæÁöÑÂ§ßÂ∞è„ÄÅËæìÂá∫ÁâπÂæÅÂõæÁöÑÂ§ßÂ∞èÂíåÊ†áÂáÜÁöÑÂç∑ÁßØ‰∏ÄÊ†∑ÔºåÊåáÁ§∫Âç∑ÁßØÊ†∏ÁöÑÂèÇÊï∞Êï¥‰ΩìÂáèÂ∞è‰∫Ü„ÄÇ
 Ê∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºàDepthwise separable convolutionÔºâ ËøôÂº†ÂõæÊÄé‰πàÂ∞ëÁöÑ‰∫ÜÂë¢Ôºö
Âõæ(a)‰ª£Ë°®Ê†áÂáÜÂç∑ÁßØ„ÄÇÂÅáËÆæËæìÂÖ•ÁâπÂæÅÂõæÂ∞∫ÂØ∏‰∏∫ ÔºåÂç∑ÁßØÊ†∏Â∞∫ÂØ∏‰∏∫ ÔºåËæìÂá∫ÁâπÂæÅÂõæÂ∞∫ÂØ∏‰∏∫ ÔºåÊ†áÂáÜÂç∑ÁßØÂ±ÇÁöÑÂèÇÊï∞Èáè‰∏∫Ôºö „ÄÇ
Âõæ(b)‰ª£Ë°®Ê∑±Â∫¶Âç∑ÁßØÔºåÂõæ(c)‰ª£Ë°®ÈÄêÁÇπÂç∑ÁßØÔºå‰∏§ËÄÖÂêàËµ∑Êù•Â∞±ÊòØÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØ„ÄÇÊ∑±Â∫¶Âç∑ÁßØË¥üË¥£Êª§Ê≥¢ÔºåÂ∞∫ÂØ∏‰∏∫(DK,DK,1)ÔºåÂÖ±M‰∏™Ôºå‰ΩúÁî®Âú®ËæìÂÖ•ÁöÑÊØè‰∏™ÈÄöÈÅì‰∏äÔºõÈÄêÁÇπÂç∑ÁßØË¥üË¥£ËΩ¨Êç¢ÈÄöÈÅìÔºåÂ∞∫ÂØ∏‰∏∫(1,1,M)ÔºåÂÖ±N‰∏™Ôºå‰ΩúÁî®Âú®Ê∑±Â∫¶Âç∑ÁßØÁöÑËæìÂá∫ÁâπÂæÅÊò†Â∞Ñ‰∏ä„ÄÇ
Ê∑±Â∫¶Âç∑ÁßØÂèÇÊï∞Èáè‰∏∫ ÔºåÈÄêÁÇπÂç∑ÁßØÂèÇÊï∞Èáè‰∏∫ ÔºåÊâÄ‰ª•Ê∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÂèÇÊï∞ÈáèÊòØÊ†áÂáÜÂç∑ÁßØÁöÑ „ÄÇ
‰∏∫‰∫Ü‰æø‰∫éÁêÜËß£„ÄÅ‰æø‰∫éÂíåÂàÜÁªÑÂç∑ÁßØÁ±ªÊØîÔºåÂÅáËÆæ „ÄÇÊ∑±Â∫¶Âç∑ÁßØÂÖ∂ÂÆûÂ∞±ÊòØ ÁöÑÂàÜÁªÑÂç∑ÁßØÔºåÂè™‰∏çËøáÊ≤°ÊúâÁõ¥Êé•Â∞Ü ÁªÑÁªìÊûúÊãºÊé•ÔºåÊâÄ‰ª•Ê∑±Â∫¶Âç∑ÁßØÂèÇÊï∞ÈáèÊòØÊ†áÂáÜÂç∑ÁßØÁöÑ „ÄÇÈÄêÁÇπÂç∑ÁßØÂÖ∂ÂÆûÂ∞±ÊòØÊääÁªÑÁªìÊûúÁî® conv ÊãºÊé•Ëµ∑Êù•ÔºåÊâÄ‰ª•ÈÄêÁÇπÂç∑ÁßØÂèÇÊï∞ÈáèÊòØÊ†áÂáÜÂç∑ÁßØÁöÑ „ÄÇ(*Âè™ËÄÉËôëÈÄêÁÇπÂç∑ÁßØÔºå‰πãÂâçËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰∏äÊØè‰∏Ä‰∏™ÁÇπÊòØÁî±ËæìÂÖ•ÁâπÂæÅÂõæ Âå∫ÂüüÂÜÖÁöÑÁÇπËÆ°ÁÆóÂæóÂà∞ÁöÑÔºõËÄåÈÄêÁÇπÂç∑ÁßØËæìÂá∫‰∏äÊØè‰∏Ä‰∏™ÁÇπÊòØÁî± Âå∫ÂüüÂÜÖÁöÑÁÇπËÆ°ÁÆóÂæóÂà∞ÁöÑ)„ÄÇ*Ëá™ÁÑ∂ÔºåÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÂèÇÊï∞ÈáèÊòØÊ†áÂáÜÂç∑ÁßØÁöÑ „ÄÇ]]></description>
</item><item>
    <title>Towards Fine-grained Human Pose Transfer with Detail Replenishing Network</title>
    <link>https://shilongshen.github.io/towards-fine-grained-human-pose-transfer-with-detail-replenishing-network/</link>
    <pubDate>Sat, 21 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/towards-fine-grained-human-pose-transfer-with-detail-replenishing-network/</guid>
    <description><![CDATA[Áé∞ÊúâÁöÑÂßøÊÄÅÂºïÂØº‰∏ãÂõæÂÉèÁîüÊàêÊñπÊ≥ïÂ≠òÂú®ÁùÄ‰∏â‰∏™ÈóÆÈ¢òÔºöÁªÜËäÇÁº∫Â§±ÔºåÂÜÖÂÆπÊ®°Á≥ä‰ª•ÂèäÈ£éÊ†º‰∏ç‰∏ÄËá¥ÔºåËøô‰∏•ÈáçÈôç‰ΩéÂèØÂõæÂÉèÁöÑË¥®Èáè‰ª•ÂèäÁúüÂÆûÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªÜÁ≤íÂ∫¶ÂßøÊÄÅÂºïÂØº‰∏ãÂõæÂÉèÁîüÊàêÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÊõ¥Âä†Ê≥®Èáç‰∫éËØ≠‰πâÁöÑÂÆåÊï¥ÊÄß‰ª•ÂèäÁªÜËäÇÁöÑË°•ÂÖÖÔºåËØ•ÊñπÊ≥ïÂ∞ÜÂÜÖÂÆπÂêàÊàê(local warping)ÂíåÁâπÂæÅËΩ¨Áßª(style transfer)ÁöÑÊ¶ÇÂøµ‰ª•Áõ∏‰∫íÂºïÂØºÁöÑÊñπÂºèÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁªÜËäÇË°•ÂÖÖÁΩëÁªúÔºàDRNÔºâ„ÄÇÊ≠§Â§ñËøòÊèêÂá∫‰∫Ü‰∏ÄÂ•óÁªÜÁ≤íÂ∫¶ËØÑ‰º∞ÊñπÊ≥ïÔºåÂåÖÊã¨‰∫ÜËØ≠‰πâÂàÜÊûê„ÄÅÁªìÊûÑÊ£ÄÊµãÂíåÊÑüÁü•Ë¥®ÈáèËØÑ‰º∞„ÄÇHPTÂíåFHPTÁöÑÊØîËæÉÂ¶Ç‰∏ãÂõæÔºö
A. HPT Methods Âü∫‰∫éÁâπÂæÅËΩ¨Êç¢Êú∫Âà∂ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÂ∑≤ÊúâÁöÑHPTÊñπÊ≥ïÂàÜ‰∏∫‰∏âÁ±ªÔºöglobal predictive methods„ÄÅlocal warping methods ‰ª•Âèä hybrid methods„ÄÇ
ËØ•ÂõæÂ±ïÁ§∫‰∫Ü‰∏âÁßçÊñπÊ≥ï‰∏çÂêå„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ªéËæìÂá∫ÁöÑÁªìÊûúÂàÜÊûêÂá∫Ëøô‰∏âÁßçÊñπÊ≥ïÂ≠òÂú®ÁöÑ‰∏§‰∏™Áº∫Èô∑Ôºö‰øùÂ≠òÂéüÂõæÁâáËØ≠‰πâÂíåÂ§ñËßÇÁªÜËäÇÁöÑËÉΩÂäõ‰ª•ÂèäÂú®ÈÅÆÊå°Âå∫ÂüüÂêàÊàêÊñ∞ÁöÑÂÜÖÂÆπÁöÑËÉΩÂäõ„ÄÇ
Global predictive methods ËØ•ÊñπÊ≥ïÂ∞ÜHPTËßÜ‰∏∫Â§öÊ®°ÊÄÅimage-to-image translationÈóÆÈ¢òÔºå‰ΩøÁî®ÂÖ∑ÊúâË∑≥Ë∑ÉËøûÊé•ÁöÑU-netÁΩëÁªúËøõË°åÁâπÂæÅÁöÑÂâçÂêë‰º†Êí≠„ÄÇÈÄöËøáÂ∞ÜÊèêÂèñÁöÑÂßøÊÄÅË°®Á§∫‰∏éÂéüÂõæÂÉèÂè†Âä†Êù•Â∞Ü‚ÄúÂßøÊÄÅÂºïÂØº‚ÄùÂºïÂÖ•„ÄÇ‰æãÂ¶Ç‰∏äÂõæ‰∏≠ÔºàaÔºâÂ∞ÜÂéüÂßøÊÄÅ$I_s$ÂíåÁõÆÊ†áÂßøÊÄÅ$I_t$ËøõË°åÁºñÁ†ÅÂπ∂‰∏éÂéüÂõæÂÉè$I_s$ËøõË°åÂè†Âä†ÔºåÁõ¥Êé•ÁîüÊàêÂÖ∑ÊúâÁõÆÊ†áÂßøÊÄÅÁöÑÂõæÂÉè$\bar{I_t}$„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πèÂáÜÁ°ÆÁöÑÂèòÂΩ¢Âª∫Ê®°ÔºåËøô‰∫õÂ∑•‰ΩúÂæÄÂæÄ‰∏çËÉΩÂèØÈù†Âú∞Ëß£ÂÜ≥[35]‰∏çÂêåÂßøÊÄÅ‰πãÈó¥ÁöÑÁªìÊûÑ‰∏çÂØπÈΩêÈóÆÈ¢ò„ÄÇÈÄöÂ∏∏Êù•ËØ¥Ôºåglobal predictive methods ‰ºöÁº∫‰πèÊçïËé∑Áõ∏Â∫îÂ±ÄÈÉ®ÁâπÂæÅÁöÑËÉΩÂäõÔºåËøô‰ºöÂØºËá¥ÁîüÊàêÂõæÂÉè‰∏≠ÁªÜËäÇÁº∫Â§±Ôºå‰æãÂ¶ÇÊ®°Á≥äÁöÑÁªÜËäÇÂíåÂ§±Áúü,Ëøá‰∫éÂπ≥ÊªëÁöÑË°£Êúç„ÄÇ
Local warping methods ËØ•ÊñπÊ≥ïÂèóÂà∞‰∫Üspatial transformer networksÁöÑÂêØÂèëÔºåÂ∞ÜdeformationÊûÑÂª∫Âú®ÁâπÂæÅÁöÑÂâçÂêë‰º†Êí≠‰∏≠„ÄÇ‰æãÂ¶ÇDSC‰∏≠‰ΩøÁî®part-wise ‰ªøÂ∞ÑÂèòÊç¢ÔºåPATN‰∏≠‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â¢ûÂä†deformation modelingÁöÑÁÅµÊ¥ªÊÄß„ÄÇ
‰∏çÂêåÂßøÊÄÅ‰πãÈó¥ÁöÑÂèòÂΩ¢Êò†Â∞ÑÈÄöÂ∏∏ÊòØÈÄöËøáÁõ∏Â∫îÊúâÈôêÁöÑÂßøÊÄÅÂÖ≥ÈîÆÁÇπÁöÑÊèíÂÄºÂÆûÁé∞ÁöÑ„ÄÇÂÅáËÆæÂéüÂõæÂÉè\ÁõÆÊ†áÂõæÂÉè‰∏≠‰∫∫‰ΩìÁöÑÂå∫Âüü‰∏∫$\Omega_s$ \ $\Omega_t$.Â∞Üdeformation ËßÜ‰∏∫‰∏Ä‰∏™ÂáΩÊï∞Ôºö$T_{st}:\Omega_s \rightarrow \Omega_t$,Â∞ÜÂå∫ÂüüÁî®ÂÖ≥ÈîÆÁÇπË°®Á§∫ÔºåÂàôÊúâ$T_{st}(p_s)=p_t$.ÈÄöÂ∏∏Êù•ËØ¥ËøôÊ†∑ÁöÑÊò†Â∞Ñ‰∏çÊòØÂîØ‰∏ÄÁöÑÔºåÈúÄË¶ÅÈÄöËøáÈ¢ùÂ§ñÁöÑÊ≠£ÂàôÈ°πÔºàblending energyÔºâÊù•ÂáèÁºìwarped contentsÁöÑÂ§±Áúü„ÄÇ
Áî±‰∫éËßÜËßíÂèòÂåñÂíåËá™ÈÅÆÊå°ÁöÑÊÉÖÂÜµÔºåÊó†Ê≥ï‰øùËØÅestimated warping Ë¶ÜÁõñÊï¥‰∏™target human body ,‰πüÂ∞±ÊòØËØ¥$\Omega_t-T_{st}\neq \phi$.Âõ†Ê≠§**local warping networkÂæàÈöæÊÅ¢Â§çÂéüÂõæÂÉè‰∏≠Ê≤°ÊúâÁ≤æÁ°ÆÂØπÂ∫îÁöÑunderlying contentÔºåËøô‰ºöÂØºËá¥ÂÜÖÂÆπÁöÑÊ®°Ê£±‰∏§ÂèØ**„ÄÇ
 Áî±‰∫éËßÜËßíÂèòÂåñÂíåËá™ÈÅÆÊå°ÁöÑÊÉÖÂÜµÔºåÊó†Ê≥ï‰øùËØÅestimated warping Ë¶ÜÁõñÊï¥‰∏™target human body ÁöÑÁêÜËß£Ôºö
Êàë‰ª¨Âª∫Á´ãwarping ÊúÄÁªàÁöÑÁõÆÁöÑÂ∞±ÊòØÂ∞ÜÂéüÂõæÂÉèËøõË°åÂΩ¢ÂèòÊìç‰Ωú„ÄÇÂÆûÈôÖÊòØÂèØ‰ª•Â∞ÜÁõÆÊ†áÂõæÂÉèÁúã‰ΩúÊòØÂéüÂõæÂÉèÁöÑÂΩ¢ÂèòÁâàÊú¨ÔºàÁî±‰∫éÂßøÊÄÅÁöÑÂèòÂåñÔºâ„ÄÇ
Âª∫Á´ãÁöÑwarpingÔºå ‰æãÂ¶ÇÂÖâÊµÅÊ≥ïÔºåÂÆûÈôÖ‰∏äÂ∞±ÊòØÂØªÊâæÂéüÂõæÂÉèÂíåÁõÆÊ†áÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Â∞±ÊòØÁõÆÊ†áÂõæÂÉè‰∏≠ÂÉèÁ¥†ÁÇπÂØπÂ∫îÁöÑÊòØÂéüÂõæÂÉèÁöÑÂì™‰∏Ä‰∏™ÂÉèÁ¥†ÁÇπÔºåÂ¶Ç‰∏äÂõæ‰∏≠Á∫¢ÁÇπÊâÄÁ§∫ÔºåÊàñËÄÖÊõ¥ÂáÜÁ°ÆÁöÑËØ¥Â∫îËØ•ÊòØÔºöÁõÆÊ†áÂõæÂÉèÁöÑÂÉèÁ¥†ÁÇπÊòØÊ†πÊçÆÂéüÂõæÂÉè‰∏≠ÁöÑÂÉèÁ¥†ÁÇπÈááÊ†∑ËÄåÊù•ÁöÑ„ÄÇ
‰ΩÜÊòØÂ≠òÂú®ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÊòØ„ÄÇÂú®pose-guided person generation ‰ªªÂä°‰∏≠ÔºåÁî±‰∫éÂßøÊÄÅÁöÑÂèòÂåñÔºåÂèØËÉΩ‰ºöÂØºËá¥ÁõÆÊ†áÂõæÂÉè‰∏≠ÁöÑÂÉèÁ¥†ÁÇπÂú®ÂéüÂõæÂÉè‰∏≠Êâæ‰∏çÂà∞ÂØπÂ∫îÁöÑÂÉèÁ¥†ÁÇπ„ÄÇÂ¶Ç‰∏äÂõæ‰∏≠ËìùÁÇπÊâÄÁ§∫„ÄÇÊç¢‰∏™ËØ¥Ê≥ïÂ∞±ÊòØÊó†Ê≥ï‰øùËØÅestimated warping Ë¶ÜÁõñÊï¥‰∏™target human body„ÄÇËøô‰∏™Êó∂ÂÄôÁîüÊàêÂõæÂÉè‰∏≠ÁöÑÊüê‰∏Ä‰∫õÈÉ®‰ΩçÂèØËÉΩÂ∞±‰ºö‰∫ßÁîüÂÜÖÂÆπÁöÑÊ®°Ê£±‰∏§ÂèØ„ÄÇÊàñËÄÖËØ¥ÊòØ‰∫ßÁîüholeÊàñËÄÖËØ¥Êòéunable to generate new contents„ÄÇ]]></description>
</item><item>
    <title>A Style-Based Generator Architecture for Generative Adversarial Networks</title>
    <link>https://shilongshen.github.io/a-style-based-generator-architecture-for-generative-adversarial-networks/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/a-style-based-generator-architecture-for-generative-adversarial-networks/</guid>
    <description><![CDATA[ÂèÇËÄÉÔºö
ÂèÇËÄÉ1
ÂèÇËÄÉ2
 ËØ•ÊñáÁ´†Âü∫‰∫éGANËÆæËÆ°È£éÊ†ºËøÅÁßªÊ®°Âûã„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üËá™Âä®Â≠¶‰π†ÔºåÊó†ÁõëÁù£ÁöÑÂàÜÁ¶ªÈ´òÂ±ÇÂ±ûÊÄßÔºà‰æãÂ¶Ç poseÔºåidentityÔºâ‰ª•ÂèäËÉΩÂØπÁîüÊàêÂõæÂÉèËøõË°åÈöèÊú∫ÂèòÂåñÔºà‰æãÂ¶ÇÈõÄÊñëÔºåÂ§¥ÂèëÔºâÂπ∂‰∏î ËÉΩÂ§üÂØπÁîüÊàêÂõæÂÉèÁöÑÂàÜËæ®ÁéáËøõË°åÊéßÂà∂„ÄÇ
 StyleGAN‰∏≠ÁöÑstyleÂÄüÁî®Ëá™ÂõæÂÉèÈ£éÊ†ºËøÅÁßªÔºåËøôÁØáÊñáÁ´†ÊòØPG-GAN‰πãÂêéÁöÑÔºå‰∏ªË¶ÅÊîπÂèò‰∫ÜÁîüÊàêÂô®ÁöÑÁªìÊûÑÂÆûÁé∞Êó†ÁõëÁù£Âú∞ÁîüÊàêÂèØÊéßÊÄßÂº∫ÁöÑÂõæÂÉè„ÄÇ
PG-GAN‰∏≠ÊòØÈÄöËøá‰∏ÄÂ±ÇÂ±ÇÂú∞ÁªôÁîüÊàêÂô®ÂíåÂà§Âà´Âô®Â¢ûÊ∑ªÂç∑ÁßØÂ±ÇÂêåÊó∂ÊèêÈ´òÂàÜËæ®ÁéáÁöÑÊñπÊ≥ïÊù•ÁîüÊàêÈ´òË¥®ÈáèÂõæÂÉèÁöÑ„ÄÇStyleGANÁöÑmotivationÂ∞±ÊòØÂèëÁé∞ËøôÁßçÊ∏êËøõÁöÑÊñπÊ≥ïÂÖ∂ÂÆûËÉΩÊéßÂà∂ÂõæÂÉèÁöÑ‰∏çÂêåÁâπÊÄßÔºå‰ΩéÂàÜËæ®Áéá‰πüÂ∞±ÊòØcoarseÂ±Ç‰∏ªË¶ÅÂΩ±ÂìçÂõæÂÉèÁöÑÂßøÊÄÅÔºåËÑ∏ÂûãÁ≠âÈ´òÁ∫ßÁâπÂæÅÔºåÈ´òÂàÜËæ®Áéá‰∏ªË¶ÅÂΩ±ÂìçËÉåÊôØÂèëËâ≤Á≠â‰ΩéÁ∫ßÁâπÂæÅ(ÁªìÂêàÊÑüÂèóÈáéÁöÑÊ¶ÇÂøµÁêÜËß£Ôºå‰ΩéÂàÜËæ®ÁéáÁöÑÁâπÂæÅËé∑ÂèñÊõ¥Â§öÂÖ®Â±Ä‰ø°ÊÅØÂπ∂Âä†‰ª•Â§ÑÁêÜÔºåÊØîÂ¶ÇÁâπÂæÅ‰ªéÁ∫øÊù°ÁªèËøáÂ§öÊ¨°Âç∑ÁßØ‰∏ÄÊ≠•Ê≠•‰ºöÁªÑÂêàÂá∫ÂΩ¢Áä∂Ôºå‰ºöÊõ¥Âä†ÊäΩË±°ÔºåÂÖ∑ÊúâÊõ¥È´òÁ∫ßÁöÑËØ≠‰πâ‰ø°ÊÅØ)„ÄÇ‰ΩÜPG-GANËøôÁßçÁªìÊûÑÂú®ÈÄíËøõÊ∑ªÂä†Â±ÇÁöÑÊó∂ÂÄôÊ≤°Êúâ‰ªª‰ΩïÊéßÂà∂Êù°‰ª∂ÔºåÂØºËá¥Êï¥‰ΩìÁöÑÁâπÂæÅÂíåÁªÜÂæÆÁöÑÁâπÂæÅÈó¥Â≠òÂú®ËÄ¶ÂêàÔºåËÄ¶ÂêàÂ∞±ÂØºËá¥‰∫ÜÂõæÂÉèÂèØÊéßÊÄßÂ∑ÆÔºåÊ≤°ÂäûÊ≥ïÂØπÂçï‰∏™ÁâπÂæÅËøõË°åË∞ÉËäÇ„ÄÇStyleGAN‰∫éÊòØÂØªÊâæ‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£‰ΩÜÂèàÂèØÊéßÊÄßÂº∫ÁöÑÊñπÊ≥ïÔºöÂØπ‰∏çÂêålevelÁöÑÂç∑ÁßØÂ±ÇËøõË°åÊìç‰Ωú„ÄÇ
ProGANÁöÑÁΩëÁªúÊ°ÜÊû∂Ôºö
 Âú®‰πãÂâçÁöÑ‰º†ÁªüÁöÑÁîüÊàêÁΩëÁªú‰∏≠ÔºåÊúâ‰∏Ä‰∏™ÈóÆÈ¢òÂ≠òÂú®ÔºåÂ∞±ÊòØÁâπÂæÅËÄ¶ÂêàÔºåÈÇ£‰πàËøô‰∏™ÊòØ‰ªÄ‰πàÂë¢ÔºüÂ¶Ç‰∏äÂõæÂ∑¶ËæπÔºå‰º†ÁªüÁöÑÁîüÊàêÁΩëÁªúÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÔºåÂÖ∂ËæìÂÖ•‰∏Ä‰∏™latent Z
ZÔºå‰∏ÄËà¨‰∏∫512ÁöÑ‰∏Ä‰∏™ÂêëÈáè„ÄÇÂÅáËÆæËøô‰∏™ÂêëÈáè‰ª£Ë°®‰∏Ä‰∏™‰∫∫ÁöÑËÑ∏ÔºåÈÇ£‰πàËØ•ÂêëÈáèÂ∞±‰ºöÂ≠òÂú®ÁâπÂæÅËÄ¶Âêà„ÄÇÂõ†‰∏∫‰∏Ä‰∏™ËÑ∏ÊòØÊúâÂæàÂ§öÁâπÂæÅÁöÑÔºåÂπ∂‰∏çÊòØ‰∏Ä‰∏™512Áª¥ÁöÑÂêëÈáèÂ∞±ËÉΩÂÆåÂÖ®Ë°®Á§∫ÁöÑÔºåÂ¶ÇÊûúÈùûË¶ÅË°®Á§∫ÔºåÂè™ËÉΩÊòØËøôÁßçÊñπÂºèÔºåÂ¶Ç‰∏ãÔºö
ÂÅáËÆæÔºöÁª¥Â∫¶1‰ª£Ë°®Â§¥ÂèëÁ≤óÁªÜÔºåÁª¥Â∫¶2‰ª£Ë°®ÁöÆËÇ§È¢úËâ≤ÔºåÁª¥Â∫¶3‰ª£Ë°®ÈºªÂ≠êÂ§ßÂ∞è‚Ä¶ÔºåÂΩì512‰∏™Áª¥Â∫¶ÂÖ®ÈÉ®ÊòØÁî®ÂÆå‰πãÂêéÔºåÂÖ∂Âè™ËÉΩÈÄöËøáÂ§ö‰∏™Áª¥Â∫¶ÂÜçÂéªË°®Á§∫ÂÖ∂‰ªñÁöÑÁâπÂæÅÔºåÂ¶ÇÔºöÁª¥Â∫¶1‰∏éÁª¥Â∫¶2ÁªºÂêàËµ∑Êù•Ë°®Á§∫‰∫ÜÂ§¥ÂèëÁöÑÈ¢úËâ≤„ÄÇËøôÊ†∑ÔºåÈÄöËøá‰∏§‰∏™ÊàñËÄÖÂ§ö‰∏™ÁªÑÂêàÔºå512ÁöÑÂêëÈáèÔºåÂ∞±ËÉΩË°®Á§∫Âá∫Êé•ËøëÊó†Êï∞ÁöÑÁâπÂæÅ„ÄÇ
‰ΩÜÊòØËøôÊ†∑Â∞±ÂæàÊòéÊòæÂá∫Áé∞‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºåÈÇ£Â∞±ÊòØÊàë‰ª¨ÊÄé‰πàÂéªÊéßÂà∂Êàë‰ª¨ÊÉ≥Ë¶ÅÂõæÁâáÁöÑÂçï‰∏™ÁâπÂæÅÂë¢ÔºüÂ¶ÇÔºåÊàëÂè™ÊÉ≥ÊîπÂèòÂ§¥ÂèëÁöÑÁ≤óÁªÜÔºå‰ΩÜÊòØÂèà‰∏çËÉΩÁõ¥Êé•Âéª‰øÆÊîπÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶ÔºåÂõ†‰∏∫Á¨¨‰∏Ä‰∏™Áª¥Â∫¶‰ºöÂΩ±ÂìçÂÖ∂‰ªñÁöÑÁª¥Â∫¶ÔºåÂèØËÉΩ‰ºöÂΩ±ÂìçÂà∞ÁöÆËÇ§ÁöÑÈ¢úËâ≤Ôºå‰ΩÜÊòØÊúâÁöÑÊó∂ÂÄôÔºåÊàëÂÅèÂÅèÈúÄË¶ÅËØ•Ë°®‰ªñÂ§¥ÂèëÁöÑÈ¢úËâ≤Ôºå‰∏ç‰øÆÊîπÁöÆËÇ§ÁöÑÈ¢úËâ≤„ÄÇ‰∏çÂêåÁöÑÁâπÂæÅÊòØ‰∫íÁõ∏ÂÖ≥ËÅîÁöÑÔºàÁâπÂæÅËÄ¶ÂêàÔºâ„ÄÇ
 1 Introduction GANËÉΩÂ§üÁîüÊàêÈ´òÂàÜËæ®ÁéáÂíåÈ´òË¥®ÈáèÁöÑÂõæÂÉèÔºå‰ΩÜÊòØÁîüÊàêÂô®‰ªçÁÑ∂Ë¢´ËßÜ‰∏∫‰∏Ä‰∏™ÈªëÁÆ±Â≠ê„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÂ∑≤ÁªèÊúâ‰∫Ü‰∏Ä‰∫õÂ∑•‰ΩúÔºå‰ΩÜÊòØÂØπÂõæÂÉèÁîüÊàêËøáÁ®ã‰∏≠ÂèòÂåñÂõ†Á¥†ÁöÑÁêÜËß£Ôºå‰æãÂ¶ÇÈöèÊú∫ÁâπÂæÅÁöÑËµ∑Ê∫êÔºå‰ªçÁÑ∂ÊòØÁº∫‰πèÁöÑ„ÄÇlatent spaceÁöÑÊÄßË¥®‰πüÊ≤°ÊúâÂæàÂ•ΩÁöÑË¢´ÁêÜËß£„ÄÇ
ËØ•ÊñáËÆæËÆ°‰∫Ü‰∏Ä‰∏™ËÉΩÂ§üËÆæËÆ°‰∫Ü‰∏Ä‰∏™ËÉΩÂ§üÊéßÂà∂ÁîüÊàêËøáÁ®ãÁöÑÁîüÊàêÂô®Ê®°Âûã„ÄÇÁîüÊàêÂô®ÁöÑËæìÂÖ•‰∏∫‰∏Ä‰∏™learned constantÔºåÊ†πÊçÆlatent codeÂú®ÊØè‰∏Ä‰∏™Âç∑ÁßØÂ±ÇË∞ÉÊï¥ÂõæÂÉèÁöÑstyleÔºå‰ªéËÄåÁõ¥Êé•ÊéßÂà∂‰∏çÂêåÂ∞∫Â∫¶‰∏ãÂõæÂÉèÁâπÂæÅÁöÑÂº∫Â∫¶„ÄÇÈÄöËøáÂ∞ÜnoiseÁõ¥Êé•ÈÄÅÂÖ•ÁΩëÁªú‰∏≠ÔºåÊù•ÂÆûÁé∞‰ªéÈöèÊú∫ÂèòÂåñÔºàÔºâ‰∏≠ËøõË°åÈ´òÂ±ÇÂ±ûÊÄßÔºàÔºâÁöÑËá™Âä®„ÄÅÊó†ÁõëÁù£ÂàÜÁ¶ªÔºåÂπ∂ÂÆûÁé∞Áõ¥ËßÇÁöÑÁâπÂÆöÂ∞∫Â∫¶ÁöÑÊ∑∑ÂêàÂíåÊèíÂÄºÊìç‰Ωú„ÄÇ
ÁîüÊàêÂô®È¶ñÂÖàÂ∞Üinput latent codeÊò†Â∞ÑÂà∞intermediate latent spaceÔºåËøôÂØπ‰∫éfactors of variationÂú®ÁΩëÁªú‰∏≠ÊòØÂ¶Ç‰ΩïË°®ËææÂà∞ÊúâÂæàÈáçË¶ÅÁöÑÂΩ±Âìç„ÄÇinput latent codeÁöÑÂàÜÂ∏ÉÔºàinput latent spaceÔºâÊòØÁ¨¶Âêàtraining dataÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÁöÑÔºåËøô‰ºöÂØºËá¥ÊüêÁßçÁ®ãÂ∫¶‰∏ä‰ø°ÊÅØÁöÑËÄ¶Âêà„ÄÇËÄåintermediate latent spaceÊòØÊ≤°ÊúâËøô‰∏ÄÈôêÂà∂ÁöÑÔºåÂõ†Ê≠§ÂèØ‰ª•ËøõË°å‰ø°ÊÅØÁöÑËß£ËÄ¶„ÄÇËØ•ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑËØÑ‰º∞latent spaceËß£ËÄ¶Á®ãÂ∫¶ÁöÑÊñπÊ≥ïÔºöÊÑüÁü•Ë∑ØÂæÑÈïøÂ∫¶ÂíåÁ∫øÊÄßÂèØÂàÜÁ¶ªÊÄß„ÄÇËØ•ÊñáÁöÑÊñπÊ≥ïÂÖ∑ÊúâÊõ¥È´òÁöÑÁ∫øÊÄßÂ∫¶ÂíåÊõ¥Â∞ëÁöÑËÄ¶ÂêàÂ∫¶„ÄÇ
Âπ∂‰∏îÊèêÂá∫‰∫Ü‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑdataset of human facesÔºàFlickr-Faces-HQ, FFHQÔºâ„ÄÇ
2 style-based generator ‰º†ÁªüÁöÑÊñπÊ≥ïÊòØÂ∞Üinput codeÈÄöËøáinput layerÈÄÅÂÖ•ÁîüÊàêÂô®„ÄÇËØ•ÊñáÊäõÂºÉ‰∫ÜËøô‰∏™ËÆæËÆ°ÔºåÂÆåÂÖ®ÂøΩÁï•‰∫ÜËæìÂÖ•Â±ÇÔºåËÄåÊòØ‰ªé‰∏Ä‰∏™learned constantÂºÄÂßã„ÄÇÁªôÂÆölatent space $\mathcal{Z}$ ‰∏≠ÁöÑ‰∏Ä‰∏™latent code $z$ÔºåÈùûÁ∫øÊÄßÊò†Â∞ÑÁΩëÁªú $f:\mathcal{Z \rightarrow W }$, Â∞Ü$z$Êò†Â∞Ñ‰∏∫$w$Ôºö$w=f(z)$.ÂÖ∂‰∏≠$f$ ‰∏∫8Â±ÇÁöÑMLP„ÄÇÈöèÂêéÂ∞Ü$w$ÂèòÊç¢‰∏∫styles:$y=(y_s,y_b)$Êù•‰Ωú‰∏∫AdaINÁöÑ‰ªøÂ∞ÑÂèÇÊï∞Ôºö $$ AdaIN(x_i,y)=y_{(s,i)}{\large(}\frac{x_i-\mu(x_i)}{\sigma(x_i)}{\large)}+y_{(b,i)} $$
 $$ z\rightarrow w \rightarrow y (‰ªøÂ∞ÑÂèÇÊï∞)\rightarrow style\]]></description>
</item><item>
    <title>A Variational U-Net for Conditional Appearance and Shape Generation</title>
    <link>https://shilongshen.github.io/a-variational-u-net-for-conditional-appearance-and-shape-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/a-variational-u-net-for-conditional-appearance-and-shape-generation/</guid>
    <description><![CDATA[ËØ•ÊñπÊ≥ïËÉΩÂ§üÁîüÊàê‰∏çÂêåÁöÑÂßøÊÄÅÁöÑ‰∫∫Áâ©ÂõæÂÉè‰ª•ÂèäÊîπÂèò‰∫∫Áâ©ÁöÑÂ§ñËßÇ„ÄÇËÄå‰∏îËøô‰∏™Ê®°ÂûãËÉΩÂ§üÂú®‰∏çÊîπÂèòshapeÁöÑÊÉÖÂÜµ‰∏ã‰ªéappearance distribution ‰∏≠ËøõË°åÈááÊ†∑„ÄÇ
1 Approach ËÆ∞$x$‰∏∫dataset $X$‰∏≠ÁöÑ‰∏ÄÂº†ÂõæÁâáÔºåÊàë‰ª¨ÊÉ≥Ë¶ÅÁêÜËß£$x$‰∏≠ÁöÑobjectÊòØÂ¶Ç‰ΩïË¢´ÂÖ∂shape $y$Âíåappearance $z$ÊâÄÂΩ±ÂìçÁöÑ„ÄÇÂõ†Ê≠§ÂõæÂÉèÁîüÊàêÂô®ÂèØ‰ª•Ë¢´Ë°®Á§∫‰∏∫ÊúÄÂ§ßÂåñÂêéÈ™åÊ¶ÇÁéáÔºàÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºöÁªôÂÆö$y$Âíå$z$ÔºåÂì™Áßç$x$ÊúÄÊúâÂèØËÉΩÂèëÁîü„ÄÇÔºâ $$ arg\ max\ p(x|y,z) $$
1.1 VAE based on latent shape and appearance $p(x|y,z)$ÂèØ‰ª•ËßÜ‰∏∫ÈöêÂèòÈáè(Âê´‰∏§‰∏™ÈöêÂèòÈáè)ÁöÑÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•Ê±ÇÂæóËøô‰∏™ÁîüÊàêÊ®°ÂûãÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É$p(x,y,z)$„ÄÇ
$\because$ $$ p(x|y,z)=\frac{p(x,y,z)}{p(y,z)} $$
$\therefore$ $$ p(x,y,z)=p(x|y,z)p(y,z) $$ Âê´ÈöêÂèòÈáèÁöÑÊ¶ÇÁéáÂØÜÂ∫¶‰º∞ËÆ°ÂèØ‰ª•ÈááÁî®VAEÁöÑÊñπÊ≥ïËøõË°åÊ±ÇËß£ÔºåÊ±ÇËß£ËøáÁ®ãÂåÖÂê´‰∫Ü‰∏§‰∏™Ê≠•È™§Êé®Êñ≠ÂíåÁîüÊàê„ÄÇÂÆûÈôÖ‰∏äÊàë‰ª¨ÊúÄÁªàÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÊ±ÇÂá∫ÂõæÂÉè$x$ÁöÑÂàÜÂ∏É$p(x,\theta),\theta$‰∏∫ÂèÇÊï∞ÔºåÁªôÂÆöÊ†∑Êú¨$x$ÔºåÂÖ∂ÂØπÊï∞ËæπÈôÖ‰ººÁÑ∂ÂáΩÊï∞‰∏∫ $$ \log p(x,\theta)=ELBO(q,x,\theta,\phi)+KL[q(y,z|x;\phi),p(y,z|x;\theta)] $$ ÂÖ∂‰∏≠$q(y,z|x;\phi)$‰∏∫ÂèòÂàÜÂØÜÂ∫¶ÂáΩÊï∞Ôºå$\phi$‰∏∫ÂèÇÊï∞ÔºåELBO‰∏∫ËØÅÊçÆ‰∏ãÁïåÔºö $$ ELBO(q,x,\theta,\phi)=\mathbb{E}_q\log \frac{p(x|y,z)p(y,z)}{q(y,z|x;\phi)} $$
 $$ \begin{aligned} \log p(x)&amp;=\log \int p(x,y,z)dz\ dy\
&amp;=\log \int \frac{p(x,y,z)}{q(y,z|x)}q(y,z|x)dz\ dy\
&amp;\geq \int q(y,z|x)\log \frac{p(x,y,z)}{q(y,z|x)}dz\ dy\
&amp;= \mathbb{E}_q\log \frac{p(x,y,z)}{q(y,z|x;\phi)}\
&amp;=\mathbb{E}_q\log \frac{p(x|y,z)p(y,z)}{q(y,z|x;\phi)}
\end{aligned} $$
ÂÆûÈôÖ‰∏ä $$ \begin{aligned} \log p(x)&amp;=\int q(y,z|x)\log \frac{p(x,y,z)}{q(y,z|x)}dz\ dy-\int q(y,z|x)\log \frac{p(y,z|x)}{q(y,z|x)}dz\ dy\]]></description>
</item><item>
    <title>AMT perceptual study</title>
    <link>https://shilongshen.github.io/amt-perceptual-studies/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/amt-perceptual-studies/</guid>
    <description><![CDATA[ÂèÇËÄÉÊñáÁåÆ
Ê≥®ÊÑèÂà∞ÔºåËøôÈáåËØÑÂà§ÁöÑÊåáÊ†áÊòØÂøóÊÑøËÄÖÂ∞ÜÁîüÊàêÂõæÂÉèÊ†áËÆ∞‰∏∫ÁúüÂÆûÂõæÂÉèÁöÑÊ¶ÇÁéáÔºå‰πüÂ∞±ÊòØÁîüÊàêÂõæÂÉè‚ÄúÊ¨∫È™ó‚ÄùÁöÑÊ¶ÇÁéáÔºàG2RÔºâ
 ÂèÇËÄÉÊñáÁåÆ
 Ë¶ÅÁÇπÔºö
 ÁªôÂá∫‰∏ÄÁªÑÁúüÂÆûÂõæÂÉèÂíå‰∏ÄÁªÑÁîüÊàêÂõæÂÉèÔºà‰ª•‰∏çÂêåÁöÑÊ¨°Â∫èÔºâÔºåËÆ©ÂøóÊÑøËÄÖ‰∏∫ÊØè‰∏ÄÂº†ÂõæÂÉèËøõË°åÊ†áËÆ∞ÔºàËØ•Âº†ÂõæÂÉèÊòØÁúüÂÆûÂõæÂÉèËøòÊòØÁîüÊàêÂõæÂÉèÔºâ ÊØè‰∏ÄÂº†ÂõæÂÉèÂè™Âá∫Áé∞1Áßí Ââç10Âº†ÂõæÂÉèÁî±‰∫éwarming upÔºå‰ºöÂëäÁü•ÂøóÊÑøËÄÖÊ≠£Á°ÆÁöÑÁªìÊûú Ëã•ÊúâÂ§ö‰∏™ÁÆóÊ≥ïËøõË°åÊØîËæÉÊó∂Ôºå‰∏ÄÁªÑÂÆûÈ™åÂè™ËøõË°åÊµãËØï‰∏ÄÁßçÁÆóÊ≥ï  Ê≥®ÊÑèÂà∞ËØÑ‰ª∑ÁöÑÊåáÊ†áÈÄöÂ∏∏‰∏∫R2G„ÄÅG2RÔºåÂç≥Â∞ÜÁúüÂÆûÂõæÂÉèÊ†áËÆ∞‰∏∫ÁúüÂÆûÂõæÂÉèÁöÑÊ¶ÇÁéá‰ª•ÂèäÂ∞ÜÁîüÊàêÂõæÂÉèÊ†áËÆ∞‰∏∫ÁîüÊàêÂõæÂÉèÁöÑÊ¶ÇÁéáÔºå‰πüÂ∞±ÊòØ‚Äúfool rate‚Äù„ÄÇ]]></description>
</item><item>
    <title>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</title>
    <link>https://shilongshen.github.io/arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization/</guid>
    <description><![CDATA[ËØ•ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰ΩÜÈ´òÊïàÁöÑÂÆûÊó∂ÔºàËΩ¨Êç¢ÈÄüÂ∫¶Êõ¥Âø´ÔºâÁöÑËÉΩËøõË°å‰ªªÊÑèÈ£éÊ†ºËøÅÁßªÁöÑÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÁöÑÊ†∏ÂøÉ‰∏∫adaptive instance normalization (AdaIN) layerÔºåAdaINËÉΩÂ§üÂØπÈΩêcontent feature ‰∏éstyle featureÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ„ÄÇ
1 Introduction Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúËÉΩÂ§üÂ∞ÜÂõæÂÉèÁöÑÂÜÖÂÆπ‰ª•ÂèäÈ£éÊ†º‰ø°ÊÅØËøõË°åÁºñÁ†ÅÔºåËÄå‰∏îÂõæÂÉèÁöÑÂÜÖÂÆπÂíåÈ£éÊ†ºÊòØÂèØÂàÜÁ¶ªÁöÑÔºåÂõ†Ê≠§ÂèØ‰ª•ÂÆûÁé∞Âú®‰øùÂ≠òÂõæÂÉèÂÜÖÂÆπÁöÑÂêåÊó∂ÂØπÂõæÂÉèÁöÑÈ£éÊ†ºËøõË°åÊîπÂèò„ÄÇ
Áé∞Â≠òÁöÑÊñπÊ≥ïÂ≠òÂú®‰∏§‰∏™ÂºäÁ´ØÔºö1.ËÉΩÂ§üÂÆûÁé∞‰ªªÊÑèÈ£éÊ†ºÁöÑËøÅÁßªÔºå‰ΩÜÊòØÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ2.ÈÄüÂ∫¶ËæÉÂø´Ôºå‰ΩÜÊòØÂè™ËÉΩÂ§üÂÆûÁé∞Âçï‰∏ÄÈ£éÊ†ºÁöÑËøÅÁßª„ÄÇ
Âú®ËøôÁØáÊñáÁ´†‰∏≠ÂÆûÁé∞‰∫ÜÈÄüÂ∫¶Âø´‰∏îËÉΩÂ§üÂÆûÁé∞‰ªªÊÑèÈ£éÊ†ºËΩ¨Êç¢„ÄÇ
AdaINÁî±instance normalization (IN)ÊâÄÂêØÂèë„ÄÇINÂú®feed-forwardÈ£éÊ†ºËøÅÁßª‰∏≠ÊòØÂçÅÂàÜÈ´òÊïàÁöÑÔºåINÁöÑ‰ΩúÁî®ÂèØ‰ª•Ëß£Èáä‰∏∫ÔºöINÈÄöËøáÂΩí‰∏ÄÂåñÂåÖÂê´ÂõæÂÉèÈ£éÊ†º‰ø°ÊÅØÁöÑfeature statisticsÔºàÁâπÂæÅÁöÑÁªüËÆ°ÁâπÊÄßÔºå‰æãÂ¶ÇÂùáÂÄºÂíåÊñπÂ∑ÆÔºâÊù•ËøõË°åÈ£éÊ†ºÂΩí‰∏ÄÂåñ„ÄÇ
AdaINsÊòØINÁöÑÊãìÂ±ïÔºåÂÖ∂‰ª•ÂÜÖÂÆπÂíåÈ£éÊ†º‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÄöËøáË∞ÉÊï¥ÂÜÖÂÆπÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÊù•ÂåπÈÖçÈ£éÊ†ºËæìÂÖ•dÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ„ÄÇ
2 related work style transfer ‚Äã	È£éÊ†ºËøÅÁßªÈóÆÈ¢òËµ∑Ê∫ê‰∫énon-photo-realistic renderingÔºàÈùûÁúüÂÆûÊÄßÊ∏≤ÊüìÔºöÈÄöËøáÈ£éÊ†ºÂΩ¢ÂºèÁöÑËâ∫ÊúØÂåñÂä†Â∑•„ÄÇÁõ∏ÂØπÁöÑÁúüÂÆûÊÄßÊ∏≤ÊüìÂº∫Ë∞ÉÂÖ∂ËæìÂá∫ÁöÑÂ§ñËßÇÂ∞ΩÂèØËÉΩÁöÑ‰∏éÁõÆÊ†áÂõæÂÉèÁõ∏Âêå„ÄÇÔºâ,Âπ∂‰∏î‰∏éÁ∫πÁêÜÂêàÊàêÂíåËΩ¨ÁßªÂØÜÂàáÁõ∏ÂÖ≥„ÄÇ‰∏Ä‰∫õÊó©Êúü‰ΩøÁî®ÁöÑÊñπÊ≥ïÂåÖÊã¨‰∫ÜÁõ¥ÊñπÂõæÂåπÈÖçÂíåÈùûÂèÇÊï∞ÈááÊ†∑„ÄÇËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫é‰ΩéÂ±ÇÊ¨°ÁöÑÁªüËÆ°ÁâπÊÄßÂπ∂‰∏î‰∏çËÉΩÂ§üÂæàÂ•ΩÁöÑÊçïËé∑ËØ≠‰πâÁªìÊûÑ‰ø°ÊÅØ„ÄÇGrayÁ≠â‰∫∫È¶ñÊ¨°ÈÄöËøáÂú®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªú‰∏≠ÂåπÈÖçÂç∑ÁßØÂ±ÇÁöÑÁªüËÆ°ÁâπÊÄßÊù•ËøõË°åÈ£éÊ†ºËøÅÁßªÔºåÂπ∂ËææÂà∞‰∫ÜÂçÅÂàÜÂá∫Ëâ≤ÁöÑÊïàÊûú„ÄÇ
‚Äã	GrayÁ≠â‰∫∫ÊèêÂá∫ÁöÑÁΩëÁªúÊ°ÜÊû∂Áî±‰∫éÈúÄË¶ÅÊúÄÂ∞èÂåñÂÜÖÂÆπÊçüÂ§±ÂáΩÊï∞ÂíåÈ£éÊ†ºÊçüÂ§±ÂáΩÊï∞Êù•Ëø≠‰ª£Êõ¥Êñ∞ÂõæÂÉèÔºåÂõ†Ê≠§‰ºòÂåñÁöÑËøáÁ®ãÂçÅÂàÜÁöÑÁºìÊÖ¢ÔºåÂú®ÂÆûÈôÖÁöÑÂ∫îÁî®‰∏≠Â∏∏Â∏∏ÈúÄË¶ÅËæÉÈïøÁöÑÊó∂Èó¥Êù•Â§ÑÁêÜÂõæÂÉèÔºàÊñá‰∏≠Âπ∂Ê≤°ÊúâÈááÁî®Â∏∏Áî®ÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÁöÑÊñπÊ≥ïÔºåÈááÁî®ÁöÑÊòØL-BFGSÁöÑ‰ºòÂåñÁÆóÊ≥ïÔºåÂÖ∂ÂÆûÊú¨Ë∫´Ëøô‰∏™È£éÊ†ºËΩ¨Êç¢Âè™ÊòØÂà©Áî®ÁöÑVGGÁΩëÁªúËøõË°åÁâπÂæÅÊèêÂèñÔºåÂÆûÈôÖ‰∏äL-BFGS‰ºòÂåñÁöÑÊòØ‰ªé‰∏ÄÂº†Áî±ÁôΩÂô™Â£∞ÁªÑÊàêÁöÑÂõæÁâáÔºåÊúÄÁªàÊ†πÊçÆÂÆö‰πâÁöÑÊçüÂ§±‰ºòÂåñÂæóÂà∞ÊúÄÁªàÁöÑÈ£éÊ†ºËΩ¨Êç¢ÂõæÁâáÔºâ„ÄÇ‰∏ÄÁßçÂ∏∏ËßÅÁöÑËß£ÂÜ≥‰∏∫‰ΩøÁî®ËÆ≠ÁªÉÂêéÁöÑÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÂ∞Ü‰ºòÂåñËøáÁ®ãÊõø‰ª£ÔºåËøôËÉΩÂ§üÊûÅÂø´ÁöÑÊèêÂçáÂ§ÑÁêÜÂõæÂÉèÁöÑÈÄüÂ∫¶ÔºåÂÆûÁé∞ÂÆûÊó∂ÁöÑËΩ¨Êç¢„ÄÇÁÑ∂ËÄåËøô‰∫õÂü∫‰∫éÂâçÈ¶àÁΩëÁªúÁöÑÊñπÊ≥ïÂ≠òÂú®ÁùÄÂ±ÄÈôêÊÄßÔºö‰∏Ä‰∏™ÁΩëÁªúÊ°ÜÊû∂Âè™ËÉΩÂ§üÁîüÊàêÊúâÈôêÈ£éÊ†ºÁöÑÂõæÂÉè„ÄÇ
&hellip;
 ÂêÑÁßçÂΩí‰∏ÄÂåñÊñπÊ≥ïÂõæ‰æã
3 Background  Â¶ÇÊûú‰∏Ä‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂú®Áº©ÊîæÂÖ®ÈÉ®ÊàñÈÉ®ÂàÜÁâπÂæÅÂêé‰∏çÂΩ±ÂìçÂÆÉÁöÑÂÆÉÁöÑÂ≠¶‰π†ÂíåÈ¢ÑÊµãÔºåÊàë‰ª¨Â∞±Áß∞ËØ•ÁÆóÊ≥ïÂÖ∑ÊúâÂ∞∫Â∫¶‰∏çÂèòÊÄß„ÄÇ
‰ªéÁêÜËÆ∫‰∏äÔºåÁ•ûÁªèÁΩëÁªúÂ∫îËØ•ÂÖ∑ÊúâÂ∞∫Â∫¶‰∏çÂèòÊÄßÔºåÂèØ‰ª•ÈÄöËøáÂèÇÊï∞ÁöÑË∞ÉÊï¥Êù•ÈÄÇÂ∫î‰∏çÂêåÁâπÂæÅÁöÑÂ∞∫Â∫¶Ôºé‰ΩÜÂ∞∫Â∫¶‰∏çÂêåÁöÑËæìÂÖ•ÁâπÂæÅ‰ºöÂ¢ûÂä†ËÆ≠ÁªÉÈöæÂ∫¶ÔºéÂÅáËÆæ‰∏Ä‰∏™Âè™Êúâ‰∏ÄÂ±ÇÁöÑÁΩëÁªúùë¶ = tanh(ùë§1ùë•1 + ùë§2ùë•2 + ùëè)ÔºåÂÖ∂‰∏≠ùë•1 ‚àà [0, 10]Ôºåùë•2 ‚àà [0, 1]Ôºé‰πãÂâçÊàë‰ª¨ÊèêÂà∞tanh ÂáΩÊï∞ÁöÑÂØºÊï∞Âú®Âå∫Èó¥[‚àí2, 2] ‰∏äÊòØÊïèÊÑüÁöÑÔºåÂÖ∂‰ΩôÁöÑÂØºÊï∞Êé•Ëøë‰∫é0ÔºéÂõ†Ê≠§ÔºåÂ¶ÇÊûúùë§1ùë•1 + ùë§2ùë•2 + ùëè ËøáÂ§ßÊàñËøáÂ∞èÔºåÈÉΩ‰ºöÂØºËá¥Ê¢ØÂ∫¶ËøáÂ∞èÔºåÈöæ‰ª•ËÆ≠ÁªÉÔºé‰∏∫‰∫ÜÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÔºåÊàë‰ª¨ÈúÄË¶Å‰Ωøùë§1ùë•1 + ùë§2ùë•2 + ùëè Âú®[‚àí2, 2] Âå∫Èó¥ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ∞Üùë§1 ËÆæÂæóÂ∞è‰∏ÄÁÇπÔºåÊØîÂ¶ÇÂú®[‚àí0.1, 0.1] ‰πãÈó¥ÔºéÂèØ‰ª•ÊÉ≥Ë±°ÔºåÂ¶ÇÊûúÊï∞ÊçÆÁª¥Êï∞ÂæàÂ§öÊó∂ÔºåÊàë‰ª¨ÂæàÈöæËøôÊ†∑Á≤æÂøÉÂéªÈÄâÊã©ÊØè‰∏Ä‰∏™ÂèÇÊï∞ÔºéÂõ†Ê≠§ÔºåÂ¶ÇÊûúÊØè‰∏Ä‰∏™ÁâπÂæÅÁöÑÂ∞∫Â∫¶Áõ∏‰ººÔºåÊØîÂ¶Ç[0, 1] ÊàñËÄÖ[‚àí1, 1]ÔºåÊàë‰ª¨Â∞±‰∏çÂ§™ÈúÄË¶ÅÂå∫Âà´ÂØπÂæÖÊØè‰∏Ä‰∏™ÂèÇÊï∞Ôºå‰ªéËÄåÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇ
Èô§‰∫ÜÂèÇÊï∞ÂàùÂßãÂåñÊØîËæÉÂõ∞ÈöæÂ§ñÔºå‰∏çÂêåËæìÂÖ•ÁâπÂæÅÁöÑÂ∞∫Â∫¶Â∑ÆÂºÇÊØîËæÉÂ§ßÊó∂ÔºåÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÁöÑÊïàÁéá‰πü‰ºöÂèóÂà∞ÂΩ±Âìç„ÄÇ]]></description>
</item><item>
    <title>ClothFlow A Folw-Based Model for Clothed Person Generation</title>
    <link>https://shilongshen.github.io/clothflow-a-flow-based-model-for-clothed-person-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/clothflow-a-flow-based-model-for-clothed-person-generation/</guid>
    <description><![CDATA[ËØ•ÊñáÊèêÂá∫‰∫ÜÂü∫‰∫éÂ§ñËßÇÊµÅÔºàclothflowÔºâÁöÑÁîüÊàêÊ®°ÂûãÊù•ËøõË°åÂßøÊÄÅÂºïÂØº‰∏ãÁöÑ‰∫∫Áâ©ÂõæÂÉèÁîüÊàêÁ†îÁ©∂Ôºà‰ª•ÂèäËôöÊãüËØïË°£Ôºâ„ÄÇ
ÈÄöËøá‰º∞ËÆ°source clothing Âíåtarget clothing‰πãÈó¥ÁöÑÂÖâÊµÅÔºåËÉΩÂ§üÁöÑÂª∫Á´ã‰∏§ËÄÖ‰πãÈó¥ÁöÑÂá†‰ΩïÂèòÊç¢„ÄÇ
 ÂÖ±ÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµ:
Á¨¨‰∏ÄÈò∂ÊÆµÔºö ‰ª•Êù°‰ª∂ÂßøÊÄÅ‰∏∫ÊåáÂØºÔºåÊù•ÁîüÊàêtarget person semantic layout Êù•ÂØπÁîüÊàêËøáÁ®ãÊèê‰æõ‰∏∞ÂØåÁöÑÊåáÂØº„ÄÇÂ∞ÜÂßøÊÄÅÂíåÂ§ñËßÇËøõË°åËß£ËÄ¶Ôºå‰ΩøÂæóclothflowÁîüÊàêÊõ¥Âä†Á©∫Èó¥Áõ∏ÂÖ≥ÁöÑÁªìÊûú„ÄÇ
Èò∂ÊÆµ‰∫åÔºö Èò∂ÊÆµ‰∫å‰∏∫clothflow flow ‰º∞ËÆ°Èò∂ÊÆµÔºàflowÁöÑ‰ΩúÁî®ÊòØ‰ªÄ‰πàÔºöÁî®‰∫éË°®Á§∫ÂéüÂõæÂÉè‰∏≠ÁöÑÂì™‰∫õÂÉèÁ¥†ÂèØ‰ª•Ë¢´Áî®‰∫éÁîüÊàêÁõÆÊ†áÂõæÂÉèÁöÑ‰∫åÁª¥ÂùêÊ†áÂêëÈáèÔºâ„ÄÇ
‰ΩøÁî®‰∏ä‰∏ÄÈò∂ÊÆµÂæóÂà∞ÁöÑtarget person semantic layout‰Ωú‰∏∫ËæìÂÖ•Êù•ÂæóÂà∞cloth flow„ÄÇsource cloth region‰πãÂêéÈÄöËøácloth flowËøõË°åwarpingÔºå‰ª•Ëß£ÂÜ≥Âá†‰ΩïÂèòÂΩ¢„ÄÇ È¢ÑÊµãÁöÑÂ§ñËßÇÊµÅÊèê‰æõ‰∫ÜËßÜËßâÂØπÂ∫îÂÖ≥Á≥ªÁöÑÂáÜÁ°Æ‰º∞ËÆ°ÔºåÂπ∂ÊúâÂä©‰∫éÊó†ÁºùËΩ¨ÁßªÊ∫êË°£ÊúçÂå∫Âüü‰ª•ÂêàÊàêÁõÆÊ†áÂõæÂÉè„ÄÇ
Èò∂ÊÆµ‰∏âÔºö ÁîüÊàêÊ®°Âûã‰ª•warped clothing region‰Ωú‰∏∫ËæìÂÖ•Êù•ÂØπtarget poseËøõË°åÊ∏≤Êüì„ÄÇ
 introduction ÂèóÂà∞image-to-image translationÂ∑•‰ΩúÁöÑÂêØÂèëÔºå‰∏Ä‰∫õÂ∑•‰ΩúÁõ¥Êé•Â∞ÜÂéüÂõæÂÉèÂíåÁõÆÊ†áÂßøÊÄÅ‰Ωú‰∏∫ËæìÂÖ•ÔºåÊù•ÁîüÊàêÁõÆÊ†áÂõæÂÉè„ÄÇ‰ΩÜÊòØËøô‰∫õÂ∑•‰ΩúÂπ∂Ê≤°ÊúâËÄÉËôëÁî±‰∫é‰∫∫‰ΩìÈùûÂàöÊÄßÁöÑÁâπÂæÅÂºïËµ∑ÁöÑÂèòÂΩ¢ÂíåÈÅÆÊå°ÈóÆÈ¢òÔºåËøôÂØºËá¥‰∫Ü‰∏çËÉΩÂ§üÁîüÊàêÁ≤æÁªÜÁöÑÁ∫πÁêÜÁªÜËäÇ„ÄÇ
‰∏∫‰∫ÜËß£ÂÜ≥geometric deformationÈóÆÈ¢ò‰ª•Êõ¥Â•ΩÁöÑËøõË°åappearance transferÔºåÊèêÂá∫‰∫Ü‰∏§Áßç‰∏çÂêåÁöÑÊñπÊ≥ïÔºödeformation-based methods and DensePose-based methods.
deformation-based methods estimate a transformationÔºåÂåÖÊã¨‰∫Ü‰ΩøÁî®affineÂíåTPSÊù•ÂØπsource image pixelÊàñËÄÖÊòØfeature mapËøõË°ådeformÔºå‰ª•Ëß£ÂÜ≥Áî±‰∫éÂßøÊÄÅÂèòÂåñÂºïËµ∑ÁöÑ‰∏çÂØπÈΩêÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°ÈÄöËøáËøô‰∏§ÁßçÂá†‰ΩïÂª∫Ê®°ÊñπÊ≥ïÂ∑≤ÁªèÂèñÂæó‰∫ÜÂæàÂ§ßÁöÑËøõÊ≠•Ôºå‰ΩÜÊòØËøôÁßçÊñπÊ≥ïÁöÑËá™Áî±Â∫¶‰∏çÂ§üÈ´òÔºå‰∏çËÉΩÂ§üÂáÜÁ°ÆÁöÑËøõË°ådeform„ÄÇ
DensePose-based methodsËÉΩÂ§üÂ∞Ü2DpixelÊò†Â∞ÑÂà∞3D body surfaceÔºåËøôËÉΩÂ§üÊõ¥ÂÆπÊòìÁöÑËé∑ÂæóÁ∫πÁêÜ‰ø°ÊÅØ„ÄÇ‰ΩÜÊòØÂü∫‰∫édense poseÁöÑÊñπÊ≥ïÁîüÊàêÁöÑÂõæÂÉèÂºïÂÖ•artifactsÔºå‰æãÂ¶ÇÂú®ÂéüÂõæÂÉèÂíåÁõÆÊ†áÂõæÂÉè‰∏≠Êúâ‰∏çÂØπÂ∫îÁöÑÈÉ®ÂàÜÔºåÁîüÊàêÁöÑÂõæÂÉè‰∫ßÁîüÁ©∫Ê¥û„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåÂü∫‰∫édense poseÁöÑÊñπÊ≥ïÁöÑËÆ°ÁÆóÈáèËæÉÂ§ß„ÄÇ
 related work Warping-based Image Matching and Synthesis Âú®ËøôÁØáÊñáÁ´†‰∏≠ÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂ∞Üsource cloth warp Êàê target cloth„ÄÇcloth regionÊòØÈùûÂàöÊÄßÁöÑÔºåsourceÂíåtarget‰πãÈó¥Ê≤°ÊúâÊòéÁ°ÆÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ]]></description>
</item><item>
    <title>Controllable Person Image Synthesis with Attribute-Decomposed GAN</title>
    <link>https://shilongshen.github.io/controllable-person-image-synthesis-with-attribute-decomposed-gan/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/controllable-person-image-synthesis-with-attribute-decomposed-gan/</guid>
    <description><![CDATA[1. Idea ÂèØÊéßÊÄßÁöÑÂõæÂÉèÁîüÊàê
ÊèêÂá∫‰∫ÜÂÖ∑Êúâ‰∏§‰∏™Áã¨Á´ãpathwaysÁöÑÁîüÊàêÂô®„ÄÇÂÖ∂‰∏≠‰∏Ä‰∏™pathwayÊòØÁî®‰∫épose encoding,Âè¶‰∏Ä‰∏™Áî®‰∫édecomposed component encoding„ÄÇ
ÂØπ‰∫éÂêéËÄÖÈ¶ñÂÖà‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑhuman parser‰ªésource person image ‰∏≠Ëá™Âä®Âú∞ÂàÜÁ¶ªÂá∫component attributesÔºàÂæóÂà∞ÁöÑÊòØsemantic layoutsÔºâ„ÄÇÂæóÂà∞ÁöÑcomponent layouts‰πãÂêéÈÄöËøámulti-branch embeddingsÈÄÅÂÖ•global texture encoder‰∏≠ÔºàÂæóÂà∞Áõ∏Â∫îÁöÑlatent codeÔºâ„ÄÇÂæóÂà∞ÁöÑlatent codeÈÄöËøá‰∏ÄÁßçÁâπÊÆäÁöÑÂΩ¢ÂºèÁªìÂêàÂæóÂà∞style code„ÄÇ‰πãÂêéËøô‰∫õË°®Á§∫component attributeÁöÑstyle codeÈÄöËøáAdnIN‰∏≠ÁöÑ‰ªøÂ∞ÑÂèòÊç¢‰∏épose codeÁõ∏ÁªìÂêà„ÄÇÊúÄÂêéËøõË°åÂõæÂÉèÁîüÊàê„ÄÇ
2. Contribution  ÈÄöËøáÁõ¥Êé•Êèê‰æõÁöÑÁöÑ‰∏çÂêåÁöÑÊ∫ê‰∫∫Áâ©ÂõæÂÉèÊù•ÊéßÂà∂‰∫∫Áâ©ÂõæÂÉèÂ±ûÊÄßÁöÑÁîüÊàêÔºåËß£ÂÜ≥poseÂíåcomponent attribute‰πãÈó¥ÈîôÁªºÂ§çÊùÇÁöÑÂÖ≥Á≥ª„ÄÇ ÊèêÂá∫‰∫Üattribute-decomposed GANÊù•ËøõË°å‰∫∫Áâ©Â±ûÊÄßÂêàÊàê„ÄÇ ÈÄöËøáÂà©Áî®off-the-shelf human parser Êù•ÊèêÂèñcomponent layoutsÔºå‰ΩøÂæócomponent attributesËøõË°åËá™Âä®ÂàÜÁ¶ªÔºåËß£ÂÜ≥‰∫Ü‰∫∫Áâ©Â±ûÊÄß‰∏çÈ´òÊïàÁöÑÊ†áÊ≥®ÈóÆÈ¢ò„ÄÇ  3. Related work image synthesis person image synthesis ÁõÆÂâçÁöÑperson image synthesisÊñπÊ≥ïÂè™ÊòØÂ∞ÜÊù°‰ª∂ÂõæÂÉèËΩ¨Êç¢‰∏∫ÂÖ∑ÊúâÁõÆÊ†áÂßøÊÄÅÁöÑÂõæÂÉè„ÄÇ‰ΩÜÊòØÊú¨Êñá‰∏≠ÁöÑÊñπÊ≥ï‰∏ç‰ªÖ‰ªÖËÉΩÂ§üÂØπÂßøÊÄÅËøõË°åÊéßÂà∂ÔºåËøòËÉΩÂ§üÂØπcomponent attributesÔºà‰æãÂ¶ÇÂ§¥Ôºå‰∏äË°£ÂíåË£§Â≠êÔºâËøõË°åÊéßÂà∂„ÄÇËÄå‰∏îÁîüÊàêÁöÑÂõæÂÉèÂÖ∑ÊúâÊõ¥Âä†ÁúüÂÆûÁöÑÁ∫πÁêÜÂíåËøûÁª≠ÁöÑID‰ø°ÊÅØ„ÄÇ
4. Method description Êú¨ÊñáÁöÑÁõÆÊ†áÁîüÊàêÂÖ∑ÊúâÁî®Êà∑ÊéßÂà∂Â±ûÊÄßÔºà‰æãÂ¶ÇÂ§¥ÂèëÔºå‰∏äË°£ÂíåË£§Â≠êÔºâÁöÑ‰∫∫Áâ©ÂõæÂÉè„ÄÇ‰∏é‰πãÂâçÁöÑÂ±ûÊÄßÁºñËæëÊñπÂºè‰∏çÂêåÔºà‰πãÂâçÁöÑÊñπÊ≥ïÈúÄË¶ÅÊØè‰∏Ä‰∏™Â±ûÊÄßÈÉΩËøõË°åÊ†áÊ≥®ÁöÑÊ†áÁ≠æÊï∞ÊçÆÔºâÔºåÊú¨Êñá‰∏≠ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÁîüÊàêÂô®Êù•ÂØπcomponent attributesËøõË°åËá™Âä®ÂíåÊó†ÁõëÁù£ÁöÑÂàÜÁ¶ª„ÄÇÂõ†Ê≠§Êú¨Êñá‰∏≠Âè™ÈúÄË¶ÅÊó†ÈúÄÂØπÊØè‰∏ÄÂ±ûÊÄßËøõË°åÊ†áÊ≥®ÁöÑ‰∫∫Áâ©ÂõæÂÉèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÂú®ËÆ≠ÁªÉÊúüÈó¥ÔºåÁõÆÊ†áÂõæÂÉè$p_t$ÂíåÊù°‰ª∂ÂõæÂÉè$I_s$ÈÄÅÂÖ•ÁîüÊàêÂô®‰∏≠ÔºåËæìÂá∫ÁîüÊàêÂõæÂÉè$I_g$„ÄÇ
4.1 Generator ÁîüÊàêÂô®ÈÄöËøá‰∏§‰∏™Áã¨Á´ãpathwaysÂ∞Ü$p_t$Âíå$I_s$Ë°®Á§∫‰∏∫‰∏§‰∏™ÈöêÂèòÈáèÔºåÂàÜÂà´Áß∞‰∏∫pose encodingÂíådecomposed component encoding„ÄÇËøô‰∏§‰∏™pathwaysÈÄöËøá‰∏ÄÁ≥ªÂàóÁöÑstyle blocksËøûÊé•Ôºåstyle blocksÂ∞ÜÊ∫ê‰∫∫Áâ©ÂõæÂÉèÁöÑÁ∫πÁêÜÈ£éÊ†ºÂµåÂÖ•Âà∞pose feature„ÄÇ
4.1.1 pose encoding Âú®pose pathways‰∏≠$p_t$ÈÄöËøápose encoderÊò†Â∞ÑÂà∞ÈöêÁ©∫Èó¥‰∏≠ÔºåÁî®$C_{pose}$Ë°®Á§∫ÔºåÂÖ∂‰∏≠pose encoderÊúâN‰∏™‰∏ãÈááÊ†∑Âç∑ÁßØÂ±ÇÊûÑÊàêÔºàN=2Ôºâ„ÄÇ]]></description>
</item><item>
    <title>Coordinate-based Texture Inpainting for Pose-Guided Human Image Generation</title>
    <link>https://shilongshen.github.io/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/</guid>
    <description><![CDATA[ËØ•ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂßøÊÄÅÂºïÂØº‰∏ãÁöÑÂõæÂÉèÁîüÊàêÊñπÊ≥ï„ÄÇ
ËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØËÉΩÂ§üÂü∫‰∫éÂçï‰∏ÄÂõæÂÉè‰º∞ËÆ°Âá∫ÂÆåÊï¥Ë∫´‰ΩìÁöÑÁ∫πÁêÜ„ÄÇ]]></description>
</item></channel>
</rss>
