<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>深度学习论文阅读笔记 - Category - 我的个人博客</title>
        <link>https://shilongshen.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <description>深度学习论文阅读笔记 - Category - 我的个人博客</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 19 Dec 2020 13:26:17 &#43;0800</lastBuildDate><atom:link href="https://shilongshen.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" rel="self" type="application/rss+xml" /><item>
    <title>video-based pose transfer method</title>
    <link>https://shilongshen.github.io/video-based-pose-transfer-method/</link>
    <pubDate>Sat, 19 Dec 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/video-based-pose-transfer-method/</guid>
    <description><![CDATA[Dance Dance Generation: Motion Transfer for Internet Videos 该文章可以实现在复杂背景下的pose transfer。
In summary, our contributions include the following.
  We demonstrate personalized motion transfer on videos from the Internet.
  We propose a novel two-stage frame-work to synthesize people performing new movements and fuse them seamlessly with background scenes. （主要贡献：实现复杂背景下的姿态转换）
  We perform qualitative and quantitative evaluations validating the superiority of our method over existing state-of-the-art.
  method: (主要的思想是先将利用语义分割图将前景中的人物进行分割，采用STN 将前景人物与目标人物进行对齐。随后通过阶段进行修正）]]></description>
</item><item>
    <title>group convolution.</title>
    <link>https://shilongshen.github.io/group-______/</link>
    <pubDate>Sat, 12 Dec 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/group-______/</guid>
    <description><![CDATA[参考
第一张图代表标准卷积操作。若输入特征图尺寸为 ，卷积核尺寸为 ，输出特征图尺寸为 ，标准卷积层的参数量为： 。（一个滤波器在输入特征图 大小的区域内操作，输出结果为1个数值，所以需要 个滤波器。）
第二张图代表分组卷积操作。将输入特征图按照通道数分成 组，则每组输入特征图的尺寸为 ，对应的卷积核尺寸为 ，每组输出特征图尺寸为 。将 组结果拼接(concat)，得到最终尺寸为 的输出特征图。分组卷积层的参数量为 。
深入思考一下，常规卷积输出的特征图上，每一个点是由输入特征图 个点计算得到的；而分组卷积输出的特征图上，每一个点是由输入特征图 个点计算得到的。自然，分组卷积的参数量是标准卷积的 。
 将输入特征图沿着通道方向进行划分，分割成不同组，每一组分别采用一个卷积和进行卷及操作。
输入特征图的大小、输出特征图的大小和标准的卷积一样，指示卷积核的参数整体减小了。
 深度可分离卷积（Depthwise separable convolution） 这张图怎么少的了呢：
图(a)代表标准卷积。假设输入特征图尺寸为 ，卷积核尺寸为 ，输出特征图尺寸为 ，标准卷积层的参数量为： 。
图(b)代表深度卷积，图(c)代表逐点卷积，两者合起来就是深度可分离卷积。深度卷积负责滤波，尺寸为(DK,DK,1)，共M个，作用在输入的每个通道上；逐点卷积负责转换通道，尺寸为(1,1,M)，共N个，作用在深度卷积的输出特征映射上。
深度卷积参数量为 ，逐点卷积参数量为 ，所以深度可分离卷积参数量是标准卷积的 。
为了便于理解、便于和分组卷积类比，假设 。深度卷积其实就是 的分组卷积，只不过没有直接将 组结果拼接，所以深度卷积参数量是标准卷积的 。逐点卷积其实就是把组结果用 conv 拼接起来，所以逐点卷积参数量是标准卷积的 。(*只考虑逐点卷积，之前输出的特征图上每一个点是由输入特征图 区域内的点计算得到的；而逐点卷积输出上每一个点是由 区域内的点计算得到的)。*自然，深度可分离卷积参数量是标准卷积的 。]]></description>
</item><item>
    <title>Towards Fine-grained Human Pose Transfer with Detail Replenishing Network</title>
    <link>https://shilongshen.github.io/towards-fine-grained-human-pose-transfer-with-detail-replenishing-network/</link>
    <pubDate>Sat, 21 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/towards-fine-grained-human-pose-transfer-with-detail-replenishing-network/</guid>
    <description><![CDATA[现有的姿态引导下图像生成方法存在着三个问题：细节缺失，内容模糊以及风格不一致，这严重降低可图像的质量以及真实性。本文提出了一种细粒度姿态引导下图像生成方法，该方法更加注重于语义的完整性以及细节的补充，该方法将内容合成(local warping)和特征转移(style transfer)的概念以相互引导的方式结合在一起。并提出了一个细节补充网络（DRN）。此外还提出了一套细粒度评估方法，包括了语义分析、结构检测和感知质量评估。HPT和FHPT的比较如下图：
A. HPT Methods 基于特征转换机制，我们可以将已有的HPT方法分为三类：global predictive methods、local warping methods 以及 hybrid methods。
该图展示了三种方法不同。我们可以从输出的结果分析出这三种方法存在的两个缺陷：保存原图片语义和外观细节的能力以及在遮挡区域合成新的内容的能力。
Global predictive methods 该方法将HPT视为多模态image-to-image translation问题，使用具有跳跃连接的U-net网络进行特征的前向传播。通过将提取的姿态表示与原图像叠加来将“姿态引导”引入。例如上图中（a）将原姿态$I_s$和目标姿态$I_t$进行编码并与原图像$I_s$进行叠加，直接生成具有目标姿态的图像$\bar{I_t}$。然而，由于缺乏准确的变形建模，这些工作往往不能可靠地解决[35]不同姿态之间的结构不对齐问题。通常来说，global predictive methods 会缺乏捕获相应局部特征的能力，这会导致生成图像中细节缺失，例如模糊的细节和失真,过于平滑的衣服。
Local warping methods 该方法受到了spatial transformer networks的启发，将deformation构建在特征的前向传播中。例如DSC中使用part-wise 仿射变换，PATN中使用注意力机制来增加deformation modeling的灵活性。
不同姿态之间的变形映射通常是通过相应有限的姿态关键点的插值实现的。假设原图像\目标图像中人体的区域为$\Omega_s$ \ $\Omega_t$.将deformation 视为一个函数：$T_{st}:\Omega_s \rightarrow \Omega_t$,将区域用关键点表示，则有$T_{st}(p_s)=p_t$.通常来说这样的映射不是唯一的，需要通过额外的正则项（blending energy）来减缓warped contents的失真。
由于视角变化和自遮挡的情况，无法保证estimated warping 覆盖整个target human body ,也就是说$\Omega_t-T_{st}\neq \phi$.因此**local warping network很难恢复原图像中没有精确对应的underlying content，这会导致内容的模棱两可**。
 由于视角变化和自遮挡的情况，无法保证estimated warping 覆盖整个target human body 的理解：
我们建立warping 最终的目的就是将原图像进行形变操作。实际是可以将目标图像看作是原图像的形变版本（由于姿态的变化）。
建立的warping， 例如光流法，实际上就是寻找原图像和目标图像之间的关系。具体来说就是目标图像中像素点对应的是原图像的哪一个像素点，如上图中红点所示，或者更准确的说应该是：目标图像的像素点是根据原图像中的像素点采样而来的。
但是存在的一个问题是。在pose-guided person generation 任务中，由于姿态的变化，可能会导致目标图像中的像素点在原图像中找不到对应的像素点。如上图中蓝点所示。换个说法就是无法保证estimated warping 覆盖整个target human body。这个时候生成图像中的某一些部位可能就会产生内容的模棱两可。或者说是产生hole或者说明unable to generate new contents。]]></description>
</item><item>
    <title>A Style-Based Generator Architecture for Generative Adversarial Networks</title>
    <link>https://shilongshen.github.io/a-style-based-generator-architecture-for-generative-adversarial-networks/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/a-style-based-generator-architecture-for-generative-adversarial-networks/</guid>
    <description><![CDATA[参考：
参考1
参考2
 该文章基于GAN设计风格迁移模型。该模型能够自动学习，无监督的分离高层属性（例如 pose，identity）以及能对生成图像进行随机变化（例如雀斑，头发）并且 能够对生成图像的分辨率进行控制。
 StyleGAN中的style借用自图像风格迁移，这篇文章是PG-GAN之后的，主要改变了生成器的结构实现无监督地生成可控性强的图像。
PG-GAN中是通过一层层地给生成器和判别器增添卷积层同时提高分辨率的方法来生成高质量图像的。StyleGAN的motivation就是发现这种渐进的方法其实能控制图像的不同特性，低分辨率也就是coarse层主要影响图像的姿态，脸型等高级特征，高分辨率主要影响背景发色等低级特征(结合感受野的概念理解，低分辨率的特征获取更多全局信息并加以处理，比如特征从线条经过多次卷积一步步会组合出形状，会更加抽象，具有更高级的语义信息)。但PG-GAN这种结构在递进添加层的时候没有任何控制条件，导致整体的特征和细微的特征间存在耦合，耦合就导致了图像可控性差，没办法对单个特征进行调节。StyleGAN于是寻找了一种无监督但又可控性强的方法：对不同level的卷积层进行操作。
ProGAN的网络框架：
 在之前的传统的生成网络中，有一个问题存在，就是特征耦合，那么这个是什么呢？如上图左边，传统的生成网络，我们可以看到，其输入一个latent Z
Z，一般为512的一个向量。假设这个向量代表一个人的脸，那么该向量就会存在特征耦合。因为一个脸是有很多特征的，并不是一个512维的向量就能完全表示的，如果非要表示，只能是这种方式，如下：
假设：维度1代表头发粗细，维度2代表皮肤颜色，维度3代表鼻子大小…，当512个维度全部是用完之后，其只能通过多个维度再去表示其他的特征，如：维度1与维度2综合起来表示了头发的颜色。这样，通过两个或者多个组合，512的向量，就能表示出接近无数的特征。
但是这样就很明显出现了一个问题，那就是我们怎么去控制我们想要图片的单个特征呢？如，我只想改变头发的粗细，但是又不能直接去修改第一个维度，因为第一个维度会影响其他的维度，可能会影响到皮肤的颜色，但是有的时候，我偏偏需要该表他头发的颜色，不修改皮肤的颜色。不同的特征是互相关联的（特征耦合）。
 1 Introduction GAN能够生成高分辨率和高质量的图像，但是生成器仍然被视为一个黑箱子。尽管最近已经有了一些工作，但是对图像生成过程中变化因素的理解，例如随机特征的起源，仍然是缺乏的。latent space的性质也没有很好的被理解。
该文设计了一个能够设计了一个能够控制生成过程的生成器模型。生成器的输入为一个learned constant，根据latent code在每一个卷积层调整图像的style，从而直接控制不同尺度下图像特征的强度。通过将noise直接送入网络中，来实现从随机变化（）中进行高层属性（）的自动、无监督分离，并实现直观的特定尺度的混合和插值操作。
生成器首先将input latent code映射到intermediate latent space，这对于factors of variation在网络中是如何表达到有很重要的影响。input latent code的分布（input latent space）是符合training data的概率密度的，这会导致某种程度上信息的耦合。而intermediate latent space是没有这一限制的，因此可以进行信息的解耦。该文提出了两种新的评估latent space解耦程度的方法：感知路径长度和线性可分离性。该文的方法具有更高的线性度和更少的耦合度。
并且提出了了一个新的dataset of human faces（Flickr-Faces-HQ, FFHQ）。
2 style-based generator 传统的方法是将input code通过input layer送入生成器。该文抛弃了这个设计，完全忽略了输入层，而是从一个learned constant开始。给定latent space $\mathcal{Z}$ 中的一个latent code $z$，非线性映射网络 $f:\mathcal{Z \rightarrow W }$, 将$z$映射为$w$：$w=f(z)$.其中$f$ 为8层的MLP。随后将$w$变换为styles:$y=(y_s,y_b)$来作为AdaIN的仿射参数： $$ AdaIN(x_i,y)=y_{(s,i)}{\large(}\frac{x_i-\mu(x_i)}{\sigma(x_i)}{\large)}+y_{(b,i)} $$
 $$ z\rightarrow w \rightarrow y (仿射参数)\rightarrow style\]]></description>
</item><item>
    <title>A Variational U-Net for Conditional Appearance and Shape Generation</title>
    <link>https://shilongshen.github.io/a-variational-u-net-for-conditional-appearance-and-shape-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/a-variational-u-net-for-conditional-appearance-and-shape-generation/</guid>
    <description><![CDATA[该方法能够生成不同的姿态的人物图像以及改变人物的外观。而且这个模型能够在不改变shape的情况下从appearance distribution 中进行采样。
1 Approach 记$x$为dataset $X$中的一张图片，我们想要理解$x$中的object是如何被其shape $y$和appearance $z$所影响的。因此图像生成器可以被表示为最大化后验概率（极大似然估计：给定$y$和$z$，哪种$x$最有可能发生。） $$ arg\ max\ p(x|y,z) $$
1.1 VAE based on latent shape and appearance $p(x|y,z)$可以视为隐变量(含两个隐变量)的生成模型，可以求得这个生成模型的联合概率分布$p(x,y,z)$。
$\because$ $$ p(x|y,z)=\frac{p(x,y,z)}{p(y,z)} $$
$\therefore$ $$ p(x,y,z)=p(x|y,z)p(y,z) $$ 含隐变量的概率密度估计可以采用VAE的方法进行求解，求解过程包含了两个步骤推断和生成。实际上我们最终的目的是为了求出图像$x$的分布$p(x,\theta),\theta$为参数，给定样本$x$，其对数边际似然函数为 $$ \log p(x,\theta)=ELBO(q,x,\theta,\phi)+KL[q(y,z|x;\phi),p(y,z|x;\theta)] $$ 其中$q(y,z|x;\phi)$为变分密度函数，$\phi$为参数，ELBO为证据下界： $$ ELBO(q,x,\theta,\phi)=\mathbb{E}_q\log \frac{p(x|y,z)p(y,z)}{q(y,z|x;\phi)} $$
 $$ \begin{aligned} \log p(x)&amp;=\log \int p(x,y,z)dz\ dy\
&amp;=\log \int \frac{p(x,y,z)}{q(y,z|x)}q(y,z|x)dz\ dy\
&amp;\geq \int q(y,z|x)\log \frac{p(x,y,z)}{q(y,z|x)}dz\ dy\
&amp;= \mathbb{E}_q\log \frac{p(x,y,z)}{q(y,z|x;\phi)}\
&amp;=\mathbb{E}_q\log \frac{p(x|y,z)p(y,z)}{q(y,z|x;\phi)}
\end{aligned} $$
实际上 $$ \begin{aligned} \log p(x)&amp;=\int q(y,z|x)\log \frac{p(x,y,z)}{q(y,z|x)}dz\ dy-\int q(y,z|x)\log \frac{p(y,z|x)}{q(y,z|x)}dz\ dy\]]></description>
</item><item>
    <title>AMT perceptual study</title>
    <link>https://shilongshen.github.io/amt-perceptual-studies/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/amt-perceptual-studies/</guid>
    <description><![CDATA[参考文献
注意到，这里评判的指标是志愿者将生成图像标记为真实图像的概率，也就是生成图像“欺骗”的概率（G2R）
 参考文献
 要点：
 给出一组真实图像和一组生成图像（以不同的次序），让志愿者为每一张图像进行标记（该张图像是真实图像还是生成图像） 每一张图像只出现1秒 前10张图像由于warming up，会告知志愿者正确的结果 若有多个算法进行比较时，一组实验只进行测试一种算法  注意到评价的指标通常为R2G、G2R，即将真实图像标记为真实图像的概率以及将生成图像标记为生成图像的概率，也就是“fool rate”。]]></description>
</item><item>
    <title>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</title>
    <link>https://shilongshen.github.io/arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization/</guid>
    <description><![CDATA[该文提出了一种简单但高效的实时（转换速度更快）的能进行任意风格迁移的模型。该模型的核心为adaptive instance normalization (AdaIN) layer，AdaIN能够对齐content feature 与style feature的均值和方差。
1 Introduction 深度神经网络能够将图像的内容以及风格信息进行编码，而且图像的内容和风格是可分离的，因此可以实现在保存图像内容的同时对图像的风格进行改变。
现存的方法存在两个弊端：1.能够实现任意风格的迁移，但是速度较慢。2.速度较快，但是只能够实现单一风格的迁移。
在这篇文章中实现了速度快且能够实现任意风格转换。
AdaIN由instance normalization (IN)所启发。IN在feed-forward风格迁移中是十分高效的，IN的作用可以解释为：IN通过归一化包含图像风格信息的feature statistics（特征的统计特性，例如均值和方差）来进行风格归一化。
AdaINs是IN的拓展，其以内容和风格作为输入，通过调整内容的均值和方差来匹配风格输入d的均值和方差。
2 related work style transfer ​	风格迁移问题起源于non-photo-realistic rendering（非真实性渲染：通过风格形式的艺术化加工。相对的真实性渲染强调其输出的外观尽可能的与目标图像相同。）,并且与纹理合成和转移密切相关。一些早期使用的方法包括了直方图匹配和非参数采样。这些方法通常依赖于低层次的统计特性并且不能够很好的捕获语义结构信息。Gray等人首次通过在深度神经网络中匹配卷积层的统计特性来进行风格迁移，并达到了十分出色的效果。
​	Gray等人提出的网络框架由于需要最小化内容损失函数和风格损失函数来迭代更新图像，因此优化的过程十分的缓慢，在实际的应用中常常需要较长的时间来处理图像（文中并没有采用常用的梯度下降的方法，采用的是L-BFGS的优化算法，其实本身这个风格转换只是利用的VGG网络进行特征提取，实际上L-BFGS优化的是从一张由白噪声组成的图片，最终根据定义的损失优化得到最终的风格转换图片）。一种常见的解决为使用训练后的前馈神经网络将优化过程替代，这能够极快的提升处理图像的速度，实现实时的转换。然而这些基于前馈网络的方法存在着局限性：一个网络框架只能够生成有限风格的图像。
&hellip;
 各种归一化方法图例
3 Background  如果一个机器学习算法在缩放全部或部分特征后不影响它的它的学习和预测，我们就称该算法具有尺度不变性。
从理论上，神经网络应该具有尺度不变性，可以通过参数的调整来适应不同特征的尺度．但尺度不同的输入特征会增加训练难度．假设一个只有一层的网络𝑦 = tanh(𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏)，其中𝑥1 ∈ [0, 10]，𝑥2 ∈ [0, 1]．之前我们提到tanh 函数的导数在区间[−2, 2] 上是敏感的，其余的导数接近于0．因此，如果𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏 过大或过小，都会导致梯度过小，难以训练．为了提高训练效率，我们需要使𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏 在[−2, 2] 区间，因此需要将𝑤1 设得小一点，比如在[−0.1, 0.1] 之间．可以想象，如果数据维数很多时，我们很难这样精心去选择每一个参数．因此，如果每一个特征的尺度相似，比如[0, 1] 或者[−1, 1]，我们就不太需要区别对待每一个参数，从而减少人工干预。
除了参数初始化比较困难外，不同输入特征的尺度差异比较大时，梯度下降法的效率也会受到影响。]]></description>
</item><item>
    <title>ClothFlow A Folw-Based Model for Clothed Person Generation</title>
    <link>https://shilongshen.github.io/clothflow-a-flow-based-model-for-clothed-person-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/clothflow-a-flow-based-model-for-clothed-person-generation/</guid>
    <description><![CDATA[该文提出了基于外观流（clothflow）的生成模型来进行姿态引导下的人物图像生成研究（以及虚拟试衣）。
通过估计source clothing 和target clothing之间的光流，能够的建立两者之间的几何变换。
 共分为三个阶段:
第一阶段： 以条件姿态为指导，来生成target person semantic layout 来对生成过程提供丰富的指导。将姿态和外观进行解耦，使得clothflow生成更加空间相关的结果。
阶段二： 阶段二为clothflow flow 估计阶段（flow的作用是什么：用于表示原图像中的哪些像素可以被用于生成目标图像的二维坐标向量）。
使用上一阶段得到的target person semantic layout作为输入来得到cloth flow。source cloth region之后通过cloth flow进行warping，以解决几何变形。 预测的外观流提供了视觉对应关系的准确估计，并有助于无缝转移源衣服区域以合成目标图像。
阶段三： 生成模型以warped clothing region作为输入来对target pose进行渲染。
 introduction 受到image-to-image translation工作的启发，一些工作直接将原图像和目标姿态作为输入，来生成目标图像。但是这些工作并没有考虑由于人体非刚性的特征引起的变形和遮挡问题，这导致了不能够生成精细的纹理细节。
为了解决geometric deformation问题以更好的进行appearance transfer，提出了两种不同的方法：deformation-based methods and DensePose-based methods.
deformation-based methods estimate a transformation，包括了使用affine和TPS来对source image pixel或者是feature map进行deform，以解决由于姿态变化引起的不对齐问题。尽管通过这两种几何建模方法已经取得了很大的进步，但是这种方法的自由度不够高，不能够准确的进行deform。
DensePose-based methods能够将2Dpixel映射到3D body surface，这能够更容易的获得纹理信息。但是基于dense pose的方法生成的图像引入artifacts，例如在原图像和目标图像中有不对应的部分，生成的图像产生空洞。除此之外，基于dense pose的方法的计算量较大。
 related work Warping-based Image Matching and Synthesis 在这篇文章中，我们的目标是将source cloth warp 成 target cloth。cloth region是非刚性的，source和target之间没有明确的对应关系。]]></description>
</item><item>
    <title>Controllable Person Image Synthesis with Attribute-Decomposed GAN</title>
    <link>https://shilongshen.github.io/controllable-person-image-synthesis-with-attribute-decomposed-gan/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/controllable-person-image-synthesis-with-attribute-decomposed-gan/</guid>
    <description><![CDATA[1. Idea 可控性的图像生成
提出了具有两个独立pathways的生成器。其中一个pathway是用于pose encoding,另一个用于decomposed component encoding。
对于后者首先使用预训练的human parser从source person image 中自动地分离出component attributes（得到的是semantic layouts）。得到的component layouts之后通过multi-branch embeddings送入global texture encoder中（得到相应的latent code）。得到的latent code通过一种特殊的形式结合得到style code。之后这些表示component attribute的style code通过AdnIN中的仿射变换与pose code相结合。最后进行图像生成。
2. Contribution  通过直接提供的的不同的源人物图像来控制人物图像属性的生成，解决pose和component attribute之间错综复杂的关系。 提出了attribute-decomposed GAN来进行人物属性合成。 通过利用off-the-shelf human parser 来提取component layouts，使得component attributes进行自动分离，解决了人物属性不高效的标注问题。  3. Related work image synthesis person image synthesis 目前的person image synthesis方法只是将条件图像转换为具有目标姿态的图像。但是本文中的方法不仅仅能够对姿态进行控制，还能够对component attributes（例如头，上衣和裤子）进行控制。而且生成的图像具有更加真实的纹理和连续的ID信息。
4. Method description 本文的目标生成具有用户控制属性（例如头发，上衣和裤子）的人物图像。与之前的属性编辑方式不同（之前的方法需要每一个属性都进行标注的标签数据），本文中通过精心设计的生成器来对component attributes进行自动和无监督的分离。因此本文中只需要无需对每一属性进行标注的人物图像训练数据。在训练期间，目标图像$p_t$和条件图像$I_s$送入生成器中，输出生成图像$I_g$。
4.1 Generator 生成器通过两个独立pathways将$p_t$和$I_s$表示为两个隐变量，分别称为pose encoding和decomposed component encoding。这两个pathways通过一系列的style blocks连接，style blocks将源人物图像的纹理风格嵌入到pose feature。
4.1.1 pose encoding 在pose pathways中$p_t$通过pose encoder映射到隐空间中，用$C_{pose}$表示，其中pose encoder有N个下采样卷积层构成（N=2）。]]></description>
</item><item>
    <title>Coordinate-based Texture Inpainting for Pose-Guided Human Image Generation</title>
    <link>https://shilongshen.github.io/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/</link>
    <pubDate>Sun, 15 Nov 2020 13:26:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://shilongshen.github.io/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/</guid>
    <description><![CDATA[该文提出了一种基于深度学习的姿态引导下的图像生成方法。
该方法的核心是能够基于单一图像估计出完整身体的纹理。]]></description>
</item></channel>
</rss>
