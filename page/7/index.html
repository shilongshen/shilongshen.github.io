<!DOCTYPE html>
<html lang="zh-cn">
    <head>
	<meta name="generator" content="Hugo 0.63.0" />
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>我的个人博客</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="我的个人博客" />
<meta property="og:description" content="这是我的全新 Hugo 网站" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://shilongshen.github.io/" />
<meta property="og:image" content="https://shilongshen.github.io/logo.png"/>
<meta property="og:updated_time" content="2021-02-04T09:17:30+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://shilongshen.github.io/logo.png"/>

<meta name="twitter:title" content="我的个人博客"/>
<meta name="twitter:description" content="这是我的全新 Hugo 网站"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://shilongshen.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="我的个人博客">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="我的个人博客"><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/shilongshen.github.io\/","inLanguage": "zh-cn","author": {
                "@type": "Person",
                "name": "shilongshen"
            },"description": "这是我的全新 Hugo 网站","name": "我的个人博客"
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="我的个人博客">首页</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="我的个人博客">首页</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="page home" posts><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="文章"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/avatar.jpg"
        data-srcset="/images/avatar.jpg, /images/avatar.jpg 1.5x, /images/avatar.jpg 2x"
        data-sizes="auto"
        alt="/images/avatar.jpg"
        title="/images/avatar.jpg" /></a></div><h1 class="home-title">我的个人博客</h1><div class="links"><a href="https://github.com/shilongshen" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github-alt fa-fw"></i></a><a href="mailto:1319144765@qq.com" title="Email" rel=" me"><i class="far fa-envelope fa-fw"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/clothflow-a-flow-based-model-for-clothed-person-generation/">ClothFlow A Folw-Based Model for Clothed Person Generation</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">该文提出了基于外观流（clothflow）的生成模型来进行姿态引导下的人物图像生成研究（以及虚拟试衣）。
通过估计source clothing 和target clothing之间的光流，能够的建立两者之间的几何变换。
 共分为三个阶段:
第一阶段： 以条件姿态为指导，来生成target person semantic layout 来对生成过程提供丰富的指导。将姿态和外观进行解耦，使得clothflow生成更加空间相关的结果。
阶段二： 阶段二为clothflow flow 估计阶段（flow的作用是什么：用于表示原图像中的哪些像素可以被用于生成目标图像的二维坐标向量）。
使用上一阶段得到的target person semantic layout作为输入来得到cloth flow。source cloth region之后通过cloth flow进行warping，以解决几何变形。 预测的外观流提供了视觉对应关系的准确估计，并有助于无缝转移源衣服区域以合成目标图像。
阶段三： 生成模型以warped clothing region作为输入来对target pose进行渲染。
 introduction 受到image-to-image translation工作的启发，一些工作直接将原图像和目标姿态作为输入，来生成目标图像。但是这些工作并没有考虑由于人体非刚性的特征引起的变形和遮挡问题，这导致了不能够生成精细的纹理细节。
为了解决geometric deformation问题以更好的进行appearance transfer，提出了两种不同的方法：deformation-based methods and DensePose-based methods.
deformation-based methods estimate a transformation，包括了使用affine和TPS来对source image pixel或者是feature map进行deform，以解决由于姿态变化引起的不对齐问题。尽管通过这两种几何建模方法已经取得了很大的进步，但是这种方法的自由度不够高，不能够准确的进行deform。
DensePose-based methods能够将2Dpixel映射到3D body surface，这能够更容易的获得纹理信息。但是基于dense pose的方法生成的图像引入artifacts，例如在原图像和目标图像中有不对应的部分，生成的图像产生空洞。除此之外，基于dense pose的方法的计算量较大。
 related work Warping-based Image Matching and Synthesis 在这篇文章中，我们的目标是将source cloth warp 成 target cloth。cloth region是非刚性的，source和target之间没有明确的对应关系。</div><div class="post-footer">
        <a href="/clothflow-a-flow-based-model-for-clothed-person-generation/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/controllable-person-image-synthesis-with-attribute-decomposed-gan/">Controllable Person Image Synthesis with Attribute-Decomposed GAN</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">1. Idea 可控性的图像生成
提出了具有两个独立pathways的生成器。其中一个pathway是用于pose encoding,另一个用于decomposed component encoding。
对于后者首先使用预训练的human parser从source person image 中自动地分离出component attributes（得到的是semantic layouts）。得到的component layouts之后通过multi-branch embeddings送入global texture encoder中（得到相应的latent code）。得到的latent code通过一种特殊的形式结合得到style code。之后这些表示component attribute的style code通过AdnIN中的仿射变换与pose code相结合。最后进行图像生成。
2. Contribution  通过直接提供的的不同的源人物图像来控制人物图像属性的生成，解决pose和component attribute之间错综复杂的关系。 提出了attribute-decomposed GAN来进行人物属性合成。 通过利用off-the-shelf human parser 来提取component layouts，使得component attributes进行自动分离，解决了人物属性不高效的标注问题。  3. Related work image synthesis person image synthesis 目前的person image synthesis方法只是将条件图像转换为具有目标姿态的图像。但是本文中的方法不仅仅能够对姿态进行控制，还能够对component attributes（例如头，上衣和裤子）进行控制。而且生成的图像具有更加真实的纹理和连续的ID信息。
4. Method description 本文的目标生成具有用户控制属性（例如头发，上衣和裤子）的人物图像。与之前的属性编辑方式不同（之前的方法需要每一个属性都进行标注的标签数据），本文中通过精心设计的生成器来对component attributes进行自动和无监督的分离。因此本文中只需要无需对每一属性进行标注的人物图像训练数据。在训练期间，目标图像$p_t$和条件图像$I_s$送入生成器中，输出生成图像$I_g$。
4.1 Generator 生成器通过两个独立pathways将$p_t$和$I_s$表示为两个隐变量，分别称为pose encoding和decomposed component encoding。这两个pathways通过一系列的style blocks连接，style blocks将源人物图像的纹理风格嵌入到pose feature。
4.1.1 pose encoding 在pose pathways中$p_t$通过pose encoder映射到隐空间中，用$C_{pose}$表示，其中pose encoder有N个下采样卷积层构成（N=2）。</div><div class="post-footer">
        <a href="/controllable-person-image-synthesis-with-attribute-decomposed-gan/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/">Coordinate-based Texture Inpainting for Pose-Guided Human Image Generation</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">该文提出了一种基于深度学习的姿态引导下的图像生成方法。
该方法的核心是能够基于单一图像估计出完整身体的纹理。</div><div class="post-footer">
        <a href="/coordinate-based-texture-inpainting-for-pose-guided-human-image-generation/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/correspondence-networks-with-adaptive-neighbourhood-consensus/">Correspondence Networks with Adaptive Neighbourhood Consensus</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">该文提出了一种建立两张图像间密集语义对应关系的模型（ANC-Net）,
 non-isotrppic（非各向同性） 4D convolution kernel &ndash; 核心， multi-scale self-similarity module orthogonal loss  ANC-Net 以两张图像作为输入，输出为4D correlation map &ndash; 包含两张图像间所有可能匹配的匹配分数。
Method 网络结构如图所示。
 输入$(I^s,I^t)$ feature extractor $\mathcal{F}$ -&gt;输出 $F^s$和$F^t$ multi-scale self-similarity $\mathcal{S}$ -&gt;输出 multi-scale self-similarity $S^s$和$S^t$-&gt;captures the complex self-similarity feature map We can then obtain the 4D correlation map $C_s$ from $S^s$ and $S^t$ , and the 4D correlation map $C_f$ from $F^s$ and $F^t$ . However, $C_s$ and $C_f$ are often noisy as they lack the constraints to enforce the correspondence validity, and thus are unreliable for directly extracting correspondences.</div><div class="post-footer">
        <a href="/correspondence-networks-with-adaptive-neighbourhood-consensus/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/cross-domain-correspondence-learning-for-exemplar-based-image-translation/">Cross-domain Correspondence Learning for Exemplar-based Image Translation</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">该文提出了一种新颖的图像翻译网络框架。
cross-domain correspondence network： 网络的输入为style input和content input，由于style input和content input在结构上是不对齐的，为了建立style input和content input之间的对应关系，提出了一种cross-domain correspondence network,如图所示：
该部分首先通过特征金字塔网络提取多尺度图像特征（这能够利用到局部和全局的图像内容），之后将提取到的多尺度特征通过下采样网络映射到相同的域$S$中（只有当$x_s$和$y_s$在同一个域时才能用某种相似性度量方法进行匹配（对齐））。数学表达式如下： $$ x_s=\mathcal{F}{A\rightarrow S}(x_A;\theta{A\rightarrow S}) \quad x_s\in\mathbb{R}^{HW \times C}\
y_s=\mathcal{F}{B\rightarrow S}(x_B;\theta{B\rightarrow S}) \quad y_s\in\mathbb{R}^{HW \times C} $$ 接下来使用空间自注意力机制操作计算$x_s$与$y_s$间pixel与pixel之间的相关性，如下图所示:
translation network： cross-domain correspondence network的输出视为将content input warping 之后的结果，translation network采用了类似style GAN的网络结构，如下图所示：
与style GAN不同的是，这里将AdaIN模块进行了替换，替换的模块为SPADE block与PN的结合，表达式为： $$ \alpha {h,w}^i(r{y \rightarrow x}) \times\frac{F_{c,h,w}^i-\mu_{h,w}^i}{\sigma_{h,w}^i}+\beta_{h,w}^i(r_{y \rightarrow x}) $$ $F_{c,h,w}^i$为输入值，注意此处的统计特征$\mu_{h,w}^i$和$\sigma_{h,w}^i$是在空间方向进行统计的，目的是为了保存$r_{y \rightarrow x}$的结构信息。
损失函数： 该文采用了无监督的训练方式。
总的网络结构： </div><div class="post-footer">
        <a href="/cross-domain-correspondence-learning-for-exemplar-based-image-translation/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/deep-image-spatial-transformation-for-person-image-generation/">Deep Image Spatial Transformation for Person Image Generation</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">姿态引导下的图像生成研究就是要在保留原图像外观的同时将原图像中人物的姿态转换为目标姿态。这一任务需要对原图像进行空间转换。但实际上基于卷积的神经网络更擅长于特征的提取而不擅长于特征的空间转换，所以单纯的使用卷积神经网络并不能很好的完成这一任务。
这篇文章提出了一种global-flow local-attention 模型进行姿态应引导下的图像生成研究。具体来说第一步使用全局流场估计器计算原图像和目标图像之间的全局相关性，以此来预测flow fileds。第二步为使用从feature maps提取出来的flowed local patch pairs来计算局部注意力系数。第三步为使用局部注意力机制作为content aware sampling method来进行图像的外观渲染。
 该文将注意力机制和流场操作相结合，使得每一个输出位置只与原图像的局部特征块相关
将目标图像视为原图像的变形结果
该文的观点是将目标图像视为原图像的变形结果，每一个输出位置只与原图像的局部特征相关。
 Introduction 图像的空间转换可以用于解决许多输入图像和目标图像空间不对齐的图像生成任务，这些不对齐可能是由于姿态变换或者是视角的变换。这一类任务包括了姿态引导下的图像生成研究
卷积神经网络使用共享参数的卷积核来计算输出，这也是卷积神经网络的一个重要特性，称为平移等变（equivariance to transformation），这意味着当输入发生平移空间变化时，输出也会发生平移相同的空间变化。这一特性对于输入输出的空间结构是对齐的任务来说是十分有益的，例如图像分割，图像检测以及图像翻译。但是这一特性也限制了卷积神经网络对输入数据进行空间变换。
 卷积神经网络具有平移等变得性质，但是并不能够对旋转、缩放等操作具有等变性，特别是对于人体这种非刚体，卷积神经网络并不能对输入数据进行空间变换
 STN的通过引入空间转换模块来解决这一问题，该模块对全局转换参数进行回归，并通过仿射转换来扭曲输入特征。但是，由于它假定了源和目标之间的全局仿射变换，因此该方法无法处理非刚性对象的变换。
注意力机制通过利用非局部信息，建立特征之间的长程依赖。但是对于空间转换任务，目标图像和原图像在空间上是不对齐的，每一个输出图像上位置与原图像上的位置有明确的对应关系。因此原图像和目标图像之间的注意力权重矩阵应该是一个稀疏矩阵。
基于流场的操作通过为每一个输出位置采样一个局部的原图像块会迫使注意力权重矩阵变为稀疏矩阵。这些方法预测二维坐标偏移量，指定可以对源中的哪些位置进行采样以生成目标。
然而为了稳定训练，大多数的基于流场的方法会在像素级别扭曲数据，这会限制模型生成新的内容。由于需要生成全分辨率流场，因此难以提取大的运动。在特征级别进行输入数据的扭曲能够解决这一问题，然而，这些网络很容易陷入局部最小值由于以下两个原因：（1）输入特征和流场相互限制。没有准确的流场，输入特征无法获得合理的梯度，没有如果没有合理的特征，网络也无法提取相似性以生成正确的流场。（2）常用的双线性采样方法提供的不良梯度传播进一步导致训练中的不稳定。
Approach 对于姿态引导下的图像生成研究，目标图像是原图像的变形结果，这意味着目标图像中的每一个点是与原图像中的某个特定的局部区域唯一对应。
该文设计了global-flow local-attention 网络结构来合理的对原图像特征进行采样和重构。网络包含两个部分：全局流场估计器F和局部自然纹理渲染器G。F负责估计原图像和目标图像的运动（差别），其生成全局流场w和二进制掩模m。利用w和m，G利用局部注意力模块将原图像的纹理渲染在目标图像。
Global Flow Filed Estimator $$ w,m=F(x_s,p_s,p_t) $$
其中$x_s$表示原图像，$p_s$表示原图像姿态，$p_t$表示目标姿态。$w$包含了原图像和目标图像的坐标偏移量。$m$的值在0至1之间，表示原图像中是否存在目标位置的信息。F为全卷积网络，w和m权重共享。
由于真实的坐标偏移量是未知的，这里使用了sampling correctness loss来计算$w$。$v_s,v_t$分别表示原图像和目标图像通过预训练的VGG19的特定层提取出来的特征。$v_{s,w}=w(v_s)$表示$v_s$通过$w$转换之后的结果。sampling correctness loss计算$v_{s,w}$和$v_t$之间的余弦相似性。 $$ \mathcal{L}_c=\frac{1}{N}\sum_{l\in \Omega}exp(-\frac{\mu(v_{s,w}^l,v_t^l)}{\mu_{max}^l}) $$ $\mu$表示余弦相似度。$\Omega$表示特征图中的所有的N个点，$l$表示其中的一个点$(x,y)$。$\mu_{max}^l$表示正则项。也就是说需要对**所有的点**计算余弦相似度，然后再求平均值。
​	sampling correctness loss能够限制流场采样语义相关的区域。由于图像领域之间的变形是高度相关的，如果能够将这种关系提取出来是十分有益的，因此进一步在流场中添加了正则项。令$c_t$表示目标特征的二位坐标。$\mathcal{N}(c_t,l)$表示$c_t$的$n\times n$个领域图像块，假设$\mathcal{N}(c_t,l)$和$\mathcal{N}(c s,l)$之间的变换为仿射变换 $$ T_l=A_l S_l $$ $T_l$表示$\mathcal{N}(C_l,l)$的坐标集合，$S_l$表示$\mathcal{N}(c_s,l)$的坐标集合，$A_l$表示仿射参数，通过最小二乘法计算出来 $$ \hat{A_l}=(S_l^HS_l)^{-1}S_l^HT_l $$ 因此正则损失函数为 $$ \mathcal{L}r=\sum{l\in\Omega}|| T_l-\hat{A}_lS_l ||^2 $$</div><div class="post-footer">
        <a href="/deep-image-spatial-transformation-for-person-image-generation/">Read More</a></div>
</article><ul class="pagination"><li class="page-item ">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/5/">5</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/6/">6</a>
                    </span>
                </li><li class="page-item active">
                    <span class="page-link">
                        <a href="/page/7/">7</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/8/">8</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/9/">9</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/15/">15</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.63.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">shilongshen</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
