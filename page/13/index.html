<!DOCTYPE html>
<html lang="zh-cn">
    <head>
	<meta name="generator" content="Hugo 0.63.0" />
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>我的个人博客</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="我的个人博客" />
<meta property="og:description" content="这是我的全新 Hugo 网站" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://shilongshen.github.io/" />
<meta property="og:image" content="https://shilongshen.github.io/logo.png"/>
<meta property="og:updated_time" content="2021-02-04T09:17:30+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://shilongshen.github.io/logo.png"/>

<meta name="twitter:title" content="我的个人博客"/>
<meta name="twitter:description" content="这是我的全新 Hugo 网站"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://shilongshen.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="我的个人博客">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="我的个人博客"><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/shilongshen.github.io\/","inLanguage": "zh-cn","author": {
                "@type": "Person",
                "name": "shilongshen"
            },"description": "这是我的全新 Hugo 网站","name": "我的个人博客"
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="我的个人博客">首页</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="我的个人博客">首页</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="page home" posts><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="文章"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/avatar.jpg"
        data-srcset="/images/avatar.jpg, /images/avatar.jpg 1.5x, /images/avatar.jpg 2x"
        data-sizes="auto"
        alt="/images/avatar.jpg"
        title="/images/avatar.jpg" /></a></div><h1 class="home-title">我的个人博客</h1><div class="links"><a href="https://github.com/shilongshen" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github-alt fa-fw"></i></a><a href="mailto:1319144765@qq.com" title="Email" rel=" me"><i class="far fa-envelope fa-fw"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">各种激活函数</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">参考
sigmoid $$ f(z)=\frac{1}{1+e^{-z}} $$
其图像如下:
特点  能够将输入的连续实值变换为0到1之间的输出  缺点  在深度神经网络中梯度反向传播是容易造成梯度爆炸和梯度消失  sigmoid导数 $$ f&rsquo;(z)=\frac{e^{-z}}{(1+e^{-z})^2} $$
其导数图像如下:
tanh $$ tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$
其图像如下:
特点 解决了sigmoid函数不是zero-centered的问题, 但是梯度消失依旧存在
导数 $$ tanh&rsquo;(x)=1-(\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}})^2 $$
导数图像
Relu $$ Relu(x)=max(0,x) $$
函数图像
导数 $$ Relu&rsquo;(x)=\left{ \begin{array}{lr} 1 &amp;x&gt;0 \
0 &amp;x&lt;0
 \end{array}  \right. $$
优点  解决了梯度消失问题 计算速度非常快 收敛速度远快于sigmoid和tanh  缺点  输出的不是zero-centered 有些神经元可能永远不会被激活(Dead ReLU)  不好的参数初始化 学习率过高, 导致网络不幸进入这种情况    Leaky Relu(PRelu) $$ f(x)=max(\alpha x,x) $$</div><div class="post-footer">
        <a href="/%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E8%B0%83%E6%95%B4/">学习率的调整</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">参考
目标函数损失值 曲线   曲线 初始时 上扬 [红线]： Solution：初始 学习率过大 导致 振荡，应减小学习率，并 从头 开始训练 。
  曲线 初始时 强势下降 没多久 归于水平 [紫线]： Solution：后期 学习率过大 导致 无法拟合，应减小学习率，并 重新训练 后几轮 。
  曲线 全程缓慢 [黄线]： Solution：初始 学习率过小 导致 收敛慢，应增大学习率，并从头 开始训练。
  </div><div class="post-footer">
        <a href="/%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E8%B0%83%E6%95%B4/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/_____________________________________________________________________/">最大似然估计、最大后验估计以及贝叶斯公式的理解</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">概率和统计的区别？
概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。
概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。
统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。
一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。
显然，本文解释的MLE和MAP都是统计领域的问题。它们都是用来推测参数的方法。为什么会存在着两种不同方法呢？ 这需要理解贝叶斯思想。我们来看看贝叶斯公式。
什么是贝叶斯公式？ &hellip;
极大似然估计 极大似然估计，通俗 理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！
换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。（给定样本来推测模型的参数）
极大后验估计 </div><div class="post-footer">
        <a href="/_____________________________________________________________________/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/">梯度消失和梯度爆炸</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">参考
首先让我们先来了解一个概念：什么是梯度不稳定呢？
概念：在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。
产生梯度不稳定的根本原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。
划重点：梯度消失和梯度爆炸属于梯度不稳定的范畴
1.梯度消失与梯度爆炸的概念 梯度消失：在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失。
梯度爆炸：在神经网络中，当前面隐藏层的学习速率高于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度爆炸。
2.梯度消失与梯度爆炸的产生原因 梯度消失：（1）隐藏层的层数过多；（2）采用了不合适的激活函数(更容易产生梯度消失，但是也有可能产生梯度爆炸)
梯度爆炸：（1）隐藏层的层数过多；（2）权重的初始化值过大
（1）隐藏层的层数过多
　总结：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。具体见下图：
从上图可以非常容易的看出来：对于四个隐层的网络来说，第四隐藏层比第一隐藏层的更新速度慢了两个数量级！！！
（2）激活函数
我们以下图的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。
3）初始化权重的值过大
当，也就是w比较大的情况下，根据2式的链式相乘可得(反向传播)，则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。所以，在一般的神经网络中，权重的初始化一般都利用高斯分布(正态分布)随机产生权重值。
3.梯度消失与梯度爆炸的解决方案 梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑一下三种方案解决：
（1）用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。(几种激活函数的比较见我的博客)
（2）用Batch Normalization。(对于Batch Normalization的理解可以见我的博客)
（3）LSTM的结构设计也可以改善RNN中的梯度消失问题。</div><div class="post-footer">
        <a href="/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8/">池化层的作用</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">池化层夹在连续的卷积层中间， 用于压缩数据和参数数量，减小过拟合。如果输入是图像的话，那么池化层的最主要作用就是压缩图像。特征不变性也就是在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的 resize，平时一张狗的图像被缩小了一倍还能识别出这是一张狗的图像，这说明这张图像中仍保留着狗最重要的特征，图像压缩时去掉的信息只是一些冗余信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。池化层去冗余信息的思路是将一块区域的特征值集合通过一个主要特征值替换，达到快速数据降维的效果，常见方法有取局部最大值(maxpooling)或取局部平均(averagepooling)。
池化层不但能够有效减小神经元的数量，还可以使得网络对一些小的局部形态改变保持不变形，并拥有更大的感受野</div><div class="post-footer">
        <a href="/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">深度学习的基本概念</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>shilongshen</a></span>&nbsp;<span class="post-publish">published on <time datetime="2020-11-15">2020-11-15</time></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="far fa-folder fa-fw"></i>深度学习论文阅读笔记</a></span></div><div class="content">1绪论 ​	基本的深度学习相当于函数逼近问题，即函数或曲面的拟合 ，所不同的是，这里用作基函数的是非线性的神经网络函数，而原来数学中的用的则是多项式、三角不等式等。
​	由于神经网络的非线性和复杂性，它有更强的表达能力，即能够从给定的神经网络函数族中可能找到对特定数据集拟合得更好的神经网络。这里既涉及设计合适的神经网络类型，也涉及从该类型中的神经网络中找出好的（即拟合误差小的）特定神经网络的方法。后者正是数学中最优化分支所研究的问题。从数学角度来说，目前深度学习中所用的优化算法还是属于比较简单的梯度下降法。
​	但是恰恰由于神经网络的非线性复杂性，使得能够通过大量的标注数据经过深度学习可以得到一个误差结果很小的神经网络，但是要用它来解释却十分的困难。近来也有学者发现，一个精度很高的神经网络，改变它的几个参数，就会使该网络的精度明显下降。换言之，深度学习方法的鲁棒性也有待研究。
2.机器学习、深度学习和人工神经网络的关系 ​	深度学习以神经网络为主要模型。深度学习一开始用来解决机器学习中的表示学习问题。但是由于其强大的能力，深度学习越来越多的用来解决一些通用的人工智能问题，比如决策、推理等。
​	从根源上来说，**深度学习是机器学习的一个分支，**是指一类问题以及解决这类问题的方法。指从有限样例中通过算法总结出一般性的规律，并可应用到新的未知数据上。
​	其次，深度学习采用的模型一般比较复杂，指样本的原始输入到输出目标 之间的数据流经过多个线性或非线性的组件（component）．因为每个组件都会对信息进行加工，并进而影响后续的组件，所以当我们最后得到输出结果时，我们并不清楚其中每个组件的贡献是多少．这个问题叫作贡献度分配问题（Credit Assignment Problem，CAP）[Minsky, 1961]． 贡献度分配问题也经 常翻译为信用分配问题或功劳分配问题．在深度学习中，贡献度分配问题是 一个很关键的问题，这关系到如何学习每个组件中的参数。
​	目前，一种可以比较好解决贡献度分配问题的模型是人工神经网络。神经网络和深度学习并不等价，深度学习可以采用神经网络模型，也可以采用其他模型（比如深度信念网络是一种概率图模型）.但是由于神经网络模型可以比较容易地解决贡献度分配问题，因此神经网络模型成为深度学习中主要采用的模型。
2.1人工智能 ​	人工智能是计算机科学的一个分支，主要研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统等．和很多其他学科不同，人工智能这个学科的诞生有着明确的标志性事件，就是1956 年的达特茅斯（Dartmouth）会议．在这次会议上，“人工智能”被提出并作为本研究领域的名称．同时，人工智能研究的使命也得以确定．John McCarthy 提出了人工智能的定义：人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样．
​	目前，人工智能的主要领域大体上可以分为以下几个方面：
（1）感知：：模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感 知和加工．主要研究领域包括语音信息处理和计算机视觉等．
（2）学习:模拟人的学习能力，主要研究如何从样例或从与环境的交互中 进行学习．主要研究领域包括监督学习、无监督学习和强化学习等．
（3）认知:模拟人的认知能力，主要研究领域包括知识表示、自然语言理 解、推理、规划、决策等．
2.2深度学习 ​	深度学习是将原始的数据特征通过多步的特征转换得到一种特征表示，并进一步输入到预测函数得到最终结果．和“浅层学习”不同，深度学习需要解决的关键问题是贡献度分配问题（Credit Assignment Problem，CAP）[Minsky, 1961]，即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响．以下围棋为例，每当下完一盘棋，最后的结果要么赢要么输．我们会思考哪几步棋导致了最后的胜利，或者又是哪几步棋导致了最后的败局．如何判断每一步棋的贡献就是贡献度分配问题，这是一个非常困难的问题．从某种意义上讲，深度学习可以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性。
​	目前，深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决贡献度分配问题．只要是超过一层的神经网络都会存在贡献度分配问题，因此可以将超过一层的神经网络都看作深度学习模型．随着深度学习的快速发展，模型深度也从早期的5 ∼ 10层增加到目前的数百层．随着模型深度的不断增加，其特征表示的能力也越来越强，从而使后续的预测更加容易。
​</div><div class="post-footer">
        <a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">Read More</a></div>
</article><ul class="pagination"><li class="page-item ">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/11/">11</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/12/">12</a>
                    </span>
                </li><li class="page-item active">
                    <span class="page-link">
                        <a href="/page/13/">13</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/14/">14</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/15/">15</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.63.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">shilongshen</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
